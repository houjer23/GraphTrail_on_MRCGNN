{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pysr in /Users/tyler/anaconda3/lib/python3.12/site-packages (1.5.2)\n",
      "Requirement already satisfied: sympy<2.0.0,>=1.0.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from pysr) (1.13.1)\n",
      "Requirement already satisfied: pandas<3.0.0,>=0.21.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from pysr) (2.2.2)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.13.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from pysr) (1.26.4)\n",
      "Requirement already satisfied: scikit_learn<2.0.0,>=1.0.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from pysr) (1.5.1)\n",
      "Requirement already satisfied: juliacall==0.9.24 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from pysr) (0.9.24)\n",
      "Requirement already satisfied: click<9.0.0,>=7.0.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from pysr) (8.1.7)\n",
      "Requirement already satisfied: setuptools>=50.0.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from pysr) (75.1.0)\n",
      "Requirement already satisfied: juliapkg~=0.1.8 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from juliacall==0.9.24->pysr) (0.1.16)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from pandas<3.0.0,>=0.21.0->pysr) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from pandas<3.0.0,>=0.21.0->pysr) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from pandas<3.0.0,>=0.21.0->pysr) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from scikit_learn<2.0.0,>=1.0.0->pysr) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from scikit_learn<2.0.0,>=1.0.0->pysr) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from scikit_learn<2.0.0,>=1.0.0->pysr) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from sympy<2.0.0,>=1.0.0->pysr) (1.3.0)\n",
      "Requirement already satisfied: filelock<4.0,>=3.16 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from juliapkg~=0.1.8->juliacall==0.9.24->pysr) (3.17.0)\n",
      "Requirement already satisfied: semver<4.0,>=3.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from juliapkg~=0.1.8->juliacall==0.9.24->pysr) (3.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=0.21.0->pysr) (1.16.0)\n",
      "Requirement already satisfied: gnn in /Users/tyler/anaconda3/lib/python3.12/site-packages (1.1.9)\n",
      "Requirement already satisfied: utils in /Users/tyler/anaconda3/lib/python3.12/site-packages (1.0.2)\n",
      "Requirement already satisfied: torch in /Users/tyler/anaconda3/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in /Users/tyler/anaconda3/lib/python3.12/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in /Users/tyler/anaconda3/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/tyler/anaconda3/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in /Users/tyler/anaconda3/lib/python3.12/site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: torch_geometric in /Users/tyler/anaconda3/lib/python3.12/site-packages (2.6.1)\n",
      "Requirement already satisfied: aiohttp in /Users/tyler/anaconda3/lib/python3.12/site-packages (from torch_geometric) (3.10.5)\n",
      "Requirement already satisfied: fsspec in /Users/tyler/anaconda3/lib/python3.12/site-packages (from torch_geometric) (2024.6.1)\n",
      "Requirement already satisfied: jinja2 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from torch_geometric) (3.1.4)\n",
      "Requirement already satisfied: numpy in /Users/tyler/anaconda3/lib/python3.12/site-packages (from torch_geometric) (1.26.4)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from torch_geometric) (5.9.0)\n",
      "Requirement already satisfied: pyparsing in /Users/tyler/anaconda3/lib/python3.12/site-packages (from torch_geometric) (3.1.2)\n",
      "Requirement already satisfied: requests in /Users/tyler/anaconda3/lib/python3.12/site-packages (from torch_geometric) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /Users/tyler/anaconda3/lib/python3.12/site-packages (from torch_geometric) (4.66.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from aiohttp->torch_geometric) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from aiohttp->torch_geometric) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from aiohttp->torch_geometric) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from aiohttp->torch_geometric) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from aiohttp->torch_geometric) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from aiohttp->torch_geometric) (1.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from jinja2->torch_geometric) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from requests->torch_geometric) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from requests->torch_geometric) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from requests->torch_geometric) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from requests->torch_geometric) (2025.1.31)\n",
      "Data(x=[572, 128], edge_index=[2, 52112], edge_type=[52112])\n"
     ]
    }
   ],
   "source": [
    "!pip install pysr\n",
    "!pip install gnn\n",
    "!pip install utils\n",
    "!pip install torch\n",
    "!pip install torch_geometric\n",
    "from pysr import PySRRegressor\n",
    "import warnings\n",
    "from argparse import ArgumentParser\n",
    "from os import makedirs\n",
    "from pickle import dump, load\n",
    "from shutil import rmtree\n",
    "from time import process_time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sympy\n",
    "import torch\n",
    "import torch_geometric as pyg\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "import gnn\n",
    "import utils\n",
    "\n",
    "sns.set()\n",
    "sns.set_style(\"white\")\n",
    "warnings.filterwarnings('ignore')\n",
    "# Load the tensor and map it to the CPU\n",
    "data = torch.load('data_o_new2.pt', map_location=torch.device('cpu'), weights_only = False)\n",
    "\n",
    "# Print the shape of the tensor\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MRCGNN Revised Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import RGCNConv\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, n_h):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.f_k = nn.Bilinear(32, 32, 1)\n",
    "        for m in self.modules():\n",
    "            self.weights_init(m)\n",
    "    def weights_init(self, m):\n",
    "        if isinstance(m, nn.Bilinear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.fill_(0.0)\n",
    "    def forward(self, c, h_pl, h_mi, s_bias1=None, s_bias2=None):\n",
    "        c_x = c.expand_as(h_pl)\n",
    "        sc_1 = self.f_k(h_pl, c_x)\n",
    "        sc_2 = self.f_k(h_mi, c_x)\n",
    "        if s_bias1 is not None:\n",
    "            sc_1 += s_bias1\n",
    "        if s_bias2 is not None:\n",
    "            sc_2 += s_bias2\n",
    "        logits = torch.cat((sc_1, sc_2), 1)\n",
    "        return logits\n",
    "\n",
    "class AvgReadout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AvgReadout, self).__init__()\n",
    "    def forward(self, seq, msk=None):\n",
    "        if msk is None:\n",
    "            return torch.mean(seq, 0)\n",
    "        else:\n",
    "            msk = torch.unsqueeze(msk, -1)\n",
    "            return torch.sum(seq * msk, 0) / torch.sum(msk)\n",
    "\n",
    "class MRCGNN(nn.Module):\n",
    "    def __init__(self, feature, hidden1, hidden2, decoder1, dropout, zhongzi):\n",
    "        super(MRCGNN, self).__init__()\n",
    "\n",
    "        # RGCN layers for the main (data_o) branch\n",
    "        self.encoder_o1 = RGCNConv(feature, hidden1, num_relations=65)\n",
    "        self.encoder_o2 = RGCNConv(hidden1, hidden2, num_relations=65)\n",
    "\n",
    "        # Two-element parameter for layer attention\n",
    "        self.attt = nn.Parameter(torch.tensor([0.5, 0.5]))\n",
    "        self.disc = Discriminator(hidden2 * 2)\n",
    "        self.dropout = dropout\n",
    "        self.sigm = nn.Sigmoid()\n",
    "        self.read = AvgReadout()\n",
    "        \n",
    "        # Final classifier: prediction solely from data_o branch.\n",
    "        # Each node's final representation is a concatenation of (hidden1 + hidden2).\n",
    "        # For a pair of entities, the dimension becomes 2*(hidden1+hidden2).\n",
    "        self.classifier = nn.Linear(2 * (hidden1 + hidden2), 65)\n",
    "\n",
    "        # We no longer load any pretrained features for skip connection.\n",
    "\n",
    "    def forward(self, data_o, data_s, data_a, idx):\n",
    "        # Process data_o branch\n",
    "        x_o, adj, e_type = data_o.x, data_o.edge_index, data_o.edge_type\n",
    "        e_type1 = data_a.edge_type\n",
    "        e_type = torch.tensor(e_type, dtype=torch.int64)\n",
    "        e_type1 = torch.tensor(e_type1, dtype=torch.int64)\n",
    "\n",
    "        # Main branch for prediction (data_o)\n",
    "        x1_o = F.relu(self.encoder_o1(x_o, adj, e_type))\n",
    "        x1_o = F.dropout(x1_o, self.dropout, training=self.training)\n",
    "        x2_o = self.encoder_o2(x1_o, adj, e_type)\n",
    "\n",
    "        # Contrastive learning branches (unused in prediction)\n",
    "        x_a = data_s.x\n",
    "        x1_o_a = F.relu(self.encoder_o1(x_a, adj, e_type))\n",
    "        x1_o_a = F.dropout(x1_o_a, self.dropout, training=self.training)\n",
    "        x2_o_a = self.encoder_o2(x1_o_a, adj, e_type)\n",
    "\n",
    "        x1_o_a_a = F.relu(self.encoder_o1(x_o, adj, e_type1))\n",
    "        x1_o_a_a = F.dropout(x1_o_a_a, self.dropout, training=self.training)\n",
    "        x2_o_a_a = self.encoder_o2(x1_o_a_a, adj, e_type1)\n",
    "\n",
    "        # Readout for contrastive learning\n",
    "        h_os = self.read(x2_o)\n",
    "        h_os = self.sigm(h_os)\n",
    "        ret_os = self.disc(h_os, x2_o, x2_o_a)\n",
    "        ret_os_a = self.disc(h_os, x2_o, x2_o_a_a)\n",
    "\n",
    "        # For final prediction, use only data_o branch:\n",
    "        final = torch.cat((self.attt[0] * x1_o, self.attt[1] * x2_o), dim=1)\n",
    "\n",
    "        a = [int(i) for i in list(idx[0])]\n",
    "        b = [int(i) for i in list(idx[1])]\n",
    "        aa = torch.tensor(a, dtype=torch.long)\n",
    "        bb = torch.tensor(b, dtype=torch.long)\n",
    "        entity1 = final[aa]\n",
    "        entity2 = final[bb]\n",
    "        concatenate = torch.cat((entity1, entity2), dim=1)\n",
    "        log = self.classifier(concatenate)\n",
    "\n",
    "        return log, ret_os, ret_os_a, x2_o\n",
    "\n",
    "    def predict(self, data_o, idx):\n",
    "        \"\"\"\n",
    "        New prediction method that uses only data_o and idx.\n",
    "        \"\"\"\n",
    "        x_o, adj, e_type = data_o.x, data_o.edge_index, data_o.edge_type\n",
    "        e_type = torch.tensor(e_type, dtype=torch.int64)\n",
    "        # Process the main branch\n",
    "        x1_o = F.relu(self.encoder_o1(x_o, adj, e_type))\n",
    "        x1_o = F.dropout(x1_o, self.dropout, training=self.training)\n",
    "        x2_o = self.encoder_o2(x1_o, adj, e_type)\n",
    "        final = torch.cat((self.attt[0] * x1_o, self.attt[1] * x2_o), dim=1)\n",
    "        \n",
    "        a = [int(i) for i in list(idx[0])]\n",
    "        b = [int(i) for i in list(idx[1])]\n",
    "        aa = torch.tensor(a, dtype=torch.long)\n",
    "        bb = torch.tensor(b, dtype=torch.long)\n",
    "        entity1 = final[aa]\n",
    "        entity2 = final[bb]\n",
    "        concatenate = torch.cat((entity1, entity2), dim=1)\n",
    "        log = self.classifier(concatenate)\n",
    "        return log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load My Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MRCGNN(\n",
       "  (encoder_o1): RGCNConv(128, 64, num_relations=65)\n",
       "  (encoder_o2): RGCNConv(64, 32, num_relations=65)\n",
       "  (disc): Discriminator(\n",
       "    (f_k): Bilinear(in1_features=32, in2_features=32, out_features=1, bias=True)\n",
       "  )\n",
       "  (sigm): Sigmoid()\n",
       "  (read): AvgReadout()\n",
       "  (classifier): Linear(in_features=192, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "model = MRCGNN(feature=128, hidden1=64, hidden2=32, decoder1=512, dropout=0.5, zhongzi=0)\n",
    "model.load_state_dict(torch.load(\"model_mrcgnn.pt\", map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "model.load_state_dict(torch.load(\"model_mrcgnn.pt\", map_location='cpu'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Computes the softmax function for a NumPy array.\n",
    "    Args:\n",
    "        x (np.ndarray): The input array.\n",
    "    Returns:\n",
    "        np.ndarray: The softmax output array.\n",
    "    \"\"\"\n",
    "    e_x = np.exp(x - np.max(x)) # Subtract max for numerical stability\n",
    "    return e_x / e_x.sum(axis=len(x.shape)-1, keepdims=True)\n",
    "\n",
    "id2row = {}\n",
    "counter = 0\n",
    "\n",
    "with open('drug_listxiao.csv', mode ='r')as file:\n",
    "  csvFile = csv.reader(file)\n",
    "  for f in file:\n",
    "    id2row[f.split(',')[0]] = counter\n",
    "    counter += 1\n",
    "\n",
    "idx0, idx1 = [], []\n",
    "def predprob(file):\n",
    "    pred, prob, xbin = [], [], []\n",
    "    with open(file, mode ='r')as file:\n",
    "      csvFile = csv.reader(file)\n",
    "      has_header = csv.Sniffer().has_header(file.read(1024))\n",
    "      file.seek(0)  # Rewind.\n",
    "      reader = csv.reader(file)\n",
    "      if has_header:\n",
    "        next(reader)  # Skip header row.\n",
    "      for lines in csvFile:\n",
    "          idx0.append(id2row[lines[0]])\n",
    "          idx1.append(id2row[lines[-1]])\n",
    "          xbin.append((id2row[lines[0]],id2row[lines[-1]]))\n",
    "    ans = model.predict(data,(idx0,idx1))\n",
    "    ans_pred = ans.detach().numpy().reshape((-1,65))\n",
    "    for i in range(ans_pred.shape[0]):\n",
    "        a = np.max(ans_pred[i])\n",
    "        b = softmax(ans_pred[i])\n",
    "        for j in range(ans_pred.shape[1]):\n",
    "            if ans_pred[i][j] == a:\n",
    "                pred.append(j)\n",
    "                prob.append(b[j])\n",
    "    return pred, prob, xbin\n",
    "train_pred, train_prob = [], []\n",
    "test_pred, test_prob = [], []\n",
    "val_pred, val_prob = [], []\n",
    "x_train_bin, x_test_bin, x_val_bin = [], [], []\n",
    "train_pred, train_prob, x_train_bin = predprob('ddi_training1xiao.csv')\n",
    "test_pred, test_prob, x_test_bin = predprob('ddi_test1xiao.csv')\n",
    "val_pred, val_prob, x_val_bin = predprob('ddi_validation1xiao.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric as pyg\n",
    "from torch_geometric.utils import k_hop_subgraph, to_networkx\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "##############################################################################\n",
    "# 1) LOAD YOUR DATA\n",
    "##############################################################################\n",
    "data = torch.load('data_o_new2.pt', map_location=torch.device('cpu'))\n",
    "print(\"Data object:\", data)\n",
    "# data is a single graph with data.x, data.edge_index, data.edge_type, etc.\n",
    "# e.g.: Data(x=[572, 128], edge_index=[2, 52112], edge_type=[52112])\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "# 2) EXTRACT COMPUTATION TREES & CREATE CONCEPT VECTOR\n",
    "##############################################################################\n",
    "L = 3  # number of hops for each node’s subgraph (the “computation tree”)\n",
    "\n",
    "# We'll collect:\n",
    "#   node_ctree_codes[v] = string code for node v’s L-hop subgraph\n",
    "#   unique_ctree_codes   = dict {code_str -> assigned_id}\n",
    "node_ctree_codes = []\n",
    "unique_ctree_codes = {}\n",
    "\n",
    "def simple_dfs_code(G, root):\n",
    "    \"\"\"\n",
    "    Simple DFS code from a NetworkX graph G (treated as a tree),\n",
    "    starting at node 'root'. This is just a placeholder function:\n",
    "    you'd use a canonical labeling or something more robust in production.\n",
    "    \"\"\"\n",
    "    code = []\n",
    "    for n in nx.dfs_preorder_nodes(G, source=root):\n",
    "        code.append(str(n))\n",
    "    return \"-\".join(code)\n",
    "\n",
    "for v in range(data.num_nodes):\n",
    "    # Extract L-hop subgraph\n",
    "    subset, sub_edge_index, mapping, _ = k_hop_subgraph(\n",
    "        node_idx=v,\n",
    "        num_hops=L,\n",
    "        edge_index=data.edge_index,\n",
    "        relabel_nodes=True\n",
    "    )\n",
    "    # Build a small Data object\n",
    "    sub_data = pyg.data.Data(\n",
    "        x=data.x[subset],\n",
    "        edge_index=sub_edge_index\n",
    "    )\n",
    "    # Convert to NetworkX\n",
    "    G_sub = to_networkx(sub_data, to_undirected=True)\n",
    "    \n",
    "    # In PyG’s relabel_nodes=True, the root node v becomes \"0\" in sub_data.\n",
    "    ctree_code = simple_dfs_code(G_sub, root=0)\n",
    "    node_ctree_codes.append(ctree_code)\n",
    "    if ctree_code not in unique_ctree_codes:\n",
    "        unique_ctree_codes[ctree_code] = len(unique_ctree_codes)\n",
    "\n",
    "# --- Step 2: Build the concept vector for the entire graph. ---\n",
    "# This vector is length (# unique ctree codes),\n",
    "# and entry i = count of nodes that have that code.\n",
    "concept_vector = np.zeros(len(unique_ctree_codes), dtype=int)\n",
    "for code in node_ctree_codes:\n",
    "    idx = unique_ctree_codes[code]\n",
    "    concept_vector[idx] += 1\n",
    "\n",
    "print(\"Number of unique ctree codes:\", len(unique_ctree_codes))\n",
    "print(\"Concept vector (frequency of each code):\")\n",
    "print(concept_vector)\n",
    "\n",
    "# Print out the first 5 unique computation tree codes and their frequencies:\n",
    "print(\"Sample unique computation tree codes and frequencies:\")\n",
    "for i, (code, idx) in enumerate(unique_ctree_codes.items()):\n",
    "    if i >= 5:\n",
    "        break\n",
    "    freq = concept_vector[idx]\n",
    "    print(f\"Code {idx}: {code}\")\n",
    "    print(f\"Frequency: {freq}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEFINE \"REMOVE CONCEPTS\" FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_concepts_from_graph(data, node_ctree_codes, unique_ctree_codes, subset_of_codes):\n",
    "    \"\"\"\n",
    "    Prune out nodes whose L-hop code is not in 'subset_of_codes'.\n",
    "    Returns a new Data object and a mapping dict (old_index -> new_index).\n",
    "    \"\"\"\n",
    "    keep_nodes_list = []\n",
    "    for v in range(data.num_nodes):\n",
    "        if node_ctree_codes[v] in subset_of_codes:\n",
    "            keep_nodes_list.append(v)\n",
    "    \n",
    "    mapping = {old: new for new, old in enumerate(keep_nodes_list)}\n",
    "    \n",
    "    if not keep_nodes_list:\n",
    "        # No nodes remain; return empty data and empty mapping\n",
    "        new_data = pyg.data.Data(\n",
    "            x=torch.empty((0, data.x.shape[1])),\n",
    "            edge_index=torch.empty((2, 0), dtype=torch.long),\n",
    "            edge_type=torch.empty((0,), dtype=torch.long)\n",
    "        )\n",
    "        return new_data, mapping\n",
    "\n",
    "    keep_nodes = torch.tensor(keep_nodes_list, dtype=torch.long)\n",
    "    # Filter node features\n",
    "    x_new = data.x[keep_nodes]\n",
    "\n",
    "    # Filter edges and corresponding edge_type\n",
    "    edges = []\n",
    "    e_types = []\n",
    "    keep_set = set(keep_nodes_list)\n",
    "    for i in range(data.edge_index.size(1)):\n",
    "        src = data.edge_index[0, i].item()\n",
    "        dst = data.edge_index[1, i].item()\n",
    "        if (src in keep_set) and (dst in keep_set):\n",
    "            edges.append([src, dst])\n",
    "            e_types.append(data.edge_type[i])  # No .item() here\n",
    "    if edges:\n",
    "        edges = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "        e_types = torch.tensor(e_types, dtype=torch.int64)\n",
    "    else:\n",
    "        edges = torch.empty((2,0), dtype=torch.long)\n",
    "        e_types = torch.empty((0,), dtype=torch.int64)\n",
    "    \n",
    "    # Relabel node IDs\n",
    "    edges = _relabel_edge_index(edges, keep_nodes)\n",
    "\n",
    "    new_data = pyg.data.Data(x=x_new, edge_index=edges, edge_type=e_types)\n",
    "    return new_data, mapping\n",
    "\n",
    "def _relabel_edge_index(edge_index, keep_nodes):\n",
    "    old_to_new = {old: i for i, old in enumerate(keep_nodes.tolist())}\n",
    "    new_edges = []\n",
    "    for i in range(edge_index.size(1)):\n",
    "        src_old = edge_index[0, i].item()\n",
    "        dst_old = edge_index[1, i].item()\n",
    "        new_edges.append([old_to_new[src_old], old_to_new[dst_old]])\n",
    "    if len(new_edges) == 0:\n",
    "        return torch.empty((2, 0), dtype=torch.long)\n",
    "    new_edges = torch.tensor(new_edges, dtype=torch.long).t().contiguous()\n",
    "    return new_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEFINE A VALUE FUNCTION THAT RUNS MRCGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_function(model, data, idx,\n",
    "                   subset_of_codes, node_ctree_codes, unique_ctree_codes,\n",
    "                   target_class=None):\n",
    "    \"\"\"\n",
    "    1) Remove concepts not in 'subset_of_codes'\n",
    "    2) Update idx based on the new node numbering\n",
    "    3) Run model.predict() and return a scalar value.\n",
    "    \"\"\"\n",
    "    # 1) Prune the graph and get the mapping from old indices to new indices.\n",
    "    modified_data, mapping = remove_concepts_from_graph(\n",
    "        data, node_ctree_codes, unique_ctree_codes, subset_of_codes\n",
    "    )\n",
    "    \n",
    "    # 2) Update idx: For each index in idx, if it exists in mapping, use the new index.\n",
    "    new_src = []\n",
    "    new_dst = []\n",
    "    for src, dst in zip(idx[0].tolist(), idx[1].tolist()):\n",
    "        if src in mapping and dst in mapping:\n",
    "            new_src.append(mapping[src])\n",
    "            new_dst.append(mapping[dst])\n",
    "    if len(new_src) == 0 or len(new_dst) == 0:\n",
    "        return 0.0  # no valid pairs remain\n",
    "\n",
    "    new_idx = (torch.tensor(new_src, dtype=torch.long),\n",
    "               torch.tensor(new_dst, dtype=torch.long))\n",
    "    \n",
    "    # 3) Forward pass\n",
    "    with torch.no_grad():\n",
    "        out = model.predict(modified_data, new_idx)  # shape: [num_pairs, 65]\n",
    "        if out.shape[0] == 0:\n",
    "            return 0.0\n",
    "\n",
    "        if target_class is None:\n",
    "            # Use the predicted class for each pair and average the logit.\n",
    "            preds = out.argmax(dim=1)\n",
    "            chosen_logits = out[torch.arange(out.size(0)), preds]\n",
    "            val = chosen_logits.mean().item()\n",
    "        else:\n",
    "            chosen_logits = out[:, target_class]\n",
    "            val = chosen_logits.mean().item()\n",
    "    return val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pysr in /Users/tyler/anaconda3/lib/python3.12/site-packages (1.5.2)\n",
      "Requirement already satisfied: sympy<2.0.0,>=1.0.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from pysr) (1.13.3)\n",
      "Requirement already satisfied: pandas<3.0.0,>=0.21.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from pysr) (2.2.2)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.13.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from pysr) (1.26.4)\n",
      "Requirement already satisfied: scikit_learn<2.0.0,>=1.0.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from pysr) (1.5.1)\n",
      "Requirement already satisfied: juliacall==0.9.24 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from pysr) (0.9.24)\n",
      "Requirement already satisfied: click<9.0.0,>=7.0.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from pysr) (8.1.7)\n",
      "Requirement already satisfied: setuptools>=50.0.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from pysr) (75.1.0)\n",
      "Requirement already satisfied: juliapkg~=0.1.8 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from juliacall==0.9.24->pysr) (0.1.16)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from pandas<3.0.0,>=0.21.0->pysr) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from pandas<3.0.0,>=0.21.0->pysr) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from pandas<3.0.0,>=0.21.0->pysr) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from scikit_learn<2.0.0,>=1.0.0->pysr) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from scikit_learn<2.0.0,>=1.0.0->pysr) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from scikit_learn<2.0.0,>=1.0.0->pysr) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from sympy<2.0.0,>=1.0.0->pysr) (1.3.0)\n",
      "Requirement already satisfied: filelock<4.0,>=3.16 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from juliapkg~=0.1.8->juliacall==0.9.24->pysr) (3.17.0)\n",
      "Requirement already satisfied: semver<4.0,>=3.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from juliapkg~=0.1.8->juliacall==0.9.24->pysr) (3.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=0.21.0->pysr) (1.16.0)\n",
      "Requirement already satisfied: gnn in /Users/tyler/anaconda3/lib/python3.12/site-packages (1.1.9)\n",
      "Requirement already satisfied: utils in /Users/tyler/anaconda3/lib/python3.12/site-packages (1.0.2)\n",
      "Detected IPython. Loading juliacall extension. See https://juliapy.github.io/PythonCall.jl/stable/compat/#IPython\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SHAPLEY VALUES (SAMPLING APPROACH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def shapley_values(model, data, idx,\n",
    "                   unique_ctree_codes, node_ctree_codes,\n",
    "                   target_class=None,\n",
    "                   num_samples=50):\n",
    "    \"\"\"\n",
    "    Approximate Shapley values via random permutations.\n",
    "    \"\"\"\n",
    "    concepts = list(unique_ctree_codes.keys())  # DFS-code strings\n",
    "    M = len(concepts)\n",
    "    shap = np.zeros(M, dtype=float)\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        perm = np.random.permutation(M)\n",
    "        current_subset = set()\n",
    "        old_val = value_function(\n",
    "            model, data, idx,\n",
    "            current_subset, node_ctree_codes, unique_ctree_codes,\n",
    "            target_class\n",
    "        )\n",
    "        for j in range(M):\n",
    "            c_idx = perm[j]\n",
    "            c_code = concepts[c_idx]\n",
    "            new_subset = current_subset.union({c_code})\n",
    "            new_val = value_function(\n",
    "                model, data, idx,\n",
    "                new_subset, node_ctree_codes, unique_ctree_codes,\n",
    "                target_class\n",
    "            )\n",
    "            shap[c_idx] += (new_val - old_val)\n",
    "            current_subset = new_subset\n",
    "            old_val = new_val\n",
    "\n",
    "    shap /= num_samples\n",
    "    code2shap = {concepts[i]: shap[i] for i in range(M)}\n",
    "    return code2shap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RUN SHAPLEY & DISPLAY RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qr/n043ky6s44q5gb_8fmmpj1h00000gn/T/ipykernel_7193/1228864276.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  e_type = torch.tensor(e_type, dtype=torch.int64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time taken: 652.350298166275\n",
      "\n",
      "Top-10 Concepts by Shapley Value:\n",
      "0-408-22-477-474-214-16-130-182-365-33-63-57-240-250-8-527-366-223-113-233-31-311-154-61-228-437-426-310-161-418-274-567-13-406-279-60-346-114-19-320-398-166-53-120-211-556-502-348-563-522-18-40-545-224-225-202-321-388-503-383-167-352-193-419-410-111-183-370-330-194-252-264-151-273-349-195-216-363-507-534-47-128-123-253-255-288-394-54-555-282-77-354-324-339-405-99-44-226-520-17-209-326-415-298-43-266-338-490-483-89-548-169-500-376-291-145-64-441-488-323-565-529-178-95-160-232-347-105-454-414-355-198-399-448-436-269-157-28-29-424-480-416-49-132-51-537-335-340-242-119-236-88-485-146-501-450-131-258-452-382-482-11-20-333-451-542-290-286-245-515-237-7-155-281-197-73-261-91-510-430-557-2-400-74-239-521-344-308-142-259-125-523-107-412-463-350-547-517-277-423-249-327-205-177-404-70-76-129-552-289-23-117-94-434-312-316-464-307-428-374-26-38-447-254-30-90-543-553-25-409-62-46-148-116-139-505-385-564-328-518-554-473-292-152-325-478-562-55-71-271-378-4-421-445-102-75-72-471-235-244-343-106-498-185-381-425-230-508-103-389-351-127-509-156-56-67-59-551-462-372-215-429-220-494-37-41-293-32-163-456-427-486-549-172-511-108-284-48-97-303-391-256-317-212-492-93-98-396-217-285-184-83-65-458-191-208-299-100-221-190-519-12-82-489-460-144-137-287-438-115-229-341-210-104-35-276-302-162-14-484-544-173-247-109-295-470-530-435-36-262-168-3-407-359-134-80-175-313-468-411-401-136-126-402-85-39-524-66-78-373-86-9-227-6-453-280-238-135-377-329-367-417-443-270-353-170-159-263-213-332-360-395-539-118-68-199-188-469-432-479-356-187-87-497-181-446-439-322-491-268-24-369-504-362-309-192-153-81-174-231-532-431-304-526-513-110-141-345-257-260-558-300-533-384-525-241-422-461-481-357-440-171-272-283-234-246-496-397-442-387-200-79-218-459-512-50-493-101-318-334-331-455-45-138-560-516-314-207-122-186-296-566-559-541-506-203-379-112-550-219-179-278-92-149-27-58-251-222-420-96-204-305-15-5-457-275-10-476-531-306-69-164-301-393-265-294-371-147-248-536-392-465-243-297-364-358-413-140-1-466-201-176-206-143-380-158-433-315-375-528-150-165-514-538-34-319-535-342-124-499-449-561-368-21-84-475-386-361-336-390-403-189-337-42-540-444-267-487-180-196-467-121-495-472-546-133-52 => 10.2671\n",
      "0-407-22-475-472-214-16-130-182-365-33-63-57-240-250-8-525-366-223-113-233-31-311-154-61-228-436-425-310-161-417-274-565-13-405-279-60-346-114-19-320-397-166-53-120-211-554-500-348-561-520-18-40-543-224-225-202-321-387-501-382-167-352-193-418-409-111-183-370-330-194-252-264-151-273-349-195-216-363-505-532-47-128-123-253-255-288-393-54-553-282-77-354-324-339-404-99-44-226-518-17-209-326-414-298-43-266-338-488-481-89-546-169-498-376-291-145-64-440-486-323-563-527-178-95-160-232-347-105-453-413-355-198-398-447-435-269-157-28-29-423-478-415-49-132-51-535-335-340-242-119-236-88-483-146-499-449-131-258-451-381-480-11-20-333-450-540-290-286-245-513-237-7-155-281-197-73-261-91-508-429-555-2-399-74-239-519-344-308-142-259-125-521-107-411-462-350-545-515-277-422-249-327-205-177-403-70-76-129-550-289-23-117-94-433-312-316-463-307-427-374-26-38-446-254-30-90-541-551-25-408-62-46-148-116-139-503-384-562-328-516-552-471-292-152-325-476-560-55-71-271-378-4-420-444-102-75-72-470-235-244-343-106-496-185-380-424-230-506-103-388-351-127-507-156-56-67-59-549-461-372-215-428-220-492-37-41-293-32-163-455-426-484-547-172-509-108-284-48-97-303-390-256-317-212-490-93-98-395-217-285-184-83-65-457-191-208-299-100-221-190-517-12-82-487-459-144-137-287-437-115-229-341-210-104-35-276-302-162-14-482-542-173-247-109-295-469-528-434-36-262-168-3-406-359-134-80-175-313-467-410-400-136-126-401-85-39-522-66-78-373-86-9-227-6-452-280-238-135-377-329-367-416-442-270-353-170-159-263-213-332-360-394-537-118-68-199-188-468-431-477-356-187-87-495-181-445-438-322-489-268-24-369-502-362-309-192-153-81-174-231-530-430-304-524-511-110-141-345-257-260-556-300-531-383-523-241-421-460-479-357-439-171-272-283-234-246-494-396-441-386-200-79-218-458-510-50-491-101-318-334-331-454-45-138-558-514-314-207-122-186-296-564-557-539-504-203-112-548-219-179-278-92-149-27-58-251-222-419-96-204-305-15-5-456-275-10-474-529-306-69-164-301-392-265-294-371-147-248-534-391-464-243-297-364-358-412-140-1-465-201-176-206-143-379-158-432-315-375-526-150-165-512-536-34-319-533-342-124-497-448-559-368-21-84-473-385-361-336-389-402-189-337-42-538-443-267-485-180-196-466-121-493-544-133-52 => 0.0031\n",
      "0-405-22-472-469-212-16-130-181-363-33-63-57-238-248-8-522-364-221-113-231-31-309-154-61-226-434-423-308-161-415-272-562-13-403-277-60-344-114-19-318-395-166-53-120-209-551-497-346-558-517-18-40-540-222-223-200-319-386-498-381-167-350-192-416-407-111-182-368-328-193-250-262-151-271-347-194-214-361-502-529-47-128-123-251-253-286-391-54-550-280-77-352-322-337-402-99-44-224-515-17-207-324-412-296-43-264-336-485-478-89-543-169-495-374-289-145-64-438-483-321-560-524-178-95-160-230-345-105-450-411-353-196-396-444-433-267-157-28-29-421-475-413-49-132-51-532-333-338-240-119-234-88-480-146-496-446-131-256-448-380-477-11-20-331-447-537-288-284-243-510-235-7-155-279-195-73-259-91-505-427-552-2-397-74-237-516-342-306-142-257-125-518-107-409-458-348-542-512-275-420-247-325-203-177-401-70-76-129-547-287-23-117-94-431-310-314-459-305-425-372-26-38-443-252-30-90-538-548-25-406-62-46-148-116-139-500-383-559-326-513-549-468-290-152-323-473-557-55-71-269-376-4-418-441-102-75-72-466-233-242-341-106-493-184-379-422-228-503-103-387-349-127-504-156-56-67-59-546-457-370-213-426-218-489-37-41-291-32-163-452-424-481-544-172-506-108-282-48-97-301-388-254-315-210-487-93-98-393-215-283-183-83-65-454-190-206-297-100-219-189-514-12-82-484-455-144-137-285-435-115-227-339-208-104-35-274-300-162-14-479-539-173-245-109-293-465-525-432-36-260-168-3-404-357-134-80-175-311-463-408-398-136-126-399-85-39-519-66-78-371-86-9-225-6-449-278-236-135-375-327-365-414-440-268-351-170-159-261-211-330-358-392-534-118-68-197-187-464-429-474-354-186-87-492-180-442-436-320-486-266-24-367-499-360-307-191-153-81-174-229-527-428-302-521-508-110-141-343-255-258-553-298-528-382-520-239-419-456-476-355-437-171-270-281-232-244-491-394-439-385-198-79-216-507-50-488-101-316-332-329-451-45-138-555-511-312-205-122-185-294-561-554-536-501-201-377-112-545-217-179-276-92-149-27-58-249-220-417-96-202-303-15-5-453-273-10-471-526-304-69-164-299-390-263-292-369-147-246-531-389-460-241-295-362-356-410-140-1-461-199-176-204-143-378-158-430-313-373-523-150-165-509-533-34-317-530-340-124-494-445-556-366-21-84-470-384-359-334-400-188-335-42-535-265-482-462-121-490-467-541-133-52 => 0.0010\n",
      "0-401-22-470-467-210-16-130-179-360-33-63-57-236-246-8-520-361-219-113-229-31-307-153-61-224-430-419-306-160-411-270-560-13-399-275-60-341-114-19-316-391-164-53-120-207-549-495-343-556-515-18-40-538-220-221-198-317-382-496-377-165-347-190-412-403-111-180-364-326-191-248-260-150-269-344-192-212-358-500-527-47-128-123-249-251-284-387-54-548-278-77-349-320-334-398-99-44-222-513-17-205-322-408-294-43-262-333-483-476-89-541-167-493-370-287-144-64-434-481-319-558-522-176-95-159-228-342-105-447-407-350-194-392-441-429-265-156-28-29-417-473-409-49-132-51-530-331-335-238-119-232-88-478-145-494-443-131-254-445-376-475-11-20-329-444-535-286-282-241-508-233-7-154-277-193-73-257-91-503-423-550-2-393-74-235-514-339-304-141-255-125-516-107-405-456-345-540-510-273-416-245-323-201-175-397-70-76-129-545-285-23-117-94-427-308-312-457-303-421-368-26-38-440-250-30-90-536-546-25-402-62-46-147-116-138-498-379-557-324-511-547-466-288-151-321-471-555-55-71-267-372-4-414-438-102-75-72-464-231-240-338-106-491-182-375-418-226-501-103-383-346-127-502-155-56-67-59-544-455-366-211-422-216-487-37-41-289-32-162-449-420-479-542-170-504-108-280-48-97-299-384-252-313-208-485-93-98-389-213-281-181-83-65-451-188-204-295-100-217-187-512-12-82-482-453-143-136-283-431-115-225-336-206-104-35-272-298-161-14-477-537-171-243-109-291-463-523-428-36-258-166-3-400-354-133-80-173-309-461-404-394-135-126-395-85-39-517-66-78-367-86-9-223-6-446-276-234-134-371-325-362-410-436-266-348-168-158-259-209-328-355-388-532-118-68-195-185-462-425-472-351-184-87-490-178-439-432-318-484-264-24-363-497-357-305-189-152-81-172-227-525-424-300-519-506-110-140-340-253-256-551-296-526-378-518-237-415-454-474-352-433-169-268-279-230-242-489-390-435-381-196-79-214-452-505-50-486-101-314-330-327-448-45-137-553-509-310-203-122-183-292-559-552-534-499-199-373-112-543-215-177-274-92-148-27-58-247-218-413-96-200-301-15-5-450-271-10-469-524-302-69-163-297-386-261-290-365-146-244-529-385-458-239-293-359-353-406-139-1-459-197-174-202-142-374-157-426-311-369-521-149-507-531-34-315-528-337-124-492-442-554-21-84-468-380-356-396-186-332-42-533-437-263-480-460-121-488-465-539-52 => 0.0005\n",
      "0-403-22-470-467-210-16-129-179-360-33-63-57-236-245-8-520-361-219-112-229-31-306-153-61-224-432-421-305-160-413-269-558-13-401-274-60-341-113-19-315-393-164-53-119-207-548-495-343-554-515-18-40-538-220-221-198-316-383-496-378-165-347-190-414-405-110-180-365-325-191-247-259-150-268-344-192-212-358-500-527-47-127-122-248-250-283-389-54-547-277-77-349-319-334-400-98-44-222-513-17-205-321-410-293-43-261-333-483-476-88-541-167-493-371-286-144-64-436-481-318-556-522-176-94-159-228-342-104-448-409-350-194-394-442-431-264-156-28-29-419-473-411-49-131-51-530-330-335-238-118-232-87-478-145-494-444-130-253-446-377-475-11-20-328-445-535-285-281-240-508-233-7-154-276-193-73-256-90-503-425-549-2-395-74-235-514-339-303-141-254-124-516-106-407-457-345-540-510-272-418-244-322-201-175-399-70-76-128-544-284-23-116-93-429-307-311-458-302-423-369-26-38-441-249-30-89-536-545-25-404-62-46-147-115-138-498-380-555-323-511-546-466-287-151-320-471-553-55-71-266-373-4-416-439-101-75-72-465-231-239-338-105-491-182-376-420-226-501-102-384-346-126-502-155-56-67-59-543-456-367-211-424-216-487-37-41-288-32-162-450-422-479-542-170-504-107-279-48-96-298-386-251-312-208-485-92-97-391-213-280-181-83-65-452-188-204-294-99-217-187-512-12-82-482-454-143-136-282-433-114-225-336-206-103-35-271-297-161-14-477-537-171-242-108-290-464-523-430-36-257-166-3-402-354-133-80-173-308-462-406-396-135-125-397-84-39-517-66-78-368-85-9-223-6-447-275-234-134-372-324-362-412-438-265-348-168-158-258-209-327-355-390-532-117-68-195-185-463-427-472-351-184-86-490-178-440-434-317-484-263-24-364-497-357-304-189-152-81-172-227-525-426-299-519-506-109-140-340-252-255-550-295-526-379-518-237-417-455-474-352-435-169-267-278-230-241-489-392-437-382-196-79-214-453-505-50-486-100-313-329-326-449-45-137-551-509-309-203-121-183-291-557-534-499-199-374-111-215-177-273-91-148-27-58-246-218-415-95-200-300-15-5-451-270-10-469-524-301-69-163-296-388-260-289-366-146-243-529-387-459-292-359-353-408-139-1-460-197-174-202-142-375-157-428-310-370-521-149-507-531-34-314-528-337-123-492-443-552-363-21-468-381-356-331-385-398-186-332-42-533-262-480-461-120-488-539-132-52 => 0.0001\n",
      "0 => 0.0000\n",
      "0-400-22-464-461-207-16-66-78-365-262-427-466-490-13-398-272-60-339-113-157-410-267-546-453-502-535-460-285-149-243-8-511-359-216-112-226-31-304-151-61-221-428-418-303-452-300-419-366-26-38-437-247-30-89-342-266-148-257-245-187-242-415-396-70-76-128-209-188-204-537-487-341-542-506-18-40-527-217-218-195-314-53-119-282-23-116-93-142-284-368-485-163-530-88-470-230-500-238-96-356-492-517-47-127-122-246-248-281-386-54-536-275-77-347-317-332-397-98-44-219-504-17-202-319-407-291-43-259-331-476-388-210-62-46-145-305-309-283-279-84-358-176-198-171-408-49-172-94-156-225-340-104-443-488-375-161-345-186-411-402-110-177-362-323-390-313-124-507-106-404-451-63-57-233-468-482-429-114-222-334-203-103-35-83-432-474-316-544-512-107-496-192-87-229-118-510-297-321-543-377-412-255-162-3-399-352-131-130-51-520-328-333-235-239-227-165-467-360-409-434-269-343-529-501-270-254-90-495-421-538-2-392-74-232-505-337-301-139-252-25-401-9-220-55-191-391-438-278-178-450-364-208-420-213-480-37-41-29-416-471-526-167-240-108-288-439-129-251-441-374-469-11-20-326-440-524-523-295-158-14-486-143-472-413-4-370-264-71-423-310-205-478-92-97-363-287-150-185-302-355-489-19-417-223-493-102-381-344-126-494-153-56-67-59-532-406-348-422-515-224-168-159-101-435-455-531-473-184-503-12-277-48-201-292-99-214-307-296-383-249-228-237-336-105-483-179-373-175-32-516-393-133-125-394-445-138-338-72-459-109-349-181-86-190-274-152-7-39-508-182-457-231-273-442-497-320-6-64-256-206-136-491-155-465-318-322-140-134-141-448-475-446-289-540-444-45-425-380-15-449-414-234-306-169-80-10-463-73-121-65-82-180-166-534-525-315-477-261-24-361-164-346-81-541-539-293-389-458-513-426-36-353-387-28-137-1-454-91-479-325-200-403-456-286-509-376-5-170-194-481-405-144-68-117-430-33-436-385-258-199-132-369-350-431-276-100-324-499-160-521-193-79-211-447-50-433-379-253-250-311-268-263-545-498-196-514-330-183-21-395-75-244-173-212-146-522-135-271-280-424-372-294-111-85-154-27-308-367-371-115-357-58-42-197-215-351-384-519-241-174-265-290-69-189-298-236-329-382-378-462-123-147-484-34-354-312-518-260-95-335-528-533-299-327-120-52 => -0.0000\n",
      "0-408-22-476-473-214-16-130-182-365-33-63-57-240-250-8-526-366-223-113-233-31-311-154-61-228-437-426-310-161-418-274-565-13-406-279-60-346-114-19-320-398-166-53-120-211-554-501-348-561-521-18-40-543-224-225-202-321-388-502-383-167-352-193-419-410-111-183-370-330-194-252-264-151-273-349-195-216-363-506-533-47-128-123-253-255-288-394-54-553-282-77-354-324-339-405-99-44-226-519-17-209-326-415-298-43-266-338-489-482-89-546-169-499-376-291-145-64-441-487-323-563-528-178-95-160-232-347-105-453-414-355-198-399-447-436-269-157-28-29-424-479-416-49-132-51-536-335-340-242-119-236-88-484-146-500-449-131-258-451-382-481-11-20-333-450-540-290-286-245-514-237-7-155-281-197-73-261-91-509-430-555-2-400-74-239-520-344-308-142-259-125-522-107-412-462-350-545-516-277-423-249-327-205-177-404-70-76-129-550-289-23-117-94-434-312-316-463-307-428-374-26-38-446-254-30-90-541-551-25-409-62-46-148-116-139-504-385-562-328-517-552-472-292-152-325-477-560-55-71-271-378-4-421-444-102-75-72-470-235-244-343-106-497-185-381-425-230-507-103-389-351-127-508-156-56-67-59-549-461-372-215-429-220-493-37-41-293-32-163-455-427-485-547-172-510-108-284-48-97-303-391-256-317-212-491-93-98-396-217-285-184-83-65-457-191-208-299-100-221-190-518-12-82-488-459-144-137-287-438-115-229-341-210-104-35-276-302-162-14-483-542-173-247-109-295-469-529-435-36-262-168-3-407-359-134-80-175-313-467-411-401-136-126-402-85-39-523-66-78-373-86-9-227-6-452-280-238-135-377-329-367-417-443-270-353-170-159-263-213-332-360-395-187-87-496-181-445-439-322-356-478-432-460-480-357-440-171-272-525-304-309-192-153-81-174-231-531-431-164-503-362-369-24-268-490-10-475-530-110-141-345-257-260-556-300-532-384-524-241-422-15-5-314-207-122-199-188-468-515-558-454-45-138-219-179-278-92-149-331-296-186-564-118-68-147-505-203-379-112-548-539-557-397-442-234-246-495-492-200-79-218-458-511-387-101-50-456-275-318-512-464-243-301-393-265-294-371-222-248-535-392-306-251-96-334-204-420-27-58-283-69-305-364-358-297-140-1-465-413-201-176-206-143-380-158-433-315-375-527-150-165-513-537-34-319-534-342-124-498-448-559-368-21-84-474-386-361-336-390-403-189-337-42-538-267-486-180-196-466-121-494-471-544-133-52 => -0.0000\n",
      "0-406-22-474-471-214-16-130-182-364-33-63-57-240-250-8-524-365-223-113-233-31-311-154-61-228-435-424-310-161-416-274-564-13-404-279-60-345-114-19-320-396-166-53-120-211-553-499-347-560-519-18-40-542-224-225-202-321-386-500-381-167-351-193-417-408-111-183-368-330-194-252-264-151-273-348-195-216-362-504-531-47-128-123-253-255-288-392-54-552-282-77-353-324-338-403-99-44-226-517-17-209-326-413-298-43-266-337-487-480-89-545-169-497-374-291-145-64-439-485-323-562-526-178-95-160-232-346-105-451-412-354-198-397-446-434-269-157-28-29-422-477-414-49-132-51-534-335-339-242-119-236-88-482-146-498-447-131-258-449-380-479-11-20-333-448-539-290-286-245-512-237-7-155-281-197-73-261-91-507-428-554-2-398-74-239-518-343-308-142-259-125-520-107-410-460-349-544-514-277-421-249-327-205-177-402-70-76-129-549-289-23-117-94-432-312-316-461-307-426-372-26-38-445-254-30-90-540-550-25-407-62-46-148-116-139-502-383-561-328-515-551-470-292-152-325-475-559-55-71-271-376-4-419-443-102-75-72-468-235-244-342-106-495-185-379-423-230-505-103-387-350-127-506-156-56-67-59-548-459-370-215-427-220-491-37-41-293-32-163-453-425-483-546-172-508-108-284-48-97-303-389-256-317-212-489-93-98-394-217-285-184-83-65-455-191-208-299-100-221-190-516-12-82-486-457-144-137-287-436-115-229-340-210-104-35-276-302-162-14-481-541-173-247-109-295-467-527-433-36-262-168-3-405-358-134-80-175-313-465-409-399-136-126-400-85-39-521-66-78-371-86-9-227-6-450-280-238-135-375-329-366-415-441-270-352-170-159-263-213-332-359-393-536-118-68-199-188-466-430-476-355-187-87-494-181-444-437-322-488-268-24-367-501-361-309-192-153-81-174-231-529-429-304-523-510-110-141-344-257-260-555-300-530-382-522-241-420-458-478-356-438-171-272-283-234-246-493-395-440-385-200-79-218-456-509-50-490-101-318-334-331-452-45-138-557-513-314-207-122-186-296-563-556-538-503-203-377-112-547-219-179-278-92-149-27-58-251-222-418-96-204-305-15-5-454-275-10-473-528-306-69-164-301-391-265-294-369-147-248-533-390-462-243-297-363-357-411-140-1-463-201-176-206-143-378-158-431-315-373-525-150-165-511-535-34-319-532-341-124-496-21-84-472-384-360-388-401-189-336-42-537-442-558-267-484-180-196-464-121-492-469-543-133-52 => -0.0000\n",
      "0-406-22-473-470-213-16-130-181-364-33-63-57-239-249-8-523-365-222-113-232-31-310-153-61-227-435-424-309-160-416-273-563-13-404-278-60-345-114-19-319-396-165-53-120-210-552-498-347-559-518-18-40-541-223-224-201-320-386-499-381-166-351-192-417-408-111-182-368-329-193-251-263-150-272-348-194-215-362-503-530-47-128-123-252-254-287-392-54-551-281-77-353-323-338-403-99-44-225-516-17-208-325-413-297-43-265-337-486-479-89-544-168-496-374-290-144-64-439-484-322-561-525-177-95-159-231-346-105-450-412-354-197-397-445-434-268-156-28-29-422-476-414-49-132-51-533-334-339-241-119-235-88-481-145-497-446-131-257-448-380-478-11-20-332-447-538-289-285-244-511-236-7-154-280-196-73-260-91-506-428-553-2-398-74-238-517-343-307-141-258-125-519-107-410-459-349-543-513-276-421-248-326-204-176-402-70-76-129-548-288-23-117-94-432-311-315-460-306-426-372-26-38-444-253-30-90-539-549-25-407-62-46-147-116-138-501-383-560-327-514-550-469-291-151-324-474-558-55-71-270-376-4-419-442-102-75-72-467-234-243-342-106-494-184-379-423-229-504-103-387-350-127-505-155-56-67-59-547-458-370-214-427-219-490-37-41-292-32-162-452-425-482-545-171-507-108-283-48-97-302-389-255-316-211-488-93-98-394-216-284-183-83-65-454-190-207-298-100-220-189-515-12-82-485-456-143-136-286-436-115-228-340-209-104-35-275-301-161-14-480-540-172-246-109-294-466-526-433-36-261-167-3-405-358-133-80-174-312-464-409-399-135-126-400-85-39-520-66-78-371-86-9-226-6-449-279-237-134-375-328-366-415-441-269-352-169-158-262-212-331-359-393-535-118-68-198-187-465-430-475-355-186-87-493-180-443-437-321-487-267-24-367-500-361-308-191-152-81-173-230-528-429-303-522-509-110-140-344-256-259-554-299-529-382-521-240-420-457-477-356-438-170-271-282-233-245-492-395-440-385-199-79-217-455-508-50-489-101-317-333-330-451-45-137-556-512-313-206-122-185-295-562-555-537-502-202-377-112-546-218-178-277-92-148-27-58-250-221-418-96-203-304-15-5-453-274-10-472-527-305-69-163-300-391-264-293-369-146-247-532-390-461-242-296-363-357-411-139-1-462-200-175-205-142-378-157-431-314-373-524-149-164-510-534-34-318-531-341-124-495-21-84-471-384-360-335-388-401-188-336-42-536-557-266-483-179-195-463-121-491-468-542-52 => -0.0001\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "start = time.time()\n",
    "num_nodes = data.num_nodes\n",
    "src_idx = [random.randint(0, num_nodes) for _ in range(10)]\n",
    "dst_idx = []\n",
    "for i in range(10):\n",
    "    dst_idx.append(random.choice(list(set([x for x in range(num_nodes)]) - set([src_idx[i]]))))\n",
    "idx = (torch.tensor(src_idx), torch.tensor(dst_idx))\n",
    "shap_vals = shapley_values(\n",
    "    model=model,\n",
    "    data=data,\n",
    "    idx=idx,\n",
    "    unique_ctree_codes=unique_ctree_codes,\n",
    "    node_ctree_codes=node_ctree_codes,\n",
    "    target_class=None,  # or an int specifying which class\n",
    "    num_samples=50\n",
    ")\n",
    "\n",
    "# Sort Shapley results\n",
    "sorted_shap = sorted(shap_vals.items(), key=lambda x: x[1], reverse=True)\n",
    "print(f\"total time taken: {time.time()-start}\")\n",
    "print(\"\\nTop-10 Concepts by Shapley Value:\")\n",
    "for c, val in sorted_shap[:10]:\n",
    "    print(f\"{c} => {val:.4f}\")\n",
    "\n",
    "# Done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Symbolic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using features ['x0' 'x1']\n",
      "Compiling Julia backend...\n",
      "[ Info: Note: you are running with more than 10,000 datapoints. You should consider turning on batching (`options.batching`), and also if you need that many datapoints. Unless you have a large amount of noise (in which case you should smooth your dataset first), generally < 10,000 datapoints is enough to find a functional form.\n",
      "[ Info: Started!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expressions evaluated per second: 1.200e+04\n",
      "Progress: 77 / 3100 total iterations (2.484%)\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Complexity  Loss       Score      Equation\n",
      "1           7.344e-01  1.594e+01  y = 1\n",
      "3           7.340e-01  3.136e-04  y = And(x1, x0)\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Expressions evaluated per second: 1.080e+04\n",
      "Progress: 137 / 3100 total iterations (4.419%)\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Complexity  Loss       Score      Equation\n",
      "1           7.344e-01  1.594e+01  y = 1\n",
      "3           7.340e-01  3.136e-04  y = And(x1, x0)\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Expressions evaluated per second: 9.020e+03\n",
      "Progress: 170 / 3100 total iterations (5.484%)\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Complexity  Loss       Score      Equation\n",
      "1           7.344e-01  1.594e+01  y = 1\n",
      "3           7.340e-01  3.136e-04  y = And(x1, x0)\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Expressions evaluated per second: 7.890e+03\n",
      "Progress: 198 / 3100 total iterations (6.387%)\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Complexity  Loss       Score      Equation\n",
      "1           7.344e-01  1.594e+01  y = 1\n",
      "3           7.340e-01  3.136e-04  y = And(x1, x0)\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Expressions evaluated per second: 7.000e+03\n",
      "Progress: 224 / 3100 total iterations (7.226%)\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Complexity  Loss       Score      Equation\n",
      "1           7.344e-01  1.594e+01  y = 1\n",
      "3           7.340e-01  3.136e-04  y = And(x1, x0)\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Expressions evaluated per second: 5.410e+03\n",
      "Progress: 259 / 3100 total iterations (8.355%)\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Complexity  Loss       Score      Equation\n",
      "1           7.344e-01  1.594e+01  y = 1\n",
      "3           7.340e-01  3.136e-04  y = And(x1, x0)\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Expressions evaluated per second: 5.200e+03\n",
      "Progress: 294 / 3100 total iterations (9.484%)\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Complexity  Loss       Score      Equation\n",
      "1           7.344e-01  1.594e+01  y = 1\n",
      "3           7.340e-01  3.136e-04  y = And(x1, x0)\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Expressions evaluated per second: 5.320e+03\n",
      "Progress: 323 / 3100 total iterations (10.419%)\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Complexity  Loss       Score      Equation\n",
      "1           7.344e-01  1.594e+01  y = 1\n",
      "3           7.340e-01  3.136e-04  y = And(x1, x0)\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Expressions evaluated per second: 5.180e+03\n",
      "Progress: 343 / 3100 total iterations (11.065%)\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Complexity  Loss       Score      Equation\n",
      "1           7.344e-01  1.594e+01  y = 1\n",
      "3           7.340e-01  3.136e-04  y = And(x1, x0)\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Expressions evaluated per second: 4.870e+03\n",
      "Progress: 360 / 3100 total iterations (11.613%)\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Complexity  Loss       Score      Equation\n",
      "1           7.344e-01  1.594e+01  y = 1\n",
      "3           7.340e-01  3.136e-04  y = And(x1, x0)\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Expressions evaluated per second: 3.930e+03\n",
      "Progress: 383 / 3100 total iterations (12.355%)\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Complexity  Loss       Score      Equation\n",
      "1           7.344e-01  1.594e+01  y = 1\n",
      "3           7.340e-01  3.136e-04  y = And(x1, x0)\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Expressions evaluated per second: 3.680e+03\n",
      "Progress: 405 / 3100 total iterations (13.065%)\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Complexity  Loss       Score      Equation\n",
      "1           7.344e-01  1.594e+01  y = 1\n",
      "3           7.340e-01  3.136e-04  y = And(x1, x0)\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Expressions evaluated per second: 3.630e+03\n",
      "Progress: 427 / 3100 total iterations (13.774%)\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Complexity  Loss       Score      Equation\n",
      "1           7.344e-01  1.594e+01  y = 1\n",
      "3           7.340e-01  3.136e-04  y = And(x1, x0)\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Expressions evaluated per second: 3.520e+03\n",
      "Progress: 444 / 3100 total iterations (14.323%)\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Complexity  Loss       Score      Equation\n",
      "1           7.344e-01  1.594e+01  y = 1\n",
      "3           7.340e-01  3.136e-04  y = And(x1, x0)\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Expressions evaluated per second: 3.780e+03\n",
      "Progress: 467 / 3100 total iterations (15.065%)\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Complexity  Loss       Score      Equation\n",
      "1           7.344e-01  1.594e+01  y = 1\n",
      "3           7.340e-01  3.136e-04  y = And(x1, x0)\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Expressions evaluated per second: 3.650e+03\n",
      "Progress: 484 / 3100 total iterations (15.613%)\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Complexity  Loss       Score      Equation\n",
      "1           7.344e-01  1.594e+01  y = 1\n",
      "3           7.340e-01  3.136e-04  y = And(x1, x0)\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Expressions evaluated per second: 3.660e+03\n",
      "Progress: 506 / 3100 total iterations (16.323%)\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Complexity  Loss       Score      Equation\n",
      "1           7.344e-01  1.594e+01  y = 1\n",
      "3           7.340e-01  3.136e-04  y = And(x1, x0)\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Expressions evaluated per second: 4.120e+03\n",
      "Progress: 542 / 3100 total iterations (17.484%)\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Complexity  Loss       Score      Equation\n",
      "1           7.344e-01  1.594e+01  y = 1\n",
      "3           7.340e-01  3.136e-04  y = And(x1, x0)\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Expressions evaluated per second: 5.480e+03\n",
      "Progress: 601 / 3100 total iterations (19.387%)\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Complexity  Loss       Score      Equation\n",
      "1           7.344e-01  1.594e+01  y = 1\n",
      "3           7.340e-01  3.136e-04  y = And(x1, x0)\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Expressions evaluated per second: 6.740e+03\n",
      "Progress: 651 / 3100 total iterations (21.000%)\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Complexity  Loss       Score      Equation\n",
      "1           7.344e-01  1.594e+01  y = 1\n",
      "3           7.340e-01  3.136e-04  y = And(x1, x0)\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Expressions evaluated per second: 7.090e+03\n",
      "Progress: 690 / 3100 total iterations (22.258%)\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Complexity  Loss       Score      Equation\n",
      "1           7.344e-01  1.594e+01  y = 1\n",
      "3           7.340e-01  3.136e-04  y = And(x1, x0)\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Expressions evaluated per second: 7.170e+03\n",
      "Progress: 718 / 3100 total iterations (23.161%)\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Complexity  Loss       Score      Equation\n",
      "1           7.344e-01  1.594e+01  y = 1\n",
      "3           7.340e-01  3.136e-04  y = And(x1, x0)\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Expressions evaluated per second: 6.550e+03\n",
      "Progress: 747 / 3100 total iterations (24.097%)\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Complexity  Loss       Score      Equation\n",
      "1           7.344e-01  1.594e+01  y = 1\n",
      "3           7.340e-01  3.136e-04  y = And(x1, x0)\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Expressions evaluated per second: 6.080e+03\n",
      "Progress: 783 / 3100 total iterations (25.258%)\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Complexity  Loss       Score      Equation\n",
      "1           7.344e-01  1.594e+01  y = 1\n",
      "3           7.340e-01  3.136e-04  y = And(x1, x0)\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Expressions evaluated per second: 5.900e+03\n",
      "Progress: 822 / 3100 total iterations (26.516%)\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Complexity  Loss       Score      Equation\n",
      "1           7.344e-01  1.594e+01  y = 1\n",
      "3           7.340e-01  3.136e-04  y = And(x1, x0)\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Expressions evaluated per second: 6.300e+03\n",
      "Progress: 867 / 3100 total iterations (27.968%)\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Complexity  Loss       Score      Equation\n",
      "1           7.344e-01  1.594e+01  y = 1\n",
      "3           7.340e-01  3.136e-04  y = And(x1, x0)\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Press 'q' and then <enter> to stop execution early.\n"
     ]
    }
   ],
   "source": [
    "pysr_weights = prob\n",
    "argsc = 1\n",
    "argsk = 200\n",
    "\n",
    "\n",
    "# * ----- Symbolic Regression\n",
    "start_time = process_time()\n",
    "\n",
    "pysrmodel = PySRRegressor(\n",
    "    unary_operators = [\"Not(x) = (x <= zero(x)) * one(x)\"],\n",
    "    binary_operators = [\n",
    "        \"And(x, y) = ((x > zero(x)) & (y > zero(y))) * one(x)\",\n",
    "        \"Or(x, y)  = ((x > zero(x)) | (y > zero(y))) * one(x)\",\n",
    "        \"Xor(x, y) = (((x > 0) & (y <= 0)) | ((x <= 0) & (y > 0))) * 1f0\",\n",
    "    ],\n",
    "    extra_sympy_mappings = {\n",
    "        \"Not\": lambda x: sympy.Piecewise((1.0, (x <= 0)), (0.0, True)),\n",
    "        \"And\": lambda x, y: sympy.Piecewise((1.0, (x > 0) & (y > 0)), (0.0, True)),\n",
    "        \"Or\":  lambda x, y: sympy.Piecewise((1.0, (x > 0) | (y > 0)), (0.0, True)),\n",
    "        \"Xor\": lambda x, y: sympy.Piecewise((1.0, (x > 0) ^ (y > 0)), (0.0, True)),\n",
    "    },\n",
    "\n",
    "    elementwise_loss = \"loss(prediction, target) = sum(prediction != target)\",\n",
    "    model_selection=\"accuracy\",\n",
    "\n",
    "    complexity_of_variables=argsc,\n",
    "    complexity_of_operators={'Not': argsc, 'And': argsc, 'Or': argsc, 'Xor': argsc},\n",
    "\n",
    "    select_k_features = min(argsk, 10),\n",
    "    weights = pysr_weights,\n",
    "\n",
    "    batch_size = 32,\n",
    "\n",
    "    # Paperwork\n",
    "    temp_equation_file = True,\n",
    "    delete_tempfiles = True,\n",
    "\n",
    "    # Determinism\n",
    "    procs=0,\n",
    "    deterministic=True,\n",
    "    multithreading=False,\n",
    "    random_state=0,\n",
    "    warm_start=False,\n",
    ")\n",
    "\n",
    "def cal_pysr_acc(X, Y, index=None):\n",
    "    Y = np.array(Y)\n",
    "    Y_pred = pysrmodel.predict(X, index=index)\n",
    "    assert Y.shape == Y_pred.shape , \"Shape mismatch!\"\n",
    "    return (Y_pred == Y).sum() / len(Y)\n",
    "\n",
    "pysrmodel.fit(x_train_bin, train_pred)\n",
    "print(pysrmodel)\n",
    "\n",
    "selected_ctrees = pysrmodel.selection_mask_\n",
    "\n",
    "df_equations = pysrmodel.equations.drop([\"sympy_format\", \"lambda_format\"], axis=1)\n",
    "# Add a column for accuracy.\n",
    "df_equations[\"acc\"] = 1 - df_equations[\"loss\"]\n",
    "# Re-arrange columns to have \"acc\" as the second column.\n",
    "cols = df_equations.columns.tolist()\n",
    "cols.insert(1, cols.pop(-1))\n",
    "df_equations = df_equations[cols]\n",
    "# Round values.\n",
    "for col in [\"acc\", \"loss\", \"score\"]:\n",
    "    df_equations[col] = df_equations[col].round(4)\n",
    "\n",
    "\n",
    "# Find the equation that performs the best on the validation set\n",
    "best_val_acc = 0\n",
    "print(\"\\nValidation accuracies:\")\n",
    "for j in range(pysrmodel.equations_.shape[0]):\n",
    "    # PySR sometimes fails to evaluate certain formulae\n",
    "    # it usually happens when C is set to a small value.\n",
    "    # We've been unable to identify when and why it happens\n",
    "    try:\n",
    "        __ = pysrmodel.predict(x_train_bin, index=j)\n",
    "        __ = pysrmodel.predict(x_test_bin, index=j)\n",
    "        pysr_val_pred = pysrmodel.predict(x_val_bin, index=j)\n",
    "    except ValueError:\n",
    "        print(f\"{j}: failed\")\n",
    "        continue\n",
    "    val_acc = (pysr_val_pred == val_pred).sum() / len(val_pred)\n",
    "    print(f\"{j}: {val_acc}\")\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_index = j\n",
    "print(\"Best equation index:\", best_index)\n",
    "\n",
    "\n",
    "# * ----- Metrics\n",
    "pysr_train_pred = pysrmodel.predict(x_train_bin, index=best_index)\n",
    "pysr_test_pred = pysrmodel.predict(x_test_bin, index=best_index)\n",
    "pysr_train_pred = torch.LongTensor(pysr_train_pred)\n",
    "pysr_test_pred = torch.LongTensor(pysr_test_pred)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Best\n",
    "equation = pysrmodel.get_best().equation\n",
    "print(f\"\\nName:{args.name} - Seed:{args.seed} - Size:{args.size}\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Equation:\", equation)\n",
    "\n",
    "train_acc = round(cal_pysr_acc(x_train_bin, train_pred), 3)\n",
    "test_acc  = round(cal_pysr_acc(x_test_bin, test_pred), 3)\n",
    "\n",
    "equation = utils.simplify_expression(equation)\n",
    "print(\"Simplified equation:\", equation)\n",
    "print(\"Train accuracy:\", train_acc)\n",
    "print(\"Test accuracy:\", test_acc)\n",
    "\"\"\"\n",
    "\n",
    "# Best based on val set\n",
    "equation = pysrmodel.get_best(index=best_index).equation\n",
    "print()\n",
    "print(\"=\" * 50)\n",
    "print(\"Equation:\", equation)\n",
    "print(\"C =\", 1)\n",
    "\n",
    "train_acc = round(cal_pysr_acc(x_train_bin, train_pred, index=best_index), 3)\n",
    "test_acc  = round(cal_pysr_acc(x_test_bin, test_pred, index=best_index), 3)\n",
    "\n",
    "equation = utils.simplify_expression(equation)\n",
    "print(\"Simplified equation:\", equation)\n",
    "print(\"Train accuracy:\", train_acc)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
