{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.3.1 in /Users/tyler/anaconda3/lib/python3.12/site-packages (2.3.1)\n",
      "Requirement already satisfied: filelock in /Users/tyler/anaconda3/lib/python3.12/site-packages (from torch==2.3.1) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from torch==2.3.1) (4.11.0)\n",
      "Requirement already satisfied: sympy in /Users/tyler/anaconda3/lib/python3.12/site-packages (from torch==2.3.1) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/tyler/anaconda3/lib/python3.12/site-packages (from torch==2.3.1) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from torch==2.3.1) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/tyler/anaconda3/lib/python3.12/site-packages (from torch==2.3.1) (2024.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from jinja2->torch==2.3.1) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from sympy->torch==2.3.1) (1.3.0)\n",
      "Requirement already satisfied: torch_geometric==2.6.1 in /Users/tyler/anaconda3/lib/python3.12/site-packages (2.6.1)\n",
      "Requirement already satisfied: aiohttp in /Users/tyler/anaconda3/lib/python3.12/site-packages (from torch_geometric==2.6.1) (3.10.5)\n",
      "Requirement already satisfied: fsspec in /Users/tyler/anaconda3/lib/python3.12/site-packages (from torch_geometric==2.6.1) (2024.6.1)\n",
      "Requirement already satisfied: jinja2 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from torch_geometric==2.6.1) (3.1.4)\n",
      "Requirement already satisfied: numpy in /Users/tyler/anaconda3/lib/python3.12/site-packages (from torch_geometric==2.6.1) (1.26.4)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from torch_geometric==2.6.1) (5.9.0)\n",
      "Requirement already satisfied: pyparsing in /Users/tyler/anaconda3/lib/python3.12/site-packages (from torch_geometric==2.6.1) (3.1.2)\n",
      "Requirement already satisfied: requests in /Users/tyler/anaconda3/lib/python3.12/site-packages (from torch_geometric==2.6.1) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /Users/tyler/anaconda3/lib/python3.12/site-packages (from torch_geometric==2.6.1) (4.66.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from aiohttp->torch_geometric==2.6.1) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from aiohttp->torch_geometric==2.6.1) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from aiohttp->torch_geometric==2.6.1) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from aiohttp->torch_geometric==2.6.1) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from aiohttp->torch_geometric==2.6.1) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from aiohttp->torch_geometric==2.6.1) (1.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from jinja2->torch_geometric==2.6.1) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from requests->torch_geometric==2.6.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from requests->torch_geometric==2.6.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from requests->torch_geometric==2.6.1) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from requests->torch_geometric==2.6.1) (2025.1.31)\n",
      "Data(x=[572, 128], edge_index=[2, 52112], edge_type=[52112])\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.3.1\n",
    "!pip install torch_geometric==2.6.1\n",
    "import torch\n",
    "# Load the tensor and map it to the CPU\n",
    "data = torch.load('data_o_new2.pt', map_location=torch.device('cpu'))\n",
    "\n",
    "# Print the shape of the tensor\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MRCGNN Revised Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.3.1\n",
      "  Downloading torch-2.3.1-cp312-none-macosx_11_0_arm64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: filelock in /Users/tyler/anaconda3/lib/python3.12/site-packages (from torch==2.3.1) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from torch==2.3.1) (4.11.0)\n",
      "Requirement already satisfied: sympy in /Users/tyler/anaconda3/lib/python3.12/site-packages (from torch==2.3.1) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/tyler/anaconda3/lib/python3.12/site-packages (from torch==2.3.1) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from torch==2.3.1) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/tyler/anaconda3/lib/python3.12/site-packages (from torch==2.3.1) (2024.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from jinja2->torch==2.3.1) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from sympy->torch==2.3.1) (1.3.0)\n",
      "Downloading torch-2.3.1-cp312-none-macosx_11_0_arm64.whl (61.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.5.1\n",
      "    Uninstalling torch-2.5.1:\n",
      "      Successfully uninstalled torch-2.5.1\n",
      "Successfully installed torch-2.3.1\n",
      "Requirement already satisfied: torch_geometric==2.6.1 in /Users/tyler/anaconda3/lib/python3.12/site-packages (2.6.1)\n",
      "Requirement already satisfied: aiohttp in /Users/tyler/anaconda3/lib/python3.12/site-packages (from torch_geometric==2.6.1) (3.10.5)\n",
      "Requirement already satisfied: fsspec in /Users/tyler/anaconda3/lib/python3.12/site-packages (from torch_geometric==2.6.1) (2024.6.1)\n",
      "Requirement already satisfied: jinja2 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from torch_geometric==2.6.1) (3.1.4)\n",
      "Requirement already satisfied: numpy in /Users/tyler/anaconda3/lib/python3.12/site-packages (from torch_geometric==2.6.1) (1.26.4)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from torch_geometric==2.6.1) (5.9.0)\n",
      "Requirement already satisfied: pyparsing in /Users/tyler/anaconda3/lib/python3.12/site-packages (from torch_geometric==2.6.1) (3.1.2)\n",
      "Requirement already satisfied: requests in /Users/tyler/anaconda3/lib/python3.12/site-packages (from torch_geometric==2.6.1) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /Users/tyler/anaconda3/lib/python3.12/site-packages (from torch_geometric==2.6.1) (4.66.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from aiohttp->torch_geometric==2.6.1) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from aiohttp->torch_geometric==2.6.1) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from aiohttp->torch_geometric==2.6.1) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from aiohttp->torch_geometric==2.6.1) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from aiohttp->torch_geometric==2.6.1) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from aiohttp->torch_geometric==2.6.1) (1.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from jinja2->torch_geometric==2.6.1) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from requests->torch_geometric==2.6.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from requests->torch_geometric==2.6.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from requests->torch_geometric==2.6.1) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from requests->torch_geometric==2.6.1) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import RGCNConv\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, n_h):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.f_k = nn.Bilinear(32, 32, 1)\n",
    "        for m in self.modules():\n",
    "            self.weights_init(m)\n",
    "    def weights_init(self, m):\n",
    "        if isinstance(m, nn.Bilinear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.fill_(0.0)\n",
    "    def forward(self, c, h_pl, h_mi, s_bias1=None, s_bias2=None):\n",
    "        c_x = c.expand_as(h_pl)\n",
    "        sc_1 = self.f_k(h_pl, c_x)\n",
    "        sc_2 = self.f_k(h_mi, c_x)\n",
    "        if s_bias1 is not None:\n",
    "            sc_1 += s_bias1\n",
    "        if s_bias2 is not None:\n",
    "            sc_2 += s_bias2\n",
    "        logits = torch.cat((sc_1, sc_2), 1)\n",
    "        return logits\n",
    "\n",
    "class AvgReadout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AvgReadout, self).__init__()\n",
    "    def forward(self, seq, msk=None):\n",
    "        if msk is None:\n",
    "            return torch.mean(seq, 0)\n",
    "        else:\n",
    "            msk = torch.unsqueeze(msk, -1)\n",
    "            return torch.sum(seq * msk, 0) / torch.sum(msk)\n",
    "\n",
    "class MRCGNN(nn.Module):\n",
    "    def __init__(self, feature, hidden1, hidden2, decoder1, dropout, zhongzi):\n",
    "        super(MRCGNN, self).__init__()\n",
    "\n",
    "        # RGCN layers for the main (data_o) branch\n",
    "        self.encoder_o1 = RGCNConv(feature, hidden1, num_relations=65)\n",
    "        self.encoder_o2 = RGCNConv(hidden1, hidden2, num_relations=65)\n",
    "\n",
    "        # Two-element parameter for layer attention\n",
    "        self.attt = nn.Parameter(torch.tensor([0.5, 0.5]))\n",
    "        self.disc = Discriminator(hidden2 * 2)\n",
    "        self.dropout = dropout\n",
    "        self.sigm = nn.Sigmoid()\n",
    "        self.read = AvgReadout()\n",
    "        \n",
    "        # Final classifier: prediction solely from data_o branch.\n",
    "        # Each node's final representation is a concatenation of (hidden1 + hidden2).\n",
    "        # For a pair of entities, the dimension becomes 2*(hidden1+hidden2).\n",
    "        self.classifier = nn.Linear(2 * (hidden1 + hidden2), 65)\n",
    "\n",
    "        # We no longer load any pretrained features for skip connection.\n",
    "\n",
    "    def forward(self, data_o, data_s, data_a, idx):\n",
    "        # Process data_o branch\n",
    "        x_o, adj, e_type = data_o.x, data_o.edge_index, data_o.edge_type\n",
    "        e_type1 = data_a.edge_type\n",
    "        e_type = torch.tensor(e_type, dtype=torch.int64)\n",
    "        e_type1 = torch.tensor(e_type1, dtype=torch.int64)\n",
    "\n",
    "        # Main branch for prediction (data_o)\n",
    "        x1_o = F.relu(self.encoder_o1(x_o, adj, e_type))\n",
    "        x1_o = F.dropout(x1_o, self.dropout, training=self.training)\n",
    "        x2_o = self.encoder_o2(x1_o, adj, e_type)\n",
    "\n",
    "        # Contrastive learning branches (unused in prediction)\n",
    "        x_a = data_s.x\n",
    "        x1_o_a = F.relu(self.encoder_o1(x_a, adj, e_type))\n",
    "        x1_o_a = F.dropout(x1_o_a, self.dropout, training=self.training)\n",
    "        x2_o_a = self.encoder_o2(x1_o_a, adj, e_type)\n",
    "\n",
    "        x1_o_a_a = F.relu(self.encoder_o1(x_o, adj, e_type1))\n",
    "        x1_o_a_a = F.dropout(x1_o_a_a, self.dropout, training=self.training)\n",
    "        x2_o_a_a = self.encoder_o2(x1_o_a_a, adj, e_type1)\n",
    "\n",
    "        # Readout for contrastive learning\n",
    "        h_os = self.read(x2_o)\n",
    "        h_os = self.sigm(h_os)\n",
    "        ret_os = self.disc(h_os, x2_o, x2_o_a)\n",
    "        ret_os_a = self.disc(h_os, x2_o, x2_o_a_a)\n",
    "\n",
    "        # For final prediction, use only data_o branch:\n",
    "        final = torch.cat((self.attt[0] * x1_o, self.attt[1] * x2_o), dim=1)\n",
    "\n",
    "        a = [int(i) for i in list(idx[0])]\n",
    "        b = [int(i) for i in list(idx[1])]\n",
    "        aa = torch.tensor(a, dtype=torch.long)\n",
    "        bb = torch.tensor(b, dtype=torch.long)\n",
    "        entity1 = final[aa]\n",
    "        entity2 = final[bb]\n",
    "        concatenate = torch.cat((entity1, entity2), dim=1)\n",
    "        log = self.classifier(concatenate)\n",
    "\n",
    "        return log, ret_os, ret_os_a, x2_o\n",
    "\n",
    "    def predict(self, data_o, idx):\n",
    "        \"\"\"\n",
    "        New prediction method that uses only data_o and idx.\n",
    "        \"\"\"\n",
    "        x_o, adj, e_type = data_o.x, data_o.edge_index, data_o.edge_type\n",
    "        e_type = torch.tensor(e_type, dtype=torch.int64)\n",
    "        # Process the main branch\n",
    "        x1_o = F.relu(self.encoder_o1(x_o, adj, e_type))\n",
    "        x1_o = F.dropout(x1_o, self.dropout, training=self.training)\n",
    "        x2_o = self.encoder_o2(x1_o, adj, e_type)\n",
    "        final = torch.cat((self.attt[0] * x1_o, self.attt[1] * x2_o), dim=1)\n",
    "        \n",
    "        a = [int(i) for i in list(idx[0])]\n",
    "        b = [int(i) for i in list(idx[1])]\n",
    "        aa = torch.tensor(a, dtype=torch.long)\n",
    "        bb = torch.tensor(b, dtype=torch.long)\n",
    "        entity1 = final[aa]\n",
    "        entity2 = final[bb]\n",
    "        concatenate = torch.cat((entity1, entity2), dim=1)\n",
    "        log = self.classifier(concatenate)\n",
    "        return log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load My Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MRCGNN(\n",
       "  (encoder_o1): RGCNConv(128, 64, num_relations=65)\n",
       "  (encoder_o2): RGCNConv(64, 32, num_relations=65)\n",
       "  (disc): Discriminator(\n",
       "    (f_k): Bilinear(in1_features=32, in2_features=32, out_features=1, bias=True)\n",
       "  )\n",
       "  (sigm): Sigmoid()\n",
       "  (read): AvgReadout()\n",
       "  (classifier): Linear(in_features=192, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MRCGNN(feature=128, hidden1=64, hidden2=32, decoder1=512, dropout=0.5, zhongzi=0)\n",
    "model.load_state_dict(torch.load(\"model_mrcgnn.pt\", map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "model.load_state_dict(torch.load(\"model_mrcgnn.pt\", map_location='cpu'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data object: Data(x=[572, 128], edge_index=[2, 52112], edge_type=[52112])\n",
      "Number of unique ctree codes: 24\n",
      "Concept vector (frequency of each code):\n",
      "[522   5   4   1   9   9   1   1   2   1   1   1   1   1   3   1   1   1\n",
      "   1   1   1   1   1   2]\n",
      "Sample unique computation tree codes and frequencies:\n",
      "Code 0: 0-408-22-477-474-214-16-130-182-365-33-63-57-240-250-8-527-366-223-113-233-31-311-154-61-228-437-426-310-161-418-274-567-13-406-279-60-346-114-19-320-398-166-53-120-211-556-502-348-563-522-18-40-545-224-225-202-321-388-503-383-167-352-193-419-410-111-183-370-330-194-252-264-151-273-349-195-216-363-507-534-47-128-123-253-255-288-394-54-555-282-77-354-324-339-405-99-44-226-520-17-209-326-415-298-43-266-338-490-483-89-548-169-500-376-291-145-64-441-488-323-565-529-178-95-160-232-347-105-454-414-355-198-399-448-436-269-157-28-29-424-480-416-49-132-51-537-335-340-242-119-236-88-485-146-501-450-131-258-452-382-482-11-20-333-451-542-290-286-245-515-237-7-155-281-197-73-261-91-510-430-557-2-400-74-239-521-344-308-142-259-125-523-107-412-463-350-547-517-277-423-249-327-205-177-404-70-76-129-552-289-23-117-94-434-312-316-464-307-428-374-26-38-447-254-30-90-543-553-25-409-62-46-148-116-139-505-385-564-328-518-554-473-292-152-325-478-562-55-71-271-378-4-421-445-102-75-72-471-235-244-343-106-498-185-381-425-230-508-103-389-351-127-509-156-56-67-59-551-462-372-215-429-220-494-37-41-293-32-163-456-427-486-549-172-511-108-284-48-97-303-391-256-317-212-492-93-98-396-217-285-184-83-65-458-191-208-299-100-221-190-519-12-82-489-460-144-137-287-438-115-229-341-210-104-35-276-302-162-14-484-544-173-247-109-295-470-530-435-36-262-168-3-407-359-134-80-175-313-468-411-401-136-126-402-85-39-524-66-78-373-86-9-227-6-453-280-238-135-377-329-367-417-443-270-353-170-159-263-213-332-360-395-539-118-68-199-188-469-432-479-356-187-87-497-181-446-439-322-491-268-24-369-504-362-309-192-153-81-174-231-532-431-304-526-513-110-141-345-257-260-558-300-533-384-525-241-422-461-481-357-440-171-272-283-234-246-496-397-442-387-200-79-218-459-512-50-493-101-318-334-331-455-45-138-560-516-314-207-122-186-296-566-559-541-506-203-379-112-550-219-179-278-92-149-27-58-251-222-420-96-204-305-15-5-457-275-10-476-531-306-69-164-301-393-265-294-371-147-248-536-392-465-243-297-364-358-413-140-1-466-201-176-206-143-380-158-433-315-375-528-150-165-514-538-34-319-535-342-124-499-449-561-368-21-84-475-386-361-336-390-403-189-337-42-540-444-267-487-180-196-467-121-495-472-546-133-52\n",
      "Frequency: 522\n",
      "\n",
      "Code 1: 0-407-22-475-472-213-16-130-181-364-33-63-57-239-249-8-525-365-222-113-232-31-310-154-61-227-436-425-309-161-417-273-565-13-405-278-60-345-114-19-319-397-165-53-120-210-554-500-347-561-520-18-40-543-223-224-201-320-387-501-382-166-351-192-418-409-111-182-369-329-193-251-263-151-272-348-194-215-362-505-532-47-128-123-252-254-287-393-54-553-281-77-353-323-338-404-99-44-225-518-17-208-325-414-297-43-265-337-488-481-89-546-168-498-375-290-145-64-440-486-322-563-527-177-95-160-231-346-105-452-413-354-197-398-446-435-268-157-28-29-423-478-415-49-132-51-535-334-339-241-119-235-88-483-146-499-448-131-257-450-381-480-11-20-332-449-540-289-285-244-513-236-7-155-280-196-73-260-91-508-429-555-2-399-74-238-519-343-307-142-258-125-521-107-411-461-349-545-515-276-422-248-326-204-176-403-70-76-129-550-288-23-117-94-433-311-315-462-306-427-373-26-38-445-253-30-90-541-551-25-408-62-46-148-116-139-503-384-562-327-516-552-471-291-152-324-476-560-55-71-270-377-4-420-443-102-75-72-469-234-243-342-106-496-184-380-424-229-506-103-388-350-127-507-156-56-67-59-549-460-371-214-428-219-492-37-41-292-32-163-454-426-484-547-171-509-108-283-48-97-302-390-255-316-211-490-93-98-395-216-284-183-83-65-456-190-207-298-100-220-189-517-12-82-487-458-144-137-286-437-115-228-340-209-104-35-275-301-162-14-482-542-172-246-109-294-468-528-434-36-261-167-3-406-358-134-80-174-312-466-410-400-136-126-401-85-39-522-66-78-372-86-9-226-6-451-279-237-135-376-328-366-416-442-269-352-169-159-262-212-331-359-394-537-118-68-198-187-467-431-477-355-186-87-495-180-444-438-321-489-267-24-368-502-361-308-191-153-81-173-230-530-430-303-524-511-110-141-344-256-259-556-299-531-383-523-240-421-459-479-356-439-170-271-282-233-245-494-396-441-386-199-79-217-457-510-50-491-101-317-333-330-453-45-138-558-514-313-206-122-185-295-564-557-539-504-202-378-112-548-218-178-277-92-149-27-58-250-221-419-96-203-304-15-5-455-274-10-474-529-305-69-164-300-392-264-293-370-147-247-534-391-463-242-296-363-357-412-140-1-464-200-175-205-143-379-158-432-314-374-526-150-512-536-34-318-533-341-124-497-447-559-367-21-84-473-385-360-335-389-402-188-336-42-538-266-485-179-195-465-121-493-470-544-133-52\n",
      "Frequency: 5\n",
      "\n",
      "Code 2: 0\n",
      "Frequency: 4\n",
      "\n",
      "Code 3: 0-403-22-472-469-210-16-130-179-361-33-63-57-236-246-8-522-362-219-113-229-31-307-153-61-224-432-421-306-160-413-270-562-13-401-275-60-342-114-19-316-393-164-53-120-207-551-497-344-558-517-18-40-540-220-221-198-317-384-498-379-165-348-190-414-405-111-180-366-326-191-248-260-150-269-345-192-212-359-502-529-47-128-123-249-251-284-389-54-550-278-77-350-320-335-400-99-44-222-515-17-205-322-410-294-43-262-334-485-478-89-543-167-495-372-287-144-64-436-483-319-560-524-176-95-159-228-343-105-449-409-351-194-394-443-431-265-156-28-29-419-475-411-49-132-51-532-331-336-238-119-232-88-480-145-496-445-131-254-447-378-477-11-20-329-446-537-286-282-241-510-233-7-154-277-193-73-257-91-505-425-552-2-395-74-235-516-340-304-141-255-125-518-107-407-458-346-542-512-273-418-245-323-201-175-399-70-76-129-547-285-23-117-94-429-308-312-459-303-423-370-26-38-442-250-30-90-538-548-25-404-62-46-147-116-138-500-381-559-324-513-549-468-288-151-321-473-557-55-71-267-374-4-416-440-102-75-72-466-231-240-339-106-493-182-377-420-226-503-103-385-347-127-504-155-56-67-59-546-457-368-211-424-216-489-37-41-289-32-162-451-422-481-544-170-506-108-280-48-97-299-386-252-313-208-487-93-98-391-213-281-181-83-65-453-188-204-295-100-217-187-514-12-82-484-455-143-136-283-433-115-225-337-206-104-35-272-298-161-14-479-539-171-243-109-291-465-525-430-36-258-166-3-402-355-133-80-173-309-463-406-396-135-126-397-85-39-519-66-78-369-86-9-223-6-448-276-234-134-373-325-363-412-438-266-349-168-158-259-209-328-356-390-534-118-68-195-185-464-427-474-352-184-87-492-178-441-434-318-486-264-24-365-499-358-305-189-152-81-172-227-527-426-300-521-508-110-140-341-253-256-553-296-528-380-520-237-417-456-476-353-435-169-268-279-230-242-491-392-437-383-196-79-214-454-507-50-488-101-314-330-327-450-45-137-555-511-310-203-122-183-292-561-554-536-501-199-375-112-545-215-177-274-92-148-27-58-247-218-415-96-200-301-15-5-452-271-10-471-526-302-69-163-297-388-261-290-367-146-244-531-387-460-239-293-360-354-408-139-1-461-197-174-202-142-376-157-428-311-371-523-149-509-533-34-315-530-338-124-494-444-556-364-21-84-470-382-357-332-398-186-333-42-535-439-263-482-462-121-490-467-541-52\n",
      "Frequency: 1\n",
      "\n",
      "Code 4: 0-408-22-476-473-214-16-130-182-365-33-63-57-240-250-8-526-366-223-113-233-31-311-154-61-228-437-426-310-161-418-274-566-13-406-279-60-346-114-19-320-398-166-53-120-211-555-501-348-562-521-18-40-544-224-225-202-321-388-502-383-167-352-193-419-410-111-183-370-330-194-252-264-151-273-349-195-216-363-506-533-47-128-123-253-255-288-394-54-554-282-77-354-324-339-405-99-44-226-519-17-209-326-415-298-43-266-338-489-482-89-547-169-499-376-291-145-64-441-487-323-564-528-178-95-160-232-347-105-453-414-355-198-399-447-436-269-157-28-29-424-479-416-49-132-51-536-335-340-242-119-236-88-484-146-500-449-131-258-451-382-481-11-20-333-450-541-290-286-245-514-237-7-155-281-197-73-261-91-509-430-556-2-400-74-239-520-344-308-142-259-125-522-107-412-462-350-546-516-277-423-249-327-205-177-404-70-76-129-551-289-23-117-94-434-312-316-463-307-428-374-26-38-446-254-30-90-542-552-25-409-62-46-148-116-139-504-385-563-328-517-553-472-292-152-325-477-561-55-71-271-378-4-421-444-102-75-72-470-235-244-343-106-497-185-381-425-230-507-103-389-351-127-508-156-56-67-59-550-461-372-215-429-220-493-37-41-293-32-163-455-427-485-548-172-510-108-284-48-97-303-391-256-317-212-491-93-98-396-217-285-184-83-65-457-191-208-299-100-221-190-518-12-82-488-459-144-137-287-438-115-229-341-210-104-35-276-302-162-14-483-543-173-247-109-295-469-529-435-36-262-168-3-407-359-134-80-175-313-467-411-401-136-126-402-85-39-523-66-78-373-86-9-227-6-452-280-238-135-377-329-367-417-443-270-353-170-159-263-213-332-360-395-538-118-68-199-188-468-432-478-356-187-87-496-181-445-439-322-490-268-24-369-503-362-309-192-153-81-174-231-531-431-304-525-512-110-141-345-257-260-557-300-532-384-524-241-422-460-480-357-440-171-272-283-234-246-495-397-442-387-200-79-218-458-511-50-492-101-318-334-331-454-45-138-559-515-314-207-122-186-296-565-558-540-505-203-379-112-549-219-179-278-92-149-27-58-251-222-420-96-204-305-15-5-456-275-10-475-530-306-69-164-301-393-265-294-371-147-248-535-392-464-243-297-364-358-413-140-1-465-201-176-206-143-380-158-433-315-375-527-150-165-513-537-34-319-534-342-124-498-448-560-368-21-84-474-386-361-336-390-403-189-337-42-539-267-486-180-196-466-121-494-471-545-133-52\n",
      "Frequency: 9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric as pyg\n",
    "from torch_geometric.utils import k_hop_subgraph, to_networkx\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "##############################################################################\n",
    "# 1) LOAD YOUR DATA\n",
    "##############################################################################\n",
    "data = torch.load('data_o_new2.pt', map_location=torch.device('cpu'))\n",
    "print(\"Data object:\", data)\n",
    "# data is a single graph with data.x, data.edge_index, data.edge_type, etc.\n",
    "# e.g.: Data(x=[572, 128], edge_index=[2, 52112], edge_type=[52112])\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "# 2) EXTRACT COMPUTATION TREES & CREATE CONCEPT VECTOR\n",
    "##############################################################################\n",
    "L = 3  # number of hops for each node’s subgraph (the “computation tree”)\n",
    "\n",
    "# We'll collect:\n",
    "#   node_ctree_codes[v] = string code for node v’s L-hop subgraph\n",
    "#   unique_ctree_codes   = dict {code_str -> assigned_id}\n",
    "node_ctree_codes = []\n",
    "unique_ctree_codes = {}\n",
    "\n",
    "def simple_dfs_code(G, root):\n",
    "    \"\"\"\n",
    "    Simple DFS code from a NetworkX graph G (treated as a tree),\n",
    "    starting at node 'root'. This is just a placeholder function:\n",
    "    you'd use a canonical labeling or something more robust in production.\n",
    "    \"\"\"\n",
    "    code = []\n",
    "    for n in nx.dfs_preorder_nodes(G, source=root):\n",
    "        code.append(str(n))\n",
    "    return \"-\".join(code)\n",
    "\n",
    "for v in range(data.num_nodes):\n",
    "    # Extract L-hop subgraph\n",
    "    subset, sub_edge_index, mapping, _ = k_hop_subgraph(\n",
    "        node_idx=v,\n",
    "        num_hops=L,\n",
    "        edge_index=data.edge_index,\n",
    "        relabel_nodes=True\n",
    "    )\n",
    "    # Build a small Data object\n",
    "    sub_data = pyg.data.Data(\n",
    "        x=data.x[subset],\n",
    "        edge_index=sub_edge_index\n",
    "    )\n",
    "    # Convert to NetworkX\n",
    "    G_sub = to_networkx(sub_data, to_undirected=True)\n",
    "    \n",
    "    # In PyG’s relabel_nodes=True, the root node v becomes \"0\" in sub_data.\n",
    "    ctree_code = simple_dfs_code(G_sub, root=0)\n",
    "    node_ctree_codes.append(ctree_code)\n",
    "    if ctree_code not in unique_ctree_codes:\n",
    "        unique_ctree_codes[ctree_code] = len(unique_ctree_codes)\n",
    "\n",
    "# --- Step 2: Build the concept vector for the entire graph. ---\n",
    "# This vector is length (# unique ctree codes),\n",
    "# and entry i = count of nodes that have that code.\n",
    "concept_vector = np.zeros(len(unique_ctree_codes), dtype=int)\n",
    "for code in node_ctree_codes:\n",
    "    idx = unique_ctree_codes[code]\n",
    "    concept_vector[idx] += 1\n",
    "\n",
    "print(\"Number of unique ctree codes:\", len(unique_ctree_codes))\n",
    "print(\"Concept vector (frequency of each code):\")\n",
    "print(concept_vector)\n",
    "\n",
    "# Print out the first 5 unique computation tree codes and their frequencies:\n",
    "print(\"Sample unique computation tree codes and frequencies:\")\n",
    "for i, (code, idx) in enumerate(unique_ctree_codes.items()):\n",
    "    if i >= 5:\n",
    "        break\n",
    "    freq = concept_vector[idx]\n",
    "    print(f\"Code {idx}: {code}\")\n",
    "    print(f\"Frequency: {freq}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEFINE \"REMOVE CONCEPTS\" FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_concepts_from_graph(data, node_ctree_codes, unique_ctree_codes, subset_of_codes):\n",
    "    \"\"\"\n",
    "    Prune out nodes whose L-hop code is not in 'subset_of_codes'.\n",
    "    Returns a new Data object and a mapping dict (old_index -> new_index).\n",
    "    \"\"\"\n",
    "    keep_nodes_list = []\n",
    "    for v in range(data.num_nodes):\n",
    "        if node_ctree_codes[v] in subset_of_codes:\n",
    "            keep_nodes_list.append(v)\n",
    "    \n",
    "    mapping = {old: new for new, old in enumerate(keep_nodes_list)}\n",
    "    \n",
    "    if not keep_nodes_list:\n",
    "        # No nodes remain; return empty data and empty mapping\n",
    "        new_data = pyg.data.Data(\n",
    "            x=torch.empty((0, data.x.shape[1])),\n",
    "            edge_index=torch.empty((2, 0), dtype=torch.long),\n",
    "            edge_type=torch.empty((0,), dtype=torch.long)\n",
    "        )\n",
    "        return new_data, mapping\n",
    "\n",
    "    keep_nodes = torch.tensor(keep_nodes_list, dtype=torch.long)\n",
    "    # Filter node features\n",
    "    x_new = data.x[keep_nodes]\n",
    "\n",
    "    # Filter edges and corresponding edge_type\n",
    "    edges = []\n",
    "    e_types = []\n",
    "    keep_set = set(keep_nodes_list)\n",
    "    for i in range(data.edge_index.size(1)):\n",
    "        src = data.edge_index[0, i].item()\n",
    "        dst = data.edge_index[1, i].item()\n",
    "        if (src in keep_set) and (dst in keep_set):\n",
    "            edges.append([src, dst])\n",
    "            e_types.append(data.edge_type[i])  # No .item() here\n",
    "    if edges:\n",
    "        edges = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "        e_types = torch.tensor(e_types, dtype=torch.int64)\n",
    "    else:\n",
    "        edges = torch.empty((2,0), dtype=torch.long)\n",
    "        e_types = torch.empty((0,), dtype=torch.int64)\n",
    "    \n",
    "    # Relabel node IDs\n",
    "    edges = _relabel_edge_index(edges, keep_nodes)\n",
    "\n",
    "    new_data = pyg.data.Data(x=x_new, edge_index=edges, edge_type=e_types)\n",
    "    return new_data, mapping\n",
    "\n",
    "def _relabel_edge_index(edge_index, keep_nodes):\n",
    "    old_to_new = {old: i for i, old in enumerate(keep_nodes.tolist())}\n",
    "    new_edges = []\n",
    "    for i in range(edge_index.size(1)):\n",
    "        src_old = edge_index[0, i].item()\n",
    "        dst_old = edge_index[1, i].item()\n",
    "        new_edges.append([old_to_new[src_old], old_to_new[dst_old]])\n",
    "    if len(new_edges) == 0:\n",
    "        return torch.empty((2, 0), dtype=torch.long)\n",
    "    new_edges = torch.tensor(new_edges, dtype=torch.long).t().contiguous()\n",
    "    return new_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEFINE A VALUE FUNCTION THAT RUNS MRCGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_function(model, data, idx,\n",
    "                   subset_of_codes, node_ctree_codes, unique_ctree_codes,\n",
    "                   target_class=None):\n",
    "    \"\"\"\n",
    "    1) Remove concepts not in 'subset_of_codes'\n",
    "    2) Update idx based on the new node numbering\n",
    "    3) Run model.predict() and return a scalar value.\n",
    "    \"\"\"\n",
    "    # 1) Prune the graph and get the mapping from old indices to new indices.\n",
    "    modified_data, mapping = remove_concepts_from_graph(\n",
    "        data, node_ctree_codes, unique_ctree_codes, subset_of_codes\n",
    "    )\n",
    "    \n",
    "    # 2) Update idx: For each index in idx, if it exists in mapping, use the new index.\n",
    "    new_src = []\n",
    "    new_dst = []\n",
    "    for src, dst in zip(idx[0].tolist(), idx[1].tolist()):\n",
    "        if src in mapping and dst in mapping:\n",
    "            new_src.append(mapping[src])\n",
    "            new_dst.append(mapping[dst])\n",
    "    if len(new_src) == 0 or len(new_dst) == 0:\n",
    "        return 0.0  # no valid pairs remain\n",
    "\n",
    "    new_idx = (torch.tensor(new_src, dtype=torch.long),\n",
    "               torch.tensor(new_dst, dtype=torch.long))\n",
    "    \n",
    "    # 3) Forward pass\n",
    "    with torch.no_grad():\n",
    "        out = model.predict(modified_data, new_idx)  # shape: [num_pairs, 65]\n",
    "        if out.shape[0] == 0:\n",
    "            return 0.0\n",
    "\n",
    "        if target_class is None:\n",
    "            # Use the predicted class for each pair and average the logit.\n",
    "            preds = out.argmax(dim=1)\n",
    "            chosen_logits = out[torch.arange(out.size(0)), preds]\n",
    "            val = chosen_logits.mean().item()\n",
    "        else:\n",
    "            chosen_logits = out[:, target_class]\n",
    "            val = chosen_logits.mean().item()\n",
    "    return val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SHAPLEY VALUES (SAMPLING APPROACH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def shapley_values(model, data, idx,\n",
    "                   unique_ctree_codes, node_ctree_codes,\n",
    "                   target_class=None,\n",
    "                   num_samples=50):\n",
    "    \"\"\"\n",
    "    Approximate Shapley values via random permutations.\n",
    "    \"\"\"\n",
    "    concepts = list(unique_ctree_codes.keys())  # DFS-code strings\n",
    "    M = len(concepts)\n",
    "    shap = np.zeros(M, dtype=float)\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        perm = np.random.permutation(M)\n",
    "        current_subset = set()\n",
    "        old_val = value_function(\n",
    "            model, data, idx,\n",
    "            current_subset, node_ctree_codes, unique_ctree_codes,\n",
    "            target_class\n",
    "        )\n",
    "        for j in range(M):\n",
    "            c_idx = perm[j]\n",
    "            c_code = concepts[c_idx]\n",
    "            new_subset = current_subset.union({c_code})\n",
    "            new_val = value_function(\n",
    "                model, data, idx,\n",
    "                new_subset, node_ctree_codes, unique_ctree_codes,\n",
    "                target_class\n",
    "            )\n",
    "            shap[c_idx] += (new_val - old_val)\n",
    "            current_subset = new_subset\n",
    "            old_val = new_val\n",
    "\n",
    "    shap /= num_samples\n",
    "    code2shap = {concepts[i]: shap[i] for i in range(M)}\n",
    "    return code2shap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RUN SHAPLEY & DISPLAY RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently on sample 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qr/n043ky6s44q5gb_8fmmpj1h00000gn/T/ipykernel_4260/1758540952.py:109: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  e_type = torch.tensor(e_type, dtype=torch.int64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently on sample 1\n",
      "Currently on sample 2\n",
      "Currently on sample 3\n",
      "Currently on sample 4\n",
      "Currently on sample 5\n",
      "Currently on sample 6\n",
      "Currently on sample 7\n",
      "Currently on sample 8\n",
      "Currently on sample 9\n",
      "Currently on sample 10\n",
      "Currently on sample 11\n",
      "Currently on sample 12\n",
      "Currently on sample 13\n",
      "Currently on sample 14\n",
      "Currently on sample 15\n",
      "Currently on sample 16\n",
      "Currently on sample 17\n",
      "Currently on sample 18\n",
      "Currently on sample 19\n",
      "Currently on sample 20\n",
      "Currently on sample 21\n",
      "Currently on sample 22\n",
      "Currently on sample 23\n",
      "Currently on sample 24\n",
      "Currently on sample 25\n",
      "Currently on sample 26\n",
      "Currently on sample 27\n",
      "Currently on sample 28\n",
      "Currently on sample 29\n",
      "Currently on sample 30\n",
      "Currently on sample 31\n",
      "Currently on sample 32\n",
      "Currently on sample 33\n",
      "Currently on sample 34\n",
      "Currently on sample 35\n",
      "Currently on sample 36\n",
      "Currently on sample 37\n",
      "Currently on sample 38\n",
      "Currently on sample 39\n",
      "Currently on sample 40\n",
      "Currently on sample 41\n",
      "Currently on sample 42\n",
      "Currently on sample 43\n",
      "Currently on sample 44\n",
      "Currently on sample 45\n",
      "Currently on sample 46\n",
      "Currently on sample 47\n",
      "Currently on sample 48\n",
      "Currently on sample 49\n",
      "total time taken: 594.8060019016266\n",
      "\n",
      "Top-10 Concepts by Shapley Value:\n",
      "0-408-22-477-474-214-16-130-182-365-33-63-57-240-250-8-527-366-223-113-233-31-311-154-61-228-437-426-310-161-418-274-567-13-406-279-60-346-114-19-320-398-166-53-120-211-556-502-348-563-522-18-40-545-224-225-202-321-388-503-383-167-352-193-419-410-111-183-370-330-194-252-264-151-273-349-195-216-363-507-534-47-128-123-253-255-288-394-54-555-282-77-354-324-339-405-99-44-226-520-17-209-326-415-298-43-266-338-490-483-89-548-169-500-376-291-145-64-441-488-323-565-529-178-95-160-232-347-105-454-414-355-198-399-448-436-269-157-28-29-424-480-416-49-132-51-537-335-340-242-119-236-88-485-146-501-450-131-258-452-382-482-11-20-333-451-542-290-286-245-515-237-7-155-281-197-73-261-91-510-430-557-2-400-74-239-521-344-308-142-259-125-523-107-412-463-350-547-517-277-423-249-327-205-177-404-70-76-129-552-289-23-117-94-434-312-316-464-307-428-374-26-38-447-254-30-90-543-553-25-409-62-46-148-116-139-505-385-564-328-518-554-473-292-152-325-478-562-55-71-271-378-4-421-445-102-75-72-471-235-244-343-106-498-185-381-425-230-508-103-389-351-127-509-156-56-67-59-551-462-372-215-429-220-494-37-41-293-32-163-456-427-486-549-172-511-108-284-48-97-303-391-256-317-212-492-93-98-396-217-285-184-83-65-458-191-208-299-100-221-190-519-12-82-489-460-144-137-287-438-115-229-341-210-104-35-276-302-162-14-484-544-173-247-109-295-470-530-435-36-262-168-3-407-359-134-80-175-313-468-411-401-136-126-402-85-39-524-66-78-373-86-9-227-6-453-280-238-135-377-329-367-417-443-270-353-170-159-263-213-332-360-395-539-118-68-199-188-469-432-479-356-187-87-497-181-446-439-322-491-268-24-369-504-362-309-192-153-81-174-231-532-431-304-526-513-110-141-345-257-260-558-300-533-384-525-241-422-461-481-357-440-171-272-283-234-246-496-397-442-387-200-79-218-459-512-50-493-101-318-334-331-455-45-138-560-516-314-207-122-186-296-566-559-541-506-203-379-112-550-219-179-278-92-149-27-58-251-222-420-96-204-305-15-5-457-275-10-476-531-306-69-164-301-393-265-294-371-147-248-536-392-465-243-297-364-358-413-140-1-466-201-176-206-143-380-158-433-315-375-528-150-165-514-538-34-319-535-342-124-499-449-561-368-21-84-475-386-361-336-390-403-189-337-42-540-444-267-487-180-196-467-121-495-472-546-133-52 => 9.5708\n",
      "0-407-22-475-472-213-16-130-181-364-33-63-57-239-249-8-525-365-222-113-232-31-310-154-61-227-436-425-309-161-417-273-565-13-405-278-60-345-114-19-319-397-165-53-120-210-554-500-347-561-520-18-40-543-223-224-201-320-387-501-382-166-351-192-418-409-111-182-369-329-193-251-263-151-272-348-194-215-362-505-532-47-128-123-252-254-287-393-54-553-281-77-353-323-338-404-99-44-225-518-17-208-325-414-297-43-265-337-488-481-89-546-168-498-375-290-145-64-440-486-322-563-527-177-95-160-231-346-105-452-413-354-197-398-446-435-268-157-28-29-423-478-415-49-132-51-535-334-339-241-119-235-88-483-146-499-448-131-257-450-381-480-11-20-332-449-540-289-285-244-513-236-7-155-280-196-73-260-91-508-429-555-2-399-74-238-519-343-307-142-258-125-521-107-411-461-349-545-515-276-422-248-326-204-176-403-70-76-129-550-288-23-117-94-433-311-315-462-306-427-373-26-38-445-253-30-90-541-551-25-408-62-46-148-116-139-503-384-562-327-516-552-471-291-152-324-476-560-55-71-270-377-4-420-443-102-75-72-469-234-243-342-106-496-184-380-424-229-506-103-388-350-127-507-156-56-67-59-549-460-371-214-428-219-492-37-41-292-32-163-454-426-484-547-171-509-108-283-48-97-302-390-255-316-211-490-93-98-395-216-284-183-83-65-456-190-207-298-100-220-189-517-12-82-487-458-144-137-286-437-115-228-340-209-104-35-275-301-162-14-482-542-172-246-109-294-468-528-434-36-261-167-3-406-358-134-80-174-312-466-410-400-136-126-401-85-39-522-66-78-372-86-9-226-6-451-279-237-135-376-328-366-416-442-269-352-169-159-262-212-331-359-394-537-118-68-198-187-467-431-477-355-186-87-495-180-444-438-321-489-267-24-368-502-361-308-191-153-81-173-230-530-430-303-524-511-110-141-344-256-259-556-299-531-383-523-240-421-459-479-356-439-170-271-282-233-245-494-396-441-386-199-79-217-457-510-50-491-101-317-333-330-453-45-138-558-514-313-206-122-185-295-564-557-539-504-202-378-112-548-218-178-277-92-149-27-58-250-221-419-96-203-304-15-5-455-274-10-474-529-305-69-164-300-392-264-293-370-147-247-534-391-463-242-296-363-357-412-140-1-464-200-175-205-143-379-158-432-314-374-526-150-512-536-34-318-533-341-124-497-447-559-367-21-84-473-385-360-335-389-402-188-336-42-538-266-485-179-195-465-121-493-470-544-133-52 => 0.0003\n",
      "0-400-22-464-461-207-16-66-78-365-262-427-466-490-13-398-272-60-339-113-157-410-267-546-453-502-535-460-285-149-243-8-511-359-216-112-226-31-304-151-61-221-428-418-303-452-300-419-366-26-38-437-247-30-89-342-266-148-257-245-187-242-415-396-70-76-128-209-188-204-537-487-341-542-506-18-40-527-217-218-195-314-53-119-282-23-116-93-142-284-368-485-163-530-88-470-230-500-238-96-356-492-517-47-127-122-246-248-281-386-54-536-275-77-347-317-332-397-98-44-219-504-17-202-319-407-291-43-259-331-476-388-210-62-46-145-305-309-283-279-84-358-176-198-171-408-49-172-94-156-225-340-104-443-488-375-161-345-186-411-402-110-177-362-323-390-313-124-507-106-404-451-63-57-233-468-482-429-114-222-334-203-103-35-83-432-474-316-544-512-107-496-192-87-229-118-510-297-321-543-377-412-255-162-3-399-352-131-130-51-520-328-333-235-239-227-165-467-360-409-434-269-343-529-501-270-254-90-495-421-538-2-392-74-232-505-337-301-139-252-25-401-9-220-55-191-391-438-278-178-450-364-208-420-213-480-37-41-29-416-471-526-167-240-108-288-439-129-251-441-374-469-11-20-326-440-524-523-295-158-14-486-143-472-413-4-370-264-71-423-310-205-478-92-97-363-287-150-185-302-355-489-19-417-223-493-102-381-344-126-494-153-56-67-59-532-406-348-422-515-224-168-159-101-435-455-531-473-184-503-12-277-48-201-292-99-214-307-296-383-249-228-237-336-105-483-179-373-175-32-516-393-133-125-394-445-138-338-72-459-109-349-181-86-190-274-152-7-39-508-182-457-231-273-442-497-320-6-64-256-206-136-491-155-465-318-322-140-134-141-448-475-446-289-540-444-45-425-380-15-449-414-234-306-169-80-10-463-73-121-65-82-180-166-534-525-315-477-261-24-361-164-346-81-541-539-293-389-458-513-426-36-353-387-28-137-1-454-91-479-325-200-403-456-286-509-376-5-170-194-481-405-144-68-117-430-33-436-385-258-199-132-369-350-431-276-100-324-499-160-521-193-79-211-447-50-433-379-253-250-311-268-263-545-498-196-514-330-183-21-395-75-244-173-212-146-522-135-271-280-424-372-294-111-85-154-27-308-367-371-115-357-58-42-197-215-351-384-519-241-174-265-290-69-189-298-236-329-382-378-462-123-147-484-34-354-312-518-260-95-335-528-533-299-327-120-52 => 0.0000\n",
      "0-401-22-470-467-210-16-130-179-360-33-63-57-236-246-8-520-361-219-113-229-31-307-153-61-224-430-419-306-160-411-270-560-13-399-275-60-341-114-19-316-391-164-53-120-207-549-495-343-556-515-18-40-538-220-221-198-317-382-496-377-165-347-190-412-403-111-180-364-326-191-248-260-150-269-344-192-212-358-500-527-47-128-123-249-251-284-387-54-548-278-77-349-320-334-398-99-44-222-513-17-205-322-408-294-43-262-333-483-476-89-541-167-493-370-287-144-64-434-481-319-558-522-176-95-159-228-342-105-447-407-350-194-392-441-429-265-156-28-29-417-473-409-49-132-51-530-331-335-238-119-232-88-478-145-494-443-131-254-445-376-475-11-20-329-444-535-286-282-241-508-233-7-154-277-193-73-257-91-503-423-550-2-393-74-235-514-339-304-141-255-125-516-107-405-456-345-540-510-273-416-245-323-201-175-397-70-76-129-545-285-23-117-94-427-308-312-457-303-421-368-26-38-440-250-30-90-536-546-25-402-62-46-147-116-138-498-379-557-324-511-547-466-288-151-321-471-555-55-71-267-372-4-414-438-102-75-72-464-231-240-338-106-491-182-375-418-226-501-103-383-346-127-502-155-56-67-59-544-455-366-211-422-216-487-37-41-289-32-162-449-420-479-542-170-504-108-280-48-97-299-384-252-313-208-485-93-98-389-213-281-181-83-65-451-188-204-295-100-217-187-512-12-82-482-453-143-136-283-431-115-225-336-206-104-35-272-298-161-14-477-537-171-243-109-291-463-523-428-36-258-166-3-400-354-133-80-173-309-461-404-394-135-126-395-85-39-517-66-78-367-86-9-223-6-446-276-234-134-371-325-362-410-436-266-348-168-158-259-209-328-355-388-532-118-68-195-185-462-425-472-351-184-87-490-178-439-432-318-484-264-24-363-497-357-305-189-152-81-172-227-525-424-300-519-506-110-140-340-253-256-551-296-526-378-518-237-415-454-474-352-433-169-268-279-230-242-489-390-435-381-196-79-214-452-505-50-486-101-314-330-327-448-45-137-553-509-310-203-122-183-292-559-552-534-499-199-373-112-543-215-177-274-92-148-27-58-247-218-413-96-200-301-15-5-450-271-10-469-524-302-69-163-297-386-261-290-365-146-244-529-385-458-239-293-359-353-406-139-1-459-197-174-202-142-374-157-426-311-369-521-149-507-531-34-315-528-337-124-492-442-554-21-84-468-380-356-396-186-332-42-533-437-263-480-460-121-488-465-539-52 => 0.0000\n",
      "0 => 0.0000\n",
      "0-401-22-466-463-207-16-126-176-358-32-61-55-233-243-8-516-359-216-110-226-30-304-149-59-221-429-418-303-156-410-267-552-13-399-272-58-339-111-19-313-391-161-51-117-204-541-491-341-548-511-18-39-531-217-218-195-314-381-492-376-162-345-187-411-403-108-177-363-323-188-245-257-146-266-342-189-209-356-496-522-46-124-119-246-248-281-387-52-540-275-75-347-317-332-398-96-43-219-509-17-202-319-407-291-42-259-331-479-472-86-533-164-489-369-284-140-62-433-477-316-550-518-173-92-155-225-340-102-445-406-348-191-392-439-428-262-152-27-28-416-469-408-48-128-50-524-328-333-235-116-229-85-474-141-490-441-127-251-443-375-471-11-20-326-442-528-283-279-238-504-230-7-150-274-190-71-254-88-499-422-542-2-393-72-232-510-337-301-137-252-121-512-104-405-454-343-532-506-270-415-242-320-198-172-397-68-74-125-537-282-57-536-87-529-538-24-402-60-45-143-305-309-455-300-420-367-25-37-438-247-29-461-70-73-99-436-507-539-462-285-147-318-467-547-53-69-264-371-4-413-208-421-213-483-36-40-286-31-158-497-223-417-374-179-487-446-44-430-112-222-334-203-101-34-81-63-449-185-201-292-310-205-494-378-549-321-297-515-502-486-175-97-214-100-382-344-123-498-151-54-65-478-451-139-132-280-389-210-278-178-453-365-105-500-192-182-459-129-352-400-3-240-106-288-460-136-338-9-220-6-444-273-231-130-370-255-163-545-475-534-167-184-508-12-277-47-94-296-384-249-228-237-336-103-350-470-452-424-468-349-181-84-482-353-388-346-165-154-256-206-90-481-495-134-519-95-364-287-148-186-302-355-493-404-458-306-170-78-239-227-166-265-276-311-253-250-543-293-521-394-131-122-395-82-38-513-64-76-366-83-66-115-431-437-315-480-261-23-530-168-362-447-419-505-307-200-360-409-435-269-295-157-14-473-169-224-520-423-79-448-107-10-465-377-514-234-414-15-5-426-35-427-114-91-98-385-241-464-41-183-120-56-386-258-174-142-212-144-322-138-425-324-390-434-380-193-77-211-450-501-49-26-308-368-526-159-527-263-89-199-118-180-80-289-432-551-544-456-236-194-485-171-268-327-244-215-412-93-197-325-294-271-330-113-351-196-476-372-109-535-373-153-135-1-357-298-290-517-145-160-329-67-383-299-503-525-133-33-312-523-335-488-440-546-361-21-396-484-379-354-260-457 => 0.0000\n",
      "0-408-22-476-473-214-16-130-182-365-33-63-57-240-250-8-526-366-223-113-233-31-311-154-61-228-437-426-310-161-418-274-565-13-406-279-60-346-114-19-320-398-166-53-120-211-554-501-348-561-521-18-40-543-224-225-202-321-388-502-383-167-352-193-419-410-111-183-370-330-194-252-264-151-273-349-195-216-363-506-533-47-128-123-253-255-288-394-54-553-282-77-354-324-339-405-99-44-226-519-17-209-326-415-298-43-266-338-489-482-89-546-169-499-376-291-145-64-441-487-323-563-528-178-95-160-232-347-105-453-414-355-198-399-447-436-269-157-28-29-424-479-416-49-132-51-536-335-340-242-119-236-88-484-146-500-449-131-258-451-382-481-11-20-333-450-540-290-286-245-514-237-7-155-281-197-73-261-91-509-430-555-2-400-74-239-520-344-308-142-259-125-522-107-412-462-350-545-516-277-423-249-327-205-177-404-70-76-129-550-289-23-117-94-434-312-316-463-307-428-374-26-38-446-254-30-90-541-551-25-409-62-46-148-116-139-504-385-562-328-517-552-472-292-152-325-477-560-55-71-271-378-4-421-444-102-75-72-470-235-244-343-106-497-185-381-425-230-507-103-389-351-127-508-156-56-67-59-549-461-372-215-429-220-493-37-41-293-32-163-455-427-485-547-172-510-108-284-48-97-303-391-256-317-212-491-93-98-396-217-285-184-83-65-457-191-208-299-100-221-190-518-12-82-488-459-144-137-287-438-115-229-341-210-104-35-276-302-162-14-483-542-173-247-109-295-469-529-435-36-262-168-3-407-359-134-80-175-313-467-411-401-136-126-402-85-39-523-66-78-373-86-9-227-6-452-280-238-135-377-329-367-417-443-270-353-170-159-263-213-332-360-395-187-87-496-181-445-439-322-356-478-432-460-480-357-440-171-272-525-304-309-192-153-81-174-231-531-431-164-503-362-369-24-268-490-10-475-530-110-141-345-257-260-556-300-532-384-524-241-422-15-5-314-207-122-199-188-468-515-558-454-45-138-219-179-278-92-149-331-296-186-564-118-68-147-505-203-379-112-548-539-557-397-442-234-246-495-492-200-79-218-458-511-387-101-50-456-275-318-512-464-243-301-393-265-294-371-222-248-535-392-306-251-96-334-204-420-27-58-283-69-305-364-358-297-140-1-465-413-201-176-206-143-380-158-433-315-375-527-150-165-513-537-34-319-534-342-124-498-448-559-368-21-84-474-386-361-336-390-403-189-337-42-538-267-486-180-196-466-121-494-471-544-133-52 => -0.0000\n",
      "0-406-22-474-471-214-16-130-182-364-33-63-57-240-250-8-524-365-223-113-233-31-311-154-61-228-435-424-310-161-416-274-564-13-404-279-60-345-114-19-320-396-166-53-120-211-553-499-347-560-519-18-40-542-224-225-202-321-386-500-381-167-351-193-417-408-111-183-368-330-194-252-264-151-273-348-195-216-362-504-531-47-128-123-253-255-288-392-54-552-282-77-353-324-338-403-99-44-226-517-17-209-326-413-298-43-266-337-487-480-89-545-169-497-374-291-145-64-439-485-323-562-526-178-95-160-232-346-105-451-412-354-198-397-446-434-269-157-28-29-422-477-414-49-132-51-534-335-339-242-119-236-88-482-146-498-447-131-258-449-380-479-11-20-333-448-539-290-286-245-512-237-7-155-281-197-73-261-91-507-428-554-2-398-74-239-518-343-308-142-259-125-520-107-410-460-349-544-514-277-421-249-327-205-177-402-70-76-129-549-289-23-117-94-432-312-316-461-307-426-372-26-38-445-254-30-90-540-550-25-407-62-46-148-116-139-502-383-561-328-515-551-470-292-152-325-475-559-55-71-271-376-4-419-443-102-75-72-468-235-244-342-106-495-185-379-423-230-505-103-387-350-127-506-156-56-67-59-548-459-370-215-427-220-491-37-41-293-32-163-453-425-483-546-172-508-108-284-48-97-303-389-256-317-212-489-93-98-394-217-285-184-83-65-455-191-208-299-100-221-190-516-12-82-486-457-144-137-287-436-115-229-340-210-104-35-276-302-162-14-481-541-173-247-109-295-467-527-433-36-262-168-3-405-358-134-80-175-313-465-409-399-136-126-400-85-39-521-66-78-371-86-9-227-6-450-280-238-135-375-329-366-415-441-270-352-170-159-263-213-332-359-393-536-118-68-199-188-466-430-476-355-187-87-494-181-444-437-322-488-268-24-367-501-361-309-192-153-81-174-231-529-429-304-523-510-110-141-344-257-260-555-300-530-382-522-241-420-458-478-356-438-171-272-283-234-246-493-395-440-385-200-79-218-456-509-50-490-101-318-334-331-452-45-138-557-513-314-207-122-186-296-563-556-538-503-203-377-112-547-219-179-278-92-149-27-58-251-222-418-96-204-305-15-5-454-275-10-473-528-306-69-164-301-391-265-294-369-147-248-533-390-462-243-297-363-357-411-140-1-463-201-176-206-143-378-158-431-315-373-525-150-165-511-535-34-319-532-341-124-496-21-84-472-384-360-388-401-189-336-42-537-442-558-267-484-180-196-464-121-492-469-543-133-52 => -0.0000\n",
      "0-406-22-473-470-213-16-130-181-364-33-63-57-239-249-8-523-365-222-113-232-31-310-153-61-227-435-424-309-160-416-273-563-13-404-278-60-345-114-19-319-396-165-53-120-210-552-498-347-559-518-18-40-541-223-224-201-320-386-499-381-166-351-192-417-408-111-182-368-329-193-251-263-150-272-348-194-215-362-503-530-47-128-123-252-254-287-392-54-551-281-77-353-323-338-403-99-44-225-516-17-208-325-413-297-43-265-337-486-479-89-544-168-496-374-290-144-64-439-484-322-561-525-177-95-159-231-346-105-450-412-354-197-397-445-434-268-156-28-29-422-476-414-49-132-51-533-334-339-241-119-235-88-481-145-497-446-131-257-448-380-478-11-20-332-447-538-289-285-244-511-236-7-154-280-196-73-260-91-506-428-553-2-398-74-238-517-343-307-141-258-125-519-107-410-459-349-543-513-276-421-248-326-204-176-402-70-76-129-548-288-23-117-94-432-311-315-460-306-426-372-26-38-444-253-30-90-539-549-25-407-62-46-147-116-138-501-383-560-327-514-550-469-291-151-324-474-558-55-71-270-376-4-419-442-102-75-72-467-234-243-342-106-494-184-379-423-229-504-103-387-350-127-505-155-56-67-59-547-458-370-214-427-219-490-37-41-292-32-162-452-425-482-545-171-507-108-283-48-97-302-389-255-316-211-488-93-98-394-216-284-183-83-65-454-190-207-298-100-220-189-515-12-82-485-456-143-136-286-436-115-228-340-209-104-35-275-301-161-14-480-540-172-246-109-294-466-526-433-36-261-167-3-405-358-133-80-174-312-464-409-399-135-126-400-85-39-520-66-78-371-86-9-226-6-449-279-237-134-375-328-366-415-441-269-352-169-158-262-212-331-359-393-535-118-68-198-187-465-430-475-355-186-87-493-180-443-437-321-487-267-24-367-500-361-308-191-152-81-173-230-528-429-303-522-509-110-140-344-256-259-554-299-529-382-521-240-420-457-477-356-438-170-271-282-233-245-492-395-440-385-199-79-217-455-508-50-489-101-317-333-330-451-45-137-556-512-313-206-122-185-295-562-555-537-502-202-377-112-546-218-178-277-92-148-27-58-250-221-418-96-203-304-15-5-453-274-10-472-527-305-69-163-300-391-264-293-369-146-247-532-390-461-242-296-363-357-411-139-1-462-200-175-205-142-378-157-431-314-373-524-149-164-510-534-34-318-531-341-124-495-21-84-471-384-360-335-388-401-188-336-42-536-557-266-483-179-195-463-121-491-468-542-52 => -0.0000\n",
      "0-404-22-472-469-211-16-130-180-362-33-63-57-237-247-8-521-363-220-113-230-31-308-154-61-225-433-422-307-161-414-271-561-13-402-276-60-343-114-19-317-394-165-53-120-208-550-497-345-557-516-18-40-539-221-222-199-318-385-498-380-166-349-191-415-406-111-181-367-327-192-249-261-151-270-346-193-213-360-502-528-47-128-123-250-252-285-390-54-549-279-77-351-321-336-401-99-44-223-514-17-206-323-411-295-43-263-335-485-478-89-542-168-495-373-288-145-64-437-483-320-559-523-177-95-160-229-344-105-450-410-352-195-395-444-432-266-157-28-29-420-475-412-49-132-51-531-332-337-239-119-233-88-480-146-496-446-131-255-448-379-477-11-20-330-447-536-287-283-242-509-234-7-155-278-194-73-258-91-505-426-551-2-396-74-236-515-341-305-142-256-125-517-107-408-458-347-541-511-274-419-246-324-202-176-400-70-76-129-546-286-23-117-94-430-309-313-459-304-424-371-26-38-443-251-30-90-537-547-25-405-62-46-148-116-139-500-382-558-325-512-548-468-289-152-322-473-556-55-71-268-375-4-417-441-102-75-72-466-232-241-340-106-493-183-378-421-227-503-103-386-348-127-504-156-56-67-59-545-457-369-212-425-217-489-37-41-290-32-163-452-423-481-543-171-506-108-281-48-97-300-387-253-314-209-487-93-98-392-214-282-182-83-65-454-189-205-296-100-218-188-513-12-82-484-455-144-137-284-434-115-226-338-207-104-35-273-299-162-14-479-538-172-244-109-292-465-524-431-36-259-167-3-403-356-134-80-174-310-463-407-397-136-126-398-85-39-518-66-78-370-86-9-224-6-449-277-235-135-374-326-364-413-439-267-350-169-159-260-210-329-357-391-533-118-68-196-186-464-428-474-353-185-87-492-179-442-435-319-486-265-24-366-499-359-306-190-153-81-173-228-526-427-301-520-508-110-141-342-254-257-552-297-527-381-519-238-418-456-476-354-436-170-269-280-231-243-491-393-438-384-197-79-215-507-50-488-101-315-331-328-451-45-138-554-510-311-204-122-184-293-560-553-535-501-200-376-112-544-216-178-275-92-149-27-58-248-219-416-96-201-302-15-5-453-272-10-471-525-303-69-164-298-389-262-291-368-147-245-530-388-460-240-294-361-355-409-140-1-461-198-175-203-143-377-158-429-312-372-522-150-532-34-316-529-339-124-494-445-555-365-21-84-470-383-358-333-399-187-334-42-534-440-264-482-462-121-490-467-540-133-52 => -0.0001\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "start = time.time()\n",
    "num_nodes = data.num_nodes\n",
    "src_idx = [random.randint(0, num_nodes) for _ in range(10)]\n",
    "dst_idx = []\n",
    "for i in range(10):\n",
    "    dst_idx.append(random.choice(list(set([x for x in range(num_nodes)]) - set([src_idx[i]]))))\n",
    "idx = (torch.tensor(src_idx), torch.tensor(dst_idx))\n",
    "shap_vals = shapley_values(\n",
    "    model=model,\n",
    "    data=data,\n",
    "    idx=idx,\n",
    "    unique_ctree_codes=unique_ctree_codes,\n",
    "    node_ctree_codes=node_ctree_codes,\n",
    "    target_class=None,  # or an int specifying which class\n",
    "    num_samples=50\n",
    ")\n",
    "\n",
    "# Sort Shapley results\n",
    "sorted_shap = sorted(shap_vals.items(), key=lambda x: x[1], reverse=True)\n",
    "print(f\"total time taken: {time.time()-start}\")\n",
    "print(\"\\nTop-10 Concepts by Shapley Value:\")\n",
    "for c, val in sorted_shap[:10]:\n",
    "    print(f\"{c} => {val:.4f}\")\n",
    "\n",
    "# Done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Generate logic formula by symbolic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pysr\n",
      "  Downloading pysr-1.5.2-py3-none-any.whl.metadata (54 kB)\n",
      "Requirement already satisfied: sympy<2.0.0,>=1.0.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from pysr) (1.13.3)\n",
      "Requirement already satisfied: pandas<3.0.0,>=0.21.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from pysr) (2.2.2)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.13.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from pysr) (1.26.4)\n",
      "Requirement already satisfied: scikit_learn<2.0.0,>=1.0.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from pysr) (1.5.1)\n",
      "Collecting juliacall==0.9.24 (from pysr)\n",
      "  Downloading juliacall-0.9.24-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.0.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from pysr) (8.1.7)\n",
      "Requirement already satisfied: setuptools>=50.0.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from pysr) (75.1.0)\n",
      "Collecting juliapkg~=0.1.8 (from juliacall==0.9.24->pysr)\n",
      "  Downloading juliapkg-0.1.16-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from pandas<3.0.0,>=0.21.0->pysr) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from pandas<3.0.0,>=0.21.0->pysr) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from pandas<3.0.0,>=0.21.0->pysr) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from scikit_learn<2.0.0,>=1.0.0->pysr) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from scikit_learn<2.0.0,>=1.0.0->pysr) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from scikit_learn<2.0.0,>=1.0.0->pysr) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from sympy<2.0.0,>=1.0.0->pysr) (1.3.0)\n",
      "Collecting filelock<4.0,>=3.16 (from juliapkg~=0.1.8->juliacall==0.9.24->pysr)\n",
      "  Downloading filelock-3.17.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: semver<4.0,>=3.0 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from juliapkg~=0.1.8->juliacall==0.9.24->pysr) (3.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/tyler/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=0.21.0->pysr) (1.16.0)\n",
      "Downloading pysr-1.5.2-py3-none-any.whl (93 kB)\n",
      "Downloading juliacall-0.9.24-py3-none-any.whl (11 kB)\n",
      "Downloading juliapkg-0.1.16-py3-none-any.whl (16 kB)\n",
      "Downloading filelock-3.17.0-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: filelock, juliapkg, juliacall, pysr\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.13.1\n",
      "    Uninstalling filelock-3.13.1:\n",
      "      Successfully uninstalled filelock-3.13.1\n",
      "Successfully installed filelock-3.17.0 juliacall-0.9.24 juliapkg-0.1.16 pysr-1.5.2\n",
      "[juliapkg] Found dependencies: /Users/tyler/anaconda3/lib/python3.12/site-packages/juliacall/juliapkg.json\n",
      "[juliapkg] Found dependencies: /Users/tyler/anaconda3/lib/python3.12/site-packages/pysr/juliapkg.json\n",
      "[juliapkg] Found dependencies: /Users/tyler/anaconda3/lib/python3.12/site-packages/juliapkg/juliapkg.json\n",
      "[juliapkg] Locating Julia =1.10.0, ^1.10.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tyler/anaconda3/lib/python3.12/site-packages/juliacall/__init__.py:61: UserWarning: torch was imported before juliacall. This may cause a segfault. To avoid this, import juliacall before importing torch. For updates, see https://github.com/pytorch/pytorch/issues/78829.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[juliapkg] Querying Julia versions from https://julialang-s3.julialang.org/bin/versions.json\n",
      "[juliapkg] WARNING: About to install Julia 1.11.3 to /Users/tyler/anaconda3/julia_env/pyjuliapkg/install.\n",
      "[juliapkg]   If you use juliapkg in more than one environment, you are likely to\n",
      "[juliapkg]   have Julia installed in multiple locations. It is recommended to\n",
      "[juliapkg]   install JuliaUp (https://github.com/JuliaLang/juliaup) or Julia\n",
      "[juliapkg]   (https://julialang.org/downloads) yourself.\n",
      "[juliapkg] Downloading Julia from https://julialang-s3.julialang.org/bin/mac/aarch64/1.11/julia-1.11.3-macaarch64.dmg\n",
      "             downloaded 165.5 MB of 205.5 MB\n",
      "             download complete\n",
      "[juliapkg] Verifying download\n",
      "[juliapkg] Installing Julia 1.11.3 to /Users/tyler/anaconda3/julia_env/pyjuliapkg/install\n",
      "[juliapkg] Using Julia 1.11.3 at /Users/tyler/anaconda3/julia_env/pyjuliapkg/install/bin/julia\n",
      "[juliapkg] Using Julia project at /Users/tyler/anaconda3/julia_env\n",
      "[juliapkg] Installing packages:\n",
      "           julia> import Pkg\n",
      "           julia> Pkg.Registry.update()\n",
      "           julia> Pkg.add([Pkg.PackageSpec(name=\"PythonCall\", uuid=\"6099a3de-0909-46bc-b1f4-468b9a2dfc0d\"), Pkg.PackageSpec(name=\"SymbolicRegression\", uuid=\"8254be44-1295-4e6a-a16d-46603ac705cb\"), Pkg.PackageSpec(name=\"Serialization\", uuid=\"9e88b42a-f829-5b0c-bbe9-9e923198166b\")])\n",
      "           julia> Pkg.resolve()\n",
      "           julia> Pkg.precompile()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Installing known registries into `~/.julia`\n",
      "       Added `General` registry to ~/.julia/registries\n",
      "    Updating registry at `~/.julia/registries/General.toml`\n",
      "   Resolving package versions...\n",
      "   Installed IrrationalConstants ───────── v0.2.4\n",
      "   Installed DiffRules ─────────────────── v1.15.1\n",
      "   Installed MicroMamba ────────────────── v0.1.14\n",
      "   Installed ScientificTypesBase ───────── v3.0.0\n",
      "   Installed Adapt ─────────────────────── v4.2.0\n",
      "   Installed Tricks ────────────────────── v0.1.10\n",
      "   Installed Scratch ───────────────────── v1.2.1\n",
      "   Installed DynamicExpressions ────────── v1.10.0\n",
      "   Installed PtrArrays ─────────────────── v1.3.0\n",
      "   Installed MLJModelInterface ─────────── v1.11.0\n",
      "   Installed JSON3 ─────────────────────── v1.14.1\n",
      "   Installed ADTypes ───────────────────── v1.14.0\n",
      "   Installed DiffResults ───────────────── v1.1.0\n",
      "   Installed TableTraits ───────────────── v1.0.1\n",
      "   Installed Preferences ───────────────── v1.4.3\n",
      "   Installed PythonCall ────────────────── v0.9.24\n",
      "   Installed Parsers ───────────────────── v2.8.1\n",
      "   Installed StatisticalTraits ─────────── v3.4.0\n",
      "   Installed DataAPI ───────────────────── v1.16.0\n",
      "   Installed PositiveFactorizations ────── v0.2.4\n",
      "   Installed DynamicDiff ───────────────── v0.2.0\n",
      "   Installed Tables ────────────────────── v1.12.0\n",
      "   Installed SpecialFunctions ──────────── v2.5.0\n",
      "   Installed Optim ─────────────────────── v1.11.0\n",
      "   Installed StaticArraysCore ──────────── v1.4.3\n",
      "   Installed micromamba_jll ────────────── v1.5.8+0\n",
      "   Installed Pidfile ───────────────────── v1.3.0\n",
      "   Installed NaNMath ───────────────────── v1.1.2\n",
      "   Installed IteratorInterfaceExtensions ─ v1.0.0\n",
      "   Installed ProgressMeter ─────────────── v1.10.2\n",
      "   Installed JLLWrappers ───────────────── v1.7.0\n",
      "   Installed Setfield ──────────────────── v1.1.2\n",
      "   Installed DataValueInterfaces ───────── v1.0.0\n",
      "   Installed OrderedCollections ────────── v1.8.0\n",
      "   Installed ArrayInterface ────────────── v7.18.0\n",
      "   Installed NLSolversBase ─────────────── v7.8.3\n",
      "   Installed ConstructionBase ──────────── v1.5.8\n",
      "   Installed ForwardDiff ───────────────── v0.10.38\n",
      "   Installed PrecompileTools ───────────── v1.2.1\n",
      "   Installed ChainRulesCore ────────────── v1.25.1\n",
      "   Installed TestItems ─────────────────── v1.0.0\n",
      "   Installed LogExpFunctions ───────────── v0.3.29\n",
      "   Installed Statistics ────────────────── v1.11.1\n",
      "   Installed FillArrays ────────────────── v1.13.0\n",
      "   Installed LineSearches ──────────────── v7.3.0\n",
      "   Installed Reexport ──────────────────── v1.2.2\n",
      "   Installed CommonSubexpressions ──────── v0.3.1\n",
      "   Installed DataStructures ────────────── v0.18.20\n",
      "   Installed Requires ──────────────────── v1.3.1\n",
      "   Installed AliasTables ───────────────── v1.1.3\n",
      "   Installed OpenSpecFun_jll ───────────── v0.5.6+0\n",
      "   Installed UnPack ────────────────────── v1.0.2\n",
      "   Installed StatsAPI ──────────────────── v1.7.0\n",
      "   Installed Compat ────────────────────── v4.16.0\n",
      "   Installed MacroTools ────────────────── v0.5.15\n",
      "   Installed UnsafePointers ────────────── v1.0.0\n",
      "   Installed Parameters ────────────────── v0.12.3\n",
      "   Installed CondaPkg ──────────────────── v0.2.26\n",
      "   Installed DynamicQuantities ─────────── v1.5.1\n",
      "   Installed SymbolicRegression ────────── v1.9.0\n",
      "   Installed DispatchDoctor ────────────── v0.4.19\n",
      "   Installed SortingAlgorithms ─────────── v1.2.1\n",
      "   Installed StatsBase ─────────────────── v0.34.4\n",
      "   Installed DifferentiationInterface ──── v0.6.43\n",
      "   Installed Interfaces ────────────────── v0.3.2\n",
      "   Installed Missings ──────────────────── v1.2.0\n",
      "   Installed DocStringExtensions ───────── v0.9.3\n",
      "   Installed pixi_jll ──────────────────── v0.41.3+0\n",
      "   Installed StructTypes ───────────────── v1.11.0\n",
      "   Installed FiniteDiff ────────────────── v2.27.0\n",
      "   Installed LossFunctions ─────────────── v1.0.1\n",
      "    Updating `~/anaconda3/julia_env/Project.toml`\n",
      "  [6099a3de] + PythonCall v0.9.24\n",
      "  [8254be44] + SymbolicRegression v1.9.0\n",
      "  [9e88b42a] ~ Serialization ⇒ v1.11.0\n",
      "    Updating `~/anaconda3/julia_env/Manifest.toml`\n",
      "  [47edcb42] + ADTypes v1.14.0\n",
      "  [79e6a3ab] + Adapt v4.2.0\n",
      "  [66dad0bd] + AliasTables v1.1.3\n",
      "  [4fba245c] + ArrayInterface v7.18.0\n",
      "  [d360d2e6] + ChainRulesCore v1.25.1\n",
      "  [bbf7d656] + CommonSubexpressions v0.3.1\n",
      "  [34da2185] + Compat v4.16.0\n",
      "  [992eb4ea] + CondaPkg v0.2.26\n",
      "  [187b0558] + ConstructionBase v1.5.8\n",
      "  [9a962f9c] + DataAPI v1.16.0\n",
      "  [864edb3b] + DataStructures v0.18.20\n",
      "  [e2d170a0] + DataValueInterfaces v1.0.0\n",
      "  [163ba53b] + DiffResults v1.1.0\n",
      "  [b552c78f] + DiffRules v1.15.1\n",
      "  [a0c0ee7d] + DifferentiationInterface v0.6.43\n",
      "  [8d63f2c5] + DispatchDoctor v0.4.19\n",
      "  [ffbed154] + DocStringExtensions v0.9.3\n",
      "  [7317a516] + DynamicDiff v0.2.0\n",
      "  [a40a106e] + DynamicExpressions v1.10.0\n",
      "  [06fc5a27] + DynamicQuantities v1.5.1\n",
      "  [1a297f60] + FillArrays v1.13.0\n",
      "  [6a86dc24] + FiniteDiff v2.27.0\n",
      "  [f6369f11] + ForwardDiff v0.10.38\n",
      "  [85a1e053] + Interfaces v0.3.2\n",
      "  [92d709cd] + IrrationalConstants v0.2.4\n",
      "  [82899510] + IteratorInterfaceExtensions v1.0.0\n",
      "  [692b3bcd] + JLLWrappers v1.7.0\n",
      "  [0f8b85d8] + JSON3 v1.14.1\n",
      "  [d3d80556] + LineSearches v7.3.0\n",
      "  [2ab3a3ac] + LogExpFunctions v0.3.29\n",
      "  [30fc2ffe] + LossFunctions v1.0.1\n",
      "  [e80e1ace] + MLJModelInterface v1.11.0\n",
      "  [1914dd2f] + MacroTools v0.5.15\n",
      "  [0b3b1443] + MicroMamba v0.1.14\n",
      "  [e1d29d7a] + Missings v1.2.0\n",
      "  [d41bc354] + NLSolversBase v7.8.3\n",
      "  [77ba4419] + NaNMath v1.1.2\n",
      "  [429524aa] + Optim v1.11.0\n",
      "  [bac558e1] + OrderedCollections v1.8.0\n",
      "  [d96e819e] + Parameters v0.12.3\n",
      "  [69de0a69] + Parsers v2.8.1\n",
      "  [fa939f87] + Pidfile v1.3.0\n",
      "  [85a6dd25] + PositiveFactorizations v0.2.4\n",
      "  [aea7be01] + PrecompileTools v1.2.1\n",
      "  [21216c6a] + Preferences v1.4.3\n",
      "  [92933f4c] + ProgressMeter v1.10.2\n",
      "  [43287f4e] + PtrArrays v1.3.0\n",
      "  [6099a3de] + PythonCall v0.9.24\n",
      "  [189a3867] + Reexport v1.2.2\n",
      "  [ae029012] + Requires v1.3.1\n",
      "  [30f210dd] + ScientificTypesBase v3.0.0\n",
      "  [6c6a2e73] + Scratch v1.2.1\n",
      "  [efcf1570] + Setfield v1.1.2\n",
      "  [a2af1166] + SortingAlgorithms v1.2.1\n",
      "  [276daf66] + SpecialFunctions v2.5.0\n",
      "  [1e83bf80] + StaticArraysCore v1.4.3\n",
      "  [64bff920] + StatisticalTraits v3.4.0\n",
      "  [10745b16] + Statistics v1.11.1\n",
      "  [82ae8749] + StatsAPI v1.7.0\n",
      "  [2913bbd2] + StatsBase v0.34.4\n",
      "  [856f2bd8] + StructTypes v1.11.0\n",
      "  [8254be44] + SymbolicRegression v1.9.0\n",
      "  [3783bdb8] + TableTraits v1.0.1\n",
      "  [bd369af6] + Tables v1.12.0\n",
      "  [1c621080] + TestItems v1.0.0\n",
      "  [410a4b4d] + Tricks v0.1.10\n",
      "  [3a884ed6] + UnPack v1.0.2\n",
      "  [e17b2a0c] + UnsafePointers v1.0.0\n",
      "  [efe28fd5] + OpenSpecFun_jll v0.5.6+0\n",
      "  [f8abcde7] + micromamba_jll v1.5.8+0\n",
      "  [4d7b5844] + pixi_jll v0.41.3+0\n",
      "  [0dad84c5] + ArgTools v1.1.2\n",
      "  [56f22d72] + Artifacts v1.11.0\n",
      "  [2a0f44e3] + Base64 v1.11.0\n",
      "  [ade2ca70] + Dates v1.11.0\n",
      "  [8ba89e20] + Distributed v1.11.0\n",
      "  [f43a241f] + Downloads v1.6.0\n",
      "  [7b1f6079] + FileWatching v1.11.0\n",
      "  [9fa8497b] + Future v1.11.0\n",
      "  [b77e0a4c] + InteractiveUtils v1.11.0\n",
      "  [4af54fe1] + LazyArtifacts v1.11.0\n",
      "  [b27032c2] + LibCURL v0.6.4\n",
      "  [76f85450] + LibGit2 v1.11.0\n",
      "  [8f399da3] + Libdl v1.11.0\n",
      "  [37e2e46d] + LinearAlgebra v1.11.0\n",
      "  [56ddb016] + Logging v1.11.0\n",
      "  [d6f4376e] + Markdown v1.11.0\n",
      "  [a63ad114] + Mmap v1.11.0\n",
      "  [ca575930] + NetworkOptions v1.2.0\n",
      "  [44cfe95a] + Pkg v1.11.0\n",
      "  [de0858da] + Printf v1.11.0\n",
      "  [9a3f8284] + Random v1.11.0\n",
      "  [ea8e919c] + SHA v0.7.0\n",
      "  [9e88b42a] + Serialization v1.11.0\n",
      "  [6462fe0b] + Sockets v1.11.0\n",
      "  [2f01184e] + SparseArrays v1.11.0\n",
      "  [f489334b] + StyledStrings v1.11.0\n",
      "  [fa267f1f] + TOML v1.0.3\n",
      "  [a4e569a6] + Tar v1.10.0\n",
      "  [8dfed614] + Test v1.11.0\n",
      "  [cf7118a7] + UUIDs v1.11.0\n",
      "  [4ec0a83e] + Unicode v1.11.0\n",
      "  [e66e0078] + CompilerSupportLibraries_jll v1.1.1+0\n",
      "  [deac9b47] + LibCURL_jll v8.6.0+0\n",
      "  [e37daf67] + LibGit2_jll v1.7.2+0\n",
      "  [29816b5a] + LibSSH2_jll v1.11.0+1\n",
      "  [c8ffd9c3] + MbedTLS_jll v2.28.6+0\n",
      "  [14a3606d] + MozillaCACerts_jll v2023.12.12\n",
      "  [4536629a] + OpenBLAS_jll v0.3.27+1\n",
      "  [05823500] + OpenLibm_jll v0.8.1+2\n",
      "  [bea87d4a] + SuiteSparse_jll v7.7.0+0\n",
      "  [83775a58] + Zlib_jll v1.2.13+1\n",
      "  [8e850b90] + libblastrampoline_jll v5.11.0+0\n",
      "  [8e850ede] + nghttp2_jll v1.59.0+0\n",
      "  [3f19e933] + p7zip_jll v17.4.0+2\n",
      "Precompiling project...\n",
      "    776.1 ms  ✓ DataValueInterfaces\n",
      "    777.4 ms  ✓ FillArrays\n",
      "    778.3 ms  ✓ UnPack\n",
      "    802.7 ms  ✓ OrderedCollections\n",
      "    776.2 ms  ✓ ScientificTypesBase\n",
      "    387.0 ms  ✓ Reexport\n",
      "    420.7 ms  ✓ UnsafePointers\n",
      "    467.3 ms  ✓ PositiveFactorizations\n",
      "    548.2 ms  ✓ Statistics\n",
      "    333.7 ms  ✓ IteratorInterfaceExtensions\n",
      "    415.0 ms  ✓ ConstructionBase\n",
      "    392.6 ms  ✓ Tricks\n",
      "    386.5 ms  ✓ DataAPI\n",
      "    959.3 ms  ✓ IrrationalConstants\n",
      "    367.4 ms  ✓ StatsAPI\n",
      "    300.5 ms  ✓ TestItems\n",
      "    331.6 ms  ✓ PtrArrays\n",
      "    525.7 ms  ✓ ADTypes\n",
      "    425.8 ms  ✓ Interfaces\n",
      "    427.3 ms  ✓ StaticArraysCore\n",
      "   1117.8 ms  ✓ ProgressMeter\n",
      "    663.3 ms  ✓ NaNMath\n",
      "    509.6 ms  ✓ Requires\n",
      "    362.2 ms  ✓ Scratch\n",
      "    383.6 ms  ✓ Pidfile\n",
      "    724.5 ms  ✓ StructTypes\n",
      "   1906.9 ms  ✓ MacroTools\n",
      "    543.9 ms  ✓ Compat\n",
      "    524.2 ms  ✓ Preferences\n",
      "    439.3 ms  ✓ DocStringExtensions\n",
      "    748.2 ms  ✓ StatisticalTraits\n",
      "   1378.7 ms  ✓ Parameters\n",
      "    678.5 ms  ✓ TableTraits\n",
      "   1416.7 ms  ✓ FillArrays → FillArraysStatisticsExt\n",
      "    549.6 ms  ✓ ConstructionBase → ConstructionBaseLinearAlgebraExt\n",
      "    692.5 ms  ✓ Missings\n",
      "    753.1 ms  ✓ AliasTables\n",
      "    433.6 ms  ✓ ADTypes → ADTypesConstructionBaseExt\n",
      "    348.1 ms  ✓ Adapt\n",
      "    485.5 ms  ✓ DiffResults\n",
      "    993.4 ms  ✓ DifferentiationInterface\n",
      "    518.5 ms  ✓ LossFunctions\n",
      "    514.7 ms  ✓ CommonSubexpressions\n",
      "    627.8 ms  ✓ Compat → CompatLinearAlgebraExt\n",
      "    639.4 ms  ✓ PrecompileTools\n",
      "    638.1 ms  ✓ JLLWrappers\n",
      "   1139.2 ms  ✓ DispatchDoctor\n",
      "   1033.3 ms  ✓ MLJModelInterface\n",
      "   1372.1 ms  ✓ LogExpFunctions\n",
      "   5587.4 ms  ✓ Statistics → SparseArraysExt\n",
      "   5854.1 ms  ✓ FillArrays → FillArraysSparseArraysExt\n",
      "    437.5 ms  ✓ ArrayInterface\n",
      "    902.9 ms  ✓ Adapt → AdaptSparseArraysExt\n",
      "   1235.7 ms  ✓ Setfield\n",
      "   1725.0 ms  ✓ Tables\n",
      "    771.6 ms  ✓ DifferentiationInterface → DifferentiationInterfaceSparseArraysExt\n",
      "    957.1 ms  ✓ ChainRulesCore\n",
      "   1323.8 ms  ✓ micromamba_jll\n",
      "   1214.7 ms  ✓ pixi_jll\n",
      "   1778.0 ms  ✓ DataStructures\n",
      "    272.9 ms  ✓ ArrayInterface → ArrayInterfaceStaticArraysCoreExt\n",
      "    614.5 ms  ✓ ArrayInterface → ArrayInterfaceSparseArraysExt\n",
      "   1524.8 ms  ✓ OpenSpecFun_jll\n",
      "    607.9 ms  ✓ ADTypes → ADTypesChainRulesCoreExt\n",
      "    882.2 ms  ✓ ChainRulesCore → ChainRulesCoreSparseArraysExt\n",
      "    408.9 ms  ✓ DifferentiationInterface → DifferentiationInterfaceChainRulesCoreExt\n",
      "    495.6 ms  ✓ DispatchDoctor → DispatchDoctorChainRulesCoreExt\n",
      "    475.7 ms  ✓ ArrayInterface → ArrayInterfaceChainRulesCoreExt\n",
      "    715.2 ms  ✓ SortingAlgorithms\n",
      "   1382.8 ms  ✓ LogExpFunctions → LogExpFunctionsChainRulesCoreExt\n",
      "    586.2 ms  ✓ FiniteDiff\n",
      "   3815.8 ms  ✓ DynamicQuantities\n",
      "   2965.3 ms  ✓ MicroMamba\n",
      "   2231.2 ms  ✓ SpecialFunctions\n",
      "    695.3 ms  ✓ FiniteDiff → FiniteDiffSparseArraysExt\n",
      "    489.2 ms  ✓ DifferentiationInterface → DifferentiationInterfaceFiniteDiffExt\n",
      "    831.8 ms  ✓ DynamicQuantities → DynamicQuantitiesLinearAlgebraExt\n",
      "    494.3 ms  ✓ DiffRules\n",
      "   2944.2 ms  ✓ StatsBase\n",
      "   1505.6 ms  ✓ SpecialFunctions → SpecialFunctionsChainRulesCoreExt\n",
      "   3117.5 ms  ✓ ForwardDiff\n",
      "   1008.1 ms  ✓ DifferentiationInterface → DifferentiationInterfaceForwardDiffExt\n",
      "   1244.8 ms  ✓ NLSolversBase\n",
      "   9210.3 ms  ✓ DynamicExpressions\n",
      "   1781.6 ms  ✓ LineSearches\n",
      "  14728.1 ms  ✓ Parsers\n",
      "   2096.6 ms  ✓ DynamicDiff\n",
      "   3768.0 ms  ✓ Optim\n",
      "    953.3 ms  ✓ DynamicExpressions → DynamicExpressionsOptimExt\n",
      "   6178.8 ms  ✓ JSON3\n",
      "   1868.7 ms  ✓ CondaPkg\n",
      "   8674.2 ms  ✓ PythonCall\n",
      "  38773.1 ms  ✓ SymbolicRegression\n",
      "   2522.2 ms  ✓ SymbolicRegression → SymbolicRegressionJSON3Ext\n",
      "  94 dependencies successfully precompiled in 70 seconds. 35 already precompiled.\n",
      "  1 dependency had output during precompilation:\n",
      "┌ MicroMamba\n",
      "│  Downloading artifact: micromamba\n",
      "└  \n",
      "  No Changes to `~/anaconda3/julia_env/Project.toml`\n",
      "  No Changes to `~/anaconda3/julia_env/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "!pip install pysr\n",
    "from pysr import PySRRegressor\n",
    "import warnings\n",
    "from argparse import ArgumentParser\n",
    "from os import makedirs\n",
    "from pickle import dump, load\n",
    "from shutil import rmtree\n",
    "from time import process_time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sympy\n",
    "import torch\n",
    "import torch_geometric as pyg\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "import gnn\n",
    "import utils\n",
    "\n",
    "sns.set()\n",
    "sns.set_style(\"white\")\n",
    "warnings.filterwarnings('ignore')\n",
    " \n",
    "start_time = process_time()\n",
    "\n",
    "pysrmodel = PySRRegressor(\n",
    "    unary_operators = [\"Not(x) = (x <= zero(x)) * one(x)\"],\n",
    "    binary_operators = [\n",
    "        \"And(x, y) = ((x > zero(x)) & (y > zero(y))) * one(x)\",\n",
    "        \"Or(x, y)  = ((x > zero(x)) | (y > zero(y))) * one(x)\",\n",
    "        \"Xor(x, y) = (((x > 0) & (y <= 0)) | ((x <= 0) & (y > 0))) * 1f0\",\n",
    "    ],\n",
    "    extra_sympy_mappings = {\n",
    "        \"Not\": lambda x: sympy.Piecewise((1.0, (x <= 0)), (0.0, True)),\n",
    "        \"And\": lambda x, y: sympy.Piecewise((1.0, (x > 0) & (y > 0)), (0.0, True)),\n",
    "        \"Or\":  lambda x, y: sympy.Piecewise((1.0, (x > 0) | (y > 0)), (0.0, True)),\n",
    "        \"Xor\": lambda x, y: sympy.Piecewise((1.0, (x > 0) ^ (y > 0)), (0.0, True)),\n",
    "    },\n",
    "\n",
    "    elementwise_loss = \"loss(prediction, target) = sum(prediction != target)\",\n",
    "    model_selection=\"accuracy\",\n",
    "\n",
    "    complexity_of_variables=args.c,\n",
    "    complexity_of_operators={'Not': args.c, 'And': args.c, 'Or': args.c, 'Xor': args.c},\n",
    "\n",
    "    select_k_features = min(args.k, 10),\n",
    "    weights = pysr_weights,\n",
    "\n",
    "    batch_size = 32,\n",
    "\n",
    "    # Paperwork\n",
    "    temp_equation_file = True,\n",
    "    delete_tempfiles = True,\n",
    "\n",
    "    # Determinism\n",
    "    procs=0,\n",
    "    deterministic=True,\n",
    "    multithreading=False,\n",
    "    random_state=0,\n",
    "    warm_start=False,\n",
    ")\n",
    "\n",
    "def cal_pysr_acc(X, Y, index=None):\n",
    "    Y = np.array(Y)\n",
    "    Y_pred = pysrmodel.predict(X, index=index)\n",
    "    assert Y.shape == Y_pred.shape , \"Shape mismatch!\"\n",
    "    return (Y_pred == Y).sum() / len(Y)\n",
    "\n",
    "\n",
    "pysrmodel.fit(x_train_bin, train_pred)\n",
    "print(pysrmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
