{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[572, 128], edge_index=[2, 52112], edge_type=[52112])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load the tensor and map it to the CPU\n",
    "data = torch.load('data_o_new2.pt', map_location=torch.device('cpu'))\n",
    "\n",
    "# Print the shape of the tensor\n",
    "print(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MRCGNN Revised Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import RGCNConv\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, n_h):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.f_k = nn.Bilinear(32, 32, 1)\n",
    "        for m in self.modules():\n",
    "            self.weights_init(m)\n",
    "    def weights_init(self, m):\n",
    "        if isinstance(m, nn.Bilinear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.fill_(0.0)\n",
    "    def forward(self, c, h_pl, h_mi, s_bias1=None, s_bias2=None):\n",
    "        c_x = c.expand_as(h_pl)\n",
    "        sc_1 = self.f_k(h_pl, c_x)\n",
    "        sc_2 = self.f_k(h_mi, c_x)\n",
    "        if s_bias1 is not None:\n",
    "            sc_1 += s_bias1\n",
    "        if s_bias2 is not None:\n",
    "            sc_2 += s_bias2\n",
    "        logits = torch.cat((sc_1, sc_2), 1)\n",
    "        return logits\n",
    "\n",
    "class AvgReadout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AvgReadout, self).__init__()\n",
    "    def forward(self, seq, msk=None):\n",
    "        if msk is None:\n",
    "            return torch.mean(seq, 0)\n",
    "        else:\n",
    "            msk = torch.unsqueeze(msk, -1)\n",
    "            return torch.sum(seq * msk, 0) / torch.sum(msk)\n",
    "\n",
    "class MRCGNN(nn.Module):\n",
    "    def __init__(self, feature, hidden1, hidden2, decoder1, dropout, zhongzi):\n",
    "        super(MRCGNN, self).__init__()\n",
    "\n",
    "        # RGCN layers for the main (data_o) branch\n",
    "        self.encoder_o1 = RGCNConv(feature, hidden1, num_relations=65)\n",
    "        self.encoder_o2 = RGCNConv(hidden1, hidden2, num_relations=65)\n",
    "\n",
    "        # Two-element parameter for layer attention\n",
    "        self.attt = nn.Parameter(torch.tensor([0.5, 0.5]))\n",
    "        self.disc = Discriminator(hidden2 * 2)\n",
    "        self.dropout = dropout\n",
    "        self.sigm = nn.Sigmoid()\n",
    "        self.read = AvgReadout()\n",
    "        \n",
    "        # Final classifier: prediction solely from data_o branch.\n",
    "        # Each node's final representation is a concatenation of (hidden1 + hidden2).\n",
    "        # For a pair of entities, the dimension becomes 2*(hidden1+hidden2).\n",
    "        self.classifier = nn.Linear(2 * (hidden1 + hidden2), 65)\n",
    "\n",
    "        # We no longer load any pretrained features for skip connection.\n",
    "\n",
    "    def forward(self, data_o, data_s, data_a, idx):\n",
    "        # Process data_o branch\n",
    "        x_o, adj, e_type = data_o.x, data_o.edge_index, data_o.edge_type\n",
    "        e_type1 = data_a.edge_type\n",
    "        e_type = torch.tensor(e_type, dtype=torch.int64)\n",
    "        e_type1 = torch.tensor(e_type1, dtype=torch.int64)\n",
    "\n",
    "        # Main branch for prediction (data_o)\n",
    "        x1_o = F.relu(self.encoder_o1(x_o, adj, e_type))\n",
    "        x1_o = F.dropout(x1_o, self.dropout, training=self.training)\n",
    "        x2_o = self.encoder_o2(x1_o, adj, e_type)\n",
    "\n",
    "        # Contrastive learning branches (unused in prediction)\n",
    "        x_a = data_s.x\n",
    "        x1_o_a = F.relu(self.encoder_o1(x_a, adj, e_type))\n",
    "        x1_o_a = F.dropout(x1_o_a, self.dropout, training=self.training)\n",
    "        x2_o_a = self.encoder_o2(x1_o_a, adj, e_type)\n",
    "\n",
    "        x1_o_a_a = F.relu(self.encoder_o1(x_o, adj, e_type1))\n",
    "        x1_o_a_a = F.dropout(x1_o_a_a, self.dropout, training=self.training)\n",
    "        x2_o_a_a = self.encoder_o2(x1_o_a_a, adj, e_type1)\n",
    "\n",
    "        # Readout for contrastive learning\n",
    "        h_os = self.read(x2_o)\n",
    "        h_os = self.sigm(h_os)\n",
    "        ret_os = self.disc(h_os, x2_o, x2_o_a)\n",
    "        ret_os_a = self.disc(h_os, x2_o, x2_o_a_a)\n",
    "\n",
    "        # For final prediction, use only data_o branch:\n",
    "        final = torch.cat((self.attt[0] * x1_o, self.attt[1] * x2_o), dim=1)\n",
    "\n",
    "        a = [int(i) for i in list(idx[0])]\n",
    "        b = [int(i) for i in list(idx[1])]\n",
    "        aa = torch.tensor(a, dtype=torch.long)\n",
    "        bb = torch.tensor(b, dtype=torch.long)\n",
    "        entity1 = final[aa]\n",
    "        entity2 = final[bb]\n",
    "        concatenate = torch.cat((entity1, entity2), dim=1)\n",
    "        log = self.classifier(concatenate)\n",
    "\n",
    "        return log, ret_os, ret_os_a, x2_o\n",
    "\n",
    "    def predict(self, data_o, idx):\n",
    "        \"\"\"\n",
    "        New prediction method that uses only data_o and idx.\n",
    "        \"\"\"\n",
    "        x_o, adj, e_type = data_o.x, data_o.edge_index, data_o.edge_type\n",
    "        e_type = torch.tensor(e_type, dtype=torch.int64)\n",
    "        # Process the main branch\n",
    "        x1_o = F.relu(self.encoder_o1(x_o, adj, e_type))\n",
    "        x1_o = F.dropout(x1_o, self.dropout, training=self.training)\n",
    "        x2_o = self.encoder_o2(x1_o, adj, e_type)\n",
    "        final = torch.cat((self.attt[0] * x1_o, self.attt[1] * x2_o), dim=1)\n",
    "        \n",
    "        a = [int(i) for i in list(idx[0])]\n",
    "        b = [int(i) for i in list(idx[1])]\n",
    "        aa = torch.tensor(a, dtype=torch.long)\n",
    "        bb = torch.tensor(b, dtype=torch.long)\n",
    "        entity1 = final[aa]\n",
    "        entity2 = final[bb]\n",
    "        concatenate = torch.cat((entity1, entity2), dim=1)\n",
    "        log = self.classifier(concatenate)\n",
    "        return log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load My Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MRCGNN(\n",
       "  (encoder_o1): RGCNConv(128, 64, num_relations=65)\n",
       "  (encoder_o2): RGCNConv(64, 32, num_relations=65)\n",
       "  (disc): Discriminator(\n",
       "    (f_k): Bilinear(in1_features=32, in2_features=32, out_features=1, bias=True)\n",
       "  )\n",
       "  (sigm): Sigmoid()\n",
       "  (read): AvgReadout()\n",
       "  (classifier): Linear(in_features=192, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MRCGNN(feature=128, hidden1=64, hidden2=32, decoder1=512, dropout=0.5, zhongzi=0)\n",
    "model.load_state_dict(torch.load(\"model_mrcgnn.pt\", map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "model.load_state_dict(torch.load(\"model_mrcgnn.pt\", map_location='cpu'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data object: Data(x=[572, 128], edge_index=[2, 52112], edge_type=[52112])\n",
      "Number of unique ctree codes: 24\n",
      "Concept vector (frequency of each code):\n",
      "[522   5   4   1   9   9   1   1   2   1   1   1   1   1   3   1   1   1\n",
      "   1   1   1   1   1   2]\n",
      "Sample unique computation tree codes and frequencies:\n",
      "Code 0: 0-408-22-477-474-214-16-130-182-365-33-63-57-240-250-8-527-366-223-113-233-31-311-154-61-228-437-426-310-161-418-274-567-13-406-279-60-346-114-19-320-398-166-53-120-211-556-502-348-563-522-18-40-545-224-225-202-321-388-503-383-167-352-193-419-410-111-183-370-330-194-252-264-151-273-349-195-216-363-507-534-47-128-123-253-255-288-394-54-555-282-77-354-324-339-405-99-44-226-520-17-209-326-415-298-43-266-338-490-483-89-548-169-500-376-291-145-64-441-488-323-565-529-178-95-160-232-347-105-454-414-355-198-399-448-436-269-157-28-29-424-480-416-49-132-51-537-335-340-242-119-236-88-485-146-501-450-131-258-452-382-482-11-20-333-451-542-290-286-245-515-237-7-155-281-197-73-261-91-510-430-557-2-400-74-239-521-344-308-142-259-125-523-107-412-463-350-547-517-277-423-249-327-205-177-404-70-76-129-552-289-23-117-94-434-312-316-464-307-428-374-26-38-447-254-30-90-543-553-25-409-62-46-148-116-139-505-385-564-328-518-554-473-292-152-325-478-562-55-71-271-378-4-421-445-102-75-72-471-235-244-343-106-498-185-381-425-230-508-103-389-351-127-509-156-56-67-59-551-462-372-215-429-220-494-37-41-293-32-163-456-427-486-549-172-511-108-284-48-97-303-391-256-317-212-492-93-98-396-217-285-184-83-65-458-191-208-299-100-221-190-519-12-82-489-460-144-137-287-438-115-229-341-210-104-35-276-302-162-14-484-544-173-247-109-295-470-530-435-36-262-168-3-407-359-134-80-175-313-468-411-401-136-126-402-85-39-524-66-78-373-86-9-227-6-453-280-238-135-377-329-367-417-443-270-353-170-159-263-213-332-360-395-539-118-68-199-188-469-432-479-356-187-87-497-181-446-439-322-491-268-24-369-504-362-309-192-153-81-174-231-532-431-304-526-513-110-141-345-257-260-558-300-533-384-525-241-422-461-481-357-440-171-272-283-234-246-496-397-442-387-200-79-218-459-512-50-493-101-318-334-331-455-45-138-560-516-314-207-122-186-296-566-559-541-506-203-379-112-550-219-179-278-92-149-27-58-251-222-420-96-204-305-15-5-457-275-10-476-531-306-69-164-301-393-265-294-371-147-248-536-392-465-243-297-364-358-413-140-1-466-201-176-206-143-380-158-433-315-375-528-150-165-514-538-34-319-535-342-124-499-449-561-368-21-84-475-386-361-336-390-403-189-337-42-540-444-267-487-180-196-467-121-495-472-546-133-52\n",
      "Frequency: 522\n",
      "\n",
      "Code 1: 0-407-22-475-472-213-16-130-181-364-33-63-57-239-249-8-525-365-222-113-232-31-310-154-61-227-436-425-309-161-417-273-565-13-405-278-60-345-114-19-319-397-165-53-120-210-554-500-347-561-520-18-40-543-223-224-201-320-387-501-382-166-351-192-418-409-111-182-369-329-193-251-263-151-272-348-194-215-362-505-532-47-128-123-252-254-287-393-54-553-281-77-353-323-338-404-99-44-225-518-17-208-325-414-297-43-265-337-488-481-89-546-168-498-375-290-145-64-440-486-322-563-527-177-95-160-231-346-105-452-413-354-197-398-446-435-268-157-28-29-423-478-415-49-132-51-535-334-339-241-119-235-88-483-146-499-448-131-257-450-381-480-11-20-332-449-540-289-285-244-513-236-7-155-280-196-73-260-91-508-429-555-2-399-74-238-519-343-307-142-258-125-521-107-411-461-349-545-515-276-422-248-326-204-176-403-70-76-129-550-288-23-117-94-433-311-315-462-306-427-373-26-38-445-253-30-90-541-551-25-408-62-46-148-116-139-503-384-562-327-516-552-471-291-152-324-476-560-55-71-270-377-4-420-443-102-75-72-469-234-243-342-106-496-184-380-424-229-506-103-388-350-127-507-156-56-67-59-549-460-371-214-428-219-492-37-41-292-32-163-454-426-484-547-171-509-108-283-48-97-302-390-255-316-211-490-93-98-395-216-284-183-83-65-456-190-207-298-100-220-189-517-12-82-487-458-144-137-286-437-115-228-340-209-104-35-275-301-162-14-482-542-172-246-109-294-468-528-434-36-261-167-3-406-358-134-80-174-312-466-410-400-136-126-401-85-39-522-66-78-372-86-9-226-6-451-279-237-135-376-328-366-416-442-269-352-169-159-262-212-331-359-394-537-118-68-198-187-467-431-477-355-186-87-495-180-444-438-321-489-267-24-368-502-361-308-191-153-81-173-230-530-430-303-524-511-110-141-344-256-259-556-299-531-383-523-240-421-459-479-356-439-170-271-282-233-245-494-396-441-386-199-79-217-457-510-50-491-101-317-333-330-453-45-138-558-514-313-206-122-185-295-564-557-539-504-202-378-112-548-218-178-277-92-149-27-58-250-221-419-96-203-304-15-5-455-274-10-474-529-305-69-164-300-392-264-293-370-147-247-534-391-463-242-296-363-357-412-140-1-464-200-175-205-143-379-158-432-314-374-526-150-512-536-34-318-533-341-124-497-447-559-367-21-84-473-385-360-335-389-402-188-336-42-538-266-485-179-195-465-121-493-470-544-133-52\n",
      "Frequency: 5\n",
      "\n",
      "Code 2: 0\n",
      "Frequency: 4\n",
      "\n",
      "Code 3: 0-403-22-472-469-210-16-130-179-361-33-63-57-236-246-8-522-362-219-113-229-31-307-153-61-224-432-421-306-160-413-270-562-13-401-275-60-342-114-19-316-393-164-53-120-207-551-497-344-558-517-18-40-540-220-221-198-317-384-498-379-165-348-190-414-405-111-180-366-326-191-248-260-150-269-345-192-212-359-502-529-47-128-123-249-251-284-389-54-550-278-77-350-320-335-400-99-44-222-515-17-205-322-410-294-43-262-334-485-478-89-543-167-495-372-287-144-64-436-483-319-560-524-176-95-159-228-343-105-449-409-351-194-394-443-431-265-156-28-29-419-475-411-49-132-51-532-331-336-238-119-232-88-480-145-496-445-131-254-447-378-477-11-20-329-446-537-286-282-241-510-233-7-154-277-193-73-257-91-505-425-552-2-395-74-235-516-340-304-141-255-125-518-107-407-458-346-542-512-273-418-245-323-201-175-399-70-76-129-547-285-23-117-94-429-308-312-459-303-423-370-26-38-442-250-30-90-538-548-25-404-62-46-147-116-138-500-381-559-324-513-549-468-288-151-321-473-557-55-71-267-374-4-416-440-102-75-72-466-231-240-339-106-493-182-377-420-226-503-103-385-347-127-504-155-56-67-59-546-457-368-211-424-216-489-37-41-289-32-162-451-422-481-544-170-506-108-280-48-97-299-386-252-313-208-487-93-98-391-213-281-181-83-65-453-188-204-295-100-217-187-514-12-82-484-455-143-136-283-433-115-225-337-206-104-35-272-298-161-14-479-539-171-243-109-291-465-525-430-36-258-166-3-402-355-133-80-173-309-463-406-396-135-126-397-85-39-519-66-78-369-86-9-223-6-448-276-234-134-373-325-363-412-438-266-349-168-158-259-209-328-356-390-534-118-68-195-185-464-427-474-352-184-87-492-178-441-434-318-486-264-24-365-499-358-305-189-152-81-172-227-527-426-300-521-508-110-140-341-253-256-553-296-528-380-520-237-417-456-476-353-435-169-268-279-230-242-491-392-437-383-196-79-214-454-507-50-488-101-314-330-327-450-45-137-555-511-310-203-122-183-292-561-554-536-501-199-375-112-545-215-177-274-92-148-27-58-247-218-415-96-200-301-15-5-452-271-10-471-526-302-69-163-297-388-261-290-367-146-244-531-387-460-239-293-360-354-408-139-1-461-197-174-202-142-376-157-428-311-371-523-149-509-533-34-315-530-338-124-494-444-556-364-21-84-470-382-357-332-398-186-333-42-535-439-263-482-462-121-490-467-541-52\n",
      "Frequency: 1\n",
      "\n",
      "Code 4: 0-408-22-476-473-214-16-130-182-365-33-63-57-240-250-8-526-366-223-113-233-31-311-154-61-228-437-426-310-161-418-274-566-13-406-279-60-346-114-19-320-398-166-53-120-211-555-501-348-562-521-18-40-544-224-225-202-321-388-502-383-167-352-193-419-410-111-183-370-330-194-252-264-151-273-349-195-216-363-506-533-47-128-123-253-255-288-394-54-554-282-77-354-324-339-405-99-44-226-519-17-209-326-415-298-43-266-338-489-482-89-547-169-499-376-291-145-64-441-487-323-564-528-178-95-160-232-347-105-453-414-355-198-399-447-436-269-157-28-29-424-479-416-49-132-51-536-335-340-242-119-236-88-484-146-500-449-131-258-451-382-481-11-20-333-450-541-290-286-245-514-237-7-155-281-197-73-261-91-509-430-556-2-400-74-239-520-344-308-142-259-125-522-107-412-462-350-546-516-277-423-249-327-205-177-404-70-76-129-551-289-23-117-94-434-312-316-463-307-428-374-26-38-446-254-30-90-542-552-25-409-62-46-148-116-139-504-385-563-328-517-553-472-292-152-325-477-561-55-71-271-378-4-421-444-102-75-72-470-235-244-343-106-497-185-381-425-230-507-103-389-351-127-508-156-56-67-59-550-461-372-215-429-220-493-37-41-293-32-163-455-427-485-548-172-510-108-284-48-97-303-391-256-317-212-491-93-98-396-217-285-184-83-65-457-191-208-299-100-221-190-518-12-82-488-459-144-137-287-438-115-229-341-210-104-35-276-302-162-14-483-543-173-247-109-295-469-529-435-36-262-168-3-407-359-134-80-175-313-467-411-401-136-126-402-85-39-523-66-78-373-86-9-227-6-452-280-238-135-377-329-367-417-443-270-353-170-159-263-213-332-360-395-538-118-68-199-188-468-432-478-356-187-87-496-181-445-439-322-490-268-24-369-503-362-309-192-153-81-174-231-531-431-304-525-512-110-141-345-257-260-557-300-532-384-524-241-422-460-480-357-440-171-272-283-234-246-495-397-442-387-200-79-218-458-511-50-492-101-318-334-331-454-45-138-559-515-314-207-122-186-296-565-558-540-505-203-379-112-549-219-179-278-92-149-27-58-251-222-420-96-204-305-15-5-456-275-10-475-530-306-69-164-301-393-265-294-371-147-248-535-392-464-243-297-364-358-413-140-1-465-201-176-206-143-380-158-433-315-375-527-150-165-513-537-34-319-534-342-124-498-448-560-368-21-84-474-386-361-336-390-403-189-337-42-539-267-486-180-196-466-121-494-471-545-133-52\n",
      "Frequency: 9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric as pyg\n",
    "from torch_geometric.utils import k_hop_subgraph, to_networkx\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "##############################################################################\n",
    "# 1) LOAD YOUR DATA\n",
    "##############################################################################\n",
    "data = torch.load('data_o_new2.pt', map_location=torch.device('cpu'))\n",
    "print(\"Data object:\", data)\n",
    "# data is a single graph with data.x, data.edge_index, data.edge_type, etc.\n",
    "# e.g.: Data(x=[572, 128], edge_index=[2, 52112], edge_type=[52112])\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "# 2) EXTRACT COMPUTATION TREES & CREATE CONCEPT VECTOR\n",
    "##############################################################################\n",
    "L = 3  # number of hops for each node’s subgraph (the “computation tree”)\n",
    "\n",
    "# We'll collect:\n",
    "#   node_ctree_codes[v] = string code for node v’s L-hop subgraph\n",
    "#   unique_ctree_codes   = dict {code_str -> assigned_id}\n",
    "node_ctree_codes = []\n",
    "unique_ctree_codes = {}\n",
    "\n",
    "def simple_dfs_code(G, root):\n",
    "    \"\"\"\n",
    "    Simple DFS code from a NetworkX graph G (treated as a tree),\n",
    "    starting at node 'root'. This is just a placeholder function:\n",
    "    you'd use a canonical labeling or something more robust in production.\n",
    "    \"\"\"\n",
    "    code = []\n",
    "    for n in nx.dfs_preorder_nodes(G, source=root):\n",
    "        code.append(str(n))\n",
    "    return \"-\".join(code)\n",
    "\n",
    "for v in range(data.num_nodes):\n",
    "    # Extract L-hop subgraph\n",
    "    subset, sub_edge_index, mapping, _ = k_hop_subgraph(\n",
    "        node_idx=v,\n",
    "        num_hops=L,\n",
    "        edge_index=data.edge_index,\n",
    "        relabel_nodes=True\n",
    "    )\n",
    "    # Build a small Data object\n",
    "    sub_data = pyg.data.Data(\n",
    "        x=data.x[subset],\n",
    "        edge_index=sub_edge_index\n",
    "    )\n",
    "    # Convert to NetworkX\n",
    "    G_sub = to_networkx(sub_data, to_undirected=True)\n",
    "    \n",
    "    # In PyG’s relabel_nodes=True, the root node v becomes \"0\" in sub_data.\n",
    "    ctree_code = simple_dfs_code(G_sub, root=0)\n",
    "    node_ctree_codes.append(ctree_code)\n",
    "    if ctree_code not in unique_ctree_codes:\n",
    "        unique_ctree_codes[ctree_code] = len(unique_ctree_codes)\n",
    "\n",
    "# --- Step 2: Build the concept vector for the entire graph. ---\n",
    "# This vector is length (# unique ctree codes),\n",
    "# and entry i = count of nodes that have that code.\n",
    "concept_vector = np.zeros(len(unique_ctree_codes), dtype=int)\n",
    "for code in node_ctree_codes:\n",
    "    idx = unique_ctree_codes[code]\n",
    "    concept_vector[idx] += 1\n",
    "\n",
    "print(\"Number of unique ctree codes:\", len(unique_ctree_codes))\n",
    "print(\"Concept vector (frequency of each code):\")\n",
    "print(concept_vector)\n",
    "\n",
    "# Print out the first 5 unique computation tree codes and their frequencies:\n",
    "print(\"Sample unique computation tree codes and frequencies:\")\n",
    "for i, (code, idx) in enumerate(unique_ctree_codes.items()):\n",
    "    if i >= 5:\n",
    "        break\n",
    "    freq = concept_vector[idx]\n",
    "    print(f\"Code {idx}: {code}\")\n",
    "    print(f\"Frequency: {freq}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEFINE \"REMOVE CONCEPTS\" FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_concepts_from_graph(data, node_ctree_codes, unique_ctree_codes, subset_of_codes):\n",
    "    \"\"\"\n",
    "    Prune out nodes whose L-hop code is not in 'subset_of_codes'.\n",
    "    Returns a new Data object and a mapping dict (old_index -> new_index).\n",
    "    \"\"\"\n",
    "    keep_nodes_list = []\n",
    "    for v in range(data.num_nodes):\n",
    "        if node_ctree_codes[v] in subset_of_codes:\n",
    "            keep_nodes_list.append(v)\n",
    "    \n",
    "    mapping = {old: new for new, old in enumerate(keep_nodes_list)}\n",
    "    \n",
    "    if not keep_nodes_list:\n",
    "        # No nodes remain; return empty data and empty mapping\n",
    "        new_data = pyg.data.Data(\n",
    "            x=torch.empty((0, data.x.shape[1])),\n",
    "            edge_index=torch.empty((2, 0), dtype=torch.long),\n",
    "            edge_type=torch.empty((0,), dtype=torch.long)\n",
    "        )\n",
    "        return new_data, mapping\n",
    "\n",
    "    keep_nodes = torch.tensor(keep_nodes_list, dtype=torch.long)\n",
    "    # Filter node features\n",
    "    x_new = data.x[keep_nodes]\n",
    "\n",
    "    # Filter edges and corresponding edge_type\n",
    "    edges = []\n",
    "    e_types = []\n",
    "    keep_set = set(keep_nodes_list)\n",
    "    for i in range(data.edge_index.size(1)):\n",
    "        src = data.edge_index[0, i].item()\n",
    "        dst = data.edge_index[1, i].item()\n",
    "        if (src in keep_set) and (dst in keep_set):\n",
    "            edges.append([src, dst])\n",
    "            e_types.append(data.edge_type[i])  # No .item() here\n",
    "    if edges:\n",
    "        edges = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "        e_types = torch.tensor(e_types, dtype=torch.int64)\n",
    "    else:\n",
    "        edges = torch.empty((2,0), dtype=torch.long)\n",
    "        e_types = torch.empty((0,), dtype=torch.int64)\n",
    "    \n",
    "    # Relabel node IDs\n",
    "    edges = _relabel_edge_index(edges, keep_nodes)\n",
    "\n",
    "    new_data = pyg.data.Data(x=x_new, edge_index=edges, edge_type=e_types)\n",
    "    return new_data, mapping\n",
    "\n",
    "def _relabel_edge_index(edge_index, keep_nodes):\n",
    "    old_to_new = {old: i for i, old in enumerate(keep_nodes.tolist())}\n",
    "    new_edges = []\n",
    "    for i in range(edge_index.size(1)):\n",
    "        src_old = edge_index[0, i].item()\n",
    "        dst_old = edge_index[1, i].item()\n",
    "        new_edges.append([old_to_new[src_old], old_to_new[dst_old]])\n",
    "    if len(new_edges) == 0:\n",
    "        return torch.empty((2, 0), dtype=torch.long)\n",
    "    new_edges = torch.tensor(new_edges, dtype=torch.long).t().contiguous()\n",
    "    return new_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEFINE A VALUE FUNCTION THAT RUNS MRCGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_function(model, data, idx,\n",
    "                   subset_of_codes, node_ctree_codes, unique_ctree_codes,\n",
    "                   target_class=None):\n",
    "    \"\"\"\n",
    "    1) Remove concepts not in 'subset_of_codes'\n",
    "    2) Update idx based on the new node numbering\n",
    "    3) Run model.predict() and return a scalar value.\n",
    "    \"\"\"\n",
    "    # 1) Prune the graph and get the mapping from old indices to new indices.\n",
    "    modified_data, mapping = remove_concepts_from_graph(\n",
    "        data, node_ctree_codes, unique_ctree_codes, subset_of_codes\n",
    "    )\n",
    "    \n",
    "    # 2) Update idx: For each index in idx, if it exists in mapping, use the new index.\n",
    "    new_src = []\n",
    "    new_dst = []\n",
    "    for src, dst in zip(idx[0].tolist(), idx[1].tolist()):\n",
    "        if src in mapping and dst in mapping:\n",
    "            new_src.append(mapping[src])\n",
    "            new_dst.append(mapping[dst])\n",
    "    if len(new_src) == 0 or len(new_dst) == 0:\n",
    "        return 0.0  # no valid pairs remain\n",
    "\n",
    "    new_idx = (torch.tensor(new_src, dtype=torch.long),\n",
    "               torch.tensor(new_dst, dtype=torch.long))\n",
    "    \n",
    "    # 3) Forward pass\n",
    "    with torch.no_grad():\n",
    "        out = model.predict(modified_data, new_idx)  # shape: [num_pairs, 65]\n",
    "        if out.shape[0] == 0:\n",
    "            return 0.0\n",
    "\n",
    "        if target_class is None:\n",
    "            # Use the predicted class for each pair and average the logit.\n",
    "            preds = out.argmax(dim=1)\n",
    "            chosen_logits = out[torch.arange(out.size(0)), preds]\n",
    "            val = chosen_logits.mean().item()\n",
    "        else:\n",
    "            chosen_logits = out[:, target_class]\n",
    "            val = chosen_logits.mean().item()\n",
    "    return val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SHAPLEY VALUES (SAMPLING APPROACH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shapley_values(model, data, idx,\n",
    "                   unique_ctree_codes, node_ctree_codes,\n",
    "                   target_class=None,\n",
    "                   num_samples=50):\n",
    "    \"\"\"\n",
    "    Approximate Shapley values via random permutations.\n",
    "    \"\"\"\n",
    "    concepts = list(unique_ctree_codes.keys())  # DFS-code strings\n",
    "    M = len(concepts)\n",
    "    shap = np.zeros(M, dtype=float)\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        perm = np.random.permutation(M)\n",
    "        current_subset = set()\n",
    "        old_val = value_function(\n",
    "            model, data, idx,\n",
    "            current_subset, node_ctree_codes, unique_ctree_codes,\n",
    "            target_class\n",
    "        )\n",
    "        for j in range(M):\n",
    "            c_idx = perm[j]\n",
    "            c_code = concepts[c_idx]\n",
    "            new_subset = current_subset.union({c_code})\n",
    "            new_val = value_function(\n",
    "                model, data, idx,\n",
    "                new_subset, node_ctree_codes, unique_ctree_codes,\n",
    "                target_class\n",
    "            )\n",
    "            shap[c_idx] += (new_val - old_val)\n",
    "            current_subset = new_subset\n",
    "            old_val = new_val\n",
    "\n",
    "    shap /= num_samples\n",
    "    code2shap = {concepts[i]: shap[i] for i in range(M)}\n",
    "    return code2shap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RUN SHAPLEY & DISPLAY RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n9/t02bfwqj217dgp6q2c1qstdr0000gn/T/ipykernel_68960/985915582.py:202: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  e_type = torch.tensor(e_type, dtype=torch.int64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-10 Concepts by Shapley Value:\n",
      "0-408-22-477-474-214-16-130-182-365-33-63-57-240-250-8-527-366-223-113-233-31-311-154-61-228-437-426-310-161-418-274-567-13-406-279-60-346-114-19-320-398-166-53-120-211-556-502-348-563-522-18-40-545-224-225-202-321-388-503-383-167-352-193-419-410-111-183-370-330-194-252-264-151-273-349-195-216-363-507-534-47-128-123-253-255-288-394-54-555-282-77-354-324-339-405-99-44-226-520-17-209-326-415-298-43-266-338-490-483-89-548-169-500-376-291-145-64-441-488-323-565-529-178-95-160-232-347-105-454-414-355-198-399-448-436-269-157-28-29-424-480-416-49-132-51-537-335-340-242-119-236-88-485-146-501-450-131-258-452-382-482-11-20-333-451-542-290-286-245-515-237-7-155-281-197-73-261-91-510-430-557-2-400-74-239-521-344-308-142-259-125-523-107-412-463-350-547-517-277-423-249-327-205-177-404-70-76-129-552-289-23-117-94-434-312-316-464-307-428-374-26-38-447-254-30-90-543-553-25-409-62-46-148-116-139-505-385-564-328-518-554-473-292-152-325-478-562-55-71-271-378-4-421-445-102-75-72-471-235-244-343-106-498-185-381-425-230-508-103-389-351-127-509-156-56-67-59-551-462-372-215-429-220-494-37-41-293-32-163-456-427-486-549-172-511-108-284-48-97-303-391-256-317-212-492-93-98-396-217-285-184-83-65-458-191-208-299-100-221-190-519-12-82-489-460-144-137-287-438-115-229-341-210-104-35-276-302-162-14-484-544-173-247-109-295-470-530-435-36-262-168-3-407-359-134-80-175-313-468-411-401-136-126-402-85-39-524-66-78-373-86-9-227-6-453-280-238-135-377-329-367-417-443-270-353-170-159-263-213-332-360-395-539-118-68-199-188-469-432-479-356-187-87-497-181-446-439-322-491-268-24-369-504-362-309-192-153-81-174-231-532-431-304-526-513-110-141-345-257-260-558-300-533-384-525-241-422-461-481-357-440-171-272-283-234-246-496-397-442-387-200-79-218-459-512-50-493-101-318-334-331-455-45-138-560-516-314-207-122-186-296-566-559-541-506-203-379-112-550-219-179-278-92-149-27-58-251-222-420-96-204-305-15-5-457-275-10-476-531-306-69-164-301-393-265-294-371-147-248-536-392-465-243-297-364-358-413-140-1-466-201-176-206-143-380-158-433-315-375-528-150-165-514-538-34-319-535-342-124-499-449-561-368-21-84-475-386-361-336-390-403-189-337-42-540-444-267-487-180-196-467-121-495-472-546-133-52 => 9.6063\n",
      "0-407-22-475-472-213-16-130-181-364-33-63-57-239-249-8-525-365-222-113-232-31-310-154-61-227-436-425-309-161-417-273-565-13-405-278-60-345-114-19-319-397-165-53-120-210-554-500-347-561-520-18-40-543-223-224-201-320-387-501-382-166-351-192-418-409-111-182-369-329-193-251-263-151-272-348-194-215-362-505-532-47-128-123-252-254-287-393-54-553-281-77-353-323-338-404-99-44-225-518-17-208-325-414-297-43-265-337-488-481-89-546-168-498-375-290-145-64-440-486-322-563-527-177-95-160-231-346-105-452-413-354-197-398-446-435-268-157-28-29-423-478-415-49-132-51-535-334-339-241-119-235-88-483-146-499-448-131-257-450-381-480-11-20-332-449-540-289-285-244-513-236-7-155-280-196-73-260-91-508-429-555-2-399-74-238-519-343-307-142-258-125-521-107-411-461-349-545-515-276-422-248-326-204-176-403-70-76-129-550-288-23-117-94-433-311-315-462-306-427-373-26-38-445-253-30-90-541-551-25-408-62-46-148-116-139-503-384-562-327-516-552-471-291-152-324-476-560-55-71-270-377-4-420-443-102-75-72-469-234-243-342-106-496-184-380-424-229-506-103-388-350-127-507-156-56-67-59-549-460-371-214-428-219-492-37-41-292-32-163-454-426-484-547-171-509-108-283-48-97-302-390-255-316-211-490-93-98-395-216-284-183-83-65-456-190-207-298-100-220-189-517-12-82-487-458-144-137-286-437-115-228-340-209-104-35-275-301-162-14-482-542-172-246-109-294-468-528-434-36-261-167-3-406-358-134-80-174-312-466-410-400-136-126-401-85-39-522-66-78-372-86-9-226-6-451-279-237-135-376-328-366-416-442-269-352-169-159-262-212-331-359-394-537-118-68-198-187-467-431-477-355-186-87-495-180-444-438-321-489-267-24-368-502-361-308-191-153-81-173-230-530-430-303-524-511-110-141-344-256-259-556-299-531-383-523-240-421-459-479-356-439-170-271-282-233-245-494-396-441-386-199-79-217-457-510-50-491-101-317-333-330-453-45-138-558-514-313-206-122-185-295-564-557-539-504-202-378-112-548-218-178-277-92-149-27-58-250-221-419-96-203-304-15-5-455-274-10-474-529-305-69-164-300-392-264-293-370-147-247-534-391-463-242-296-363-357-412-140-1-464-200-175-205-143-379-158-432-314-374-526-150-512-536-34-318-533-341-124-497-447-559-367-21-84-473-385-360-335-389-402-188-336-42-538-266-485-179-195-465-121-493-470-544-133-52 => 0.1613\n",
      "0-407-22-476-473-213-16-130-181-364-33-63-57-239-249-8-526-365-222-113-232-31-310-154-61-227-436-425-309-161-417-273-566-13-405-278-60-345-114-19-319-397-165-53-120-210-555-501-347-562-521-18-40-544-223-224-201-320-387-502-382-166-351-192-418-409-111-182-369-329-193-251-263-151-272-348-194-215-362-506-533-47-128-123-252-254-287-393-54-554-281-77-353-323-338-404-99-44-225-519-17-208-325-414-297-43-265-337-489-482-89-547-168-499-375-290-145-64-440-487-322-564-528-177-95-160-231-346-105-453-413-354-197-398-447-435-268-157-28-29-423-479-415-49-132-51-536-334-339-241-119-235-88-484-146-500-449-131-257-451-381-481-11-20-332-450-541-289-285-244-514-236-7-155-280-196-73-260-91-509-429-556-2-399-74-238-520-343-307-142-258-125-522-107-411-462-349-546-516-276-422-248-326-204-176-403-70-76-129-551-288-23-117-94-433-311-315-463-306-427-373-26-38-446-253-30-90-542-552-25-408-62-46-148-116-139-504-384-563-327-517-553-472-291-152-324-477-561-55-71-270-377-4-420-444-102-75-72-470-234-243-342-106-497-184-380-424-229-507-103-388-350-127-508-156-56-67-59-550-461-371-214-428-219-493-37-41-292-32-163-455-426-485-548-171-510-108-283-48-97-302-390-255-316-211-491-93-98-395-216-284-183-83-65-457-190-207-298-100-220-189-518-12-82-488-459-144-137-286-437-115-228-340-209-104-35-275-301-162-14-483-543-172-246-109-294-469-529-434-36-261-167-3-406-358-134-80-174-312-467-410-400-136-126-401-85-39-523-66-78-372-86-9-226-6-452-279-237-135-376-328-366-416-442-269-352-169-159-262-212-331-359-394-538-118-68-198-187-468-431-478-355-186-87-496-180-445-438-321-490-267-24-368-503-361-308-191-153-81-173-230-531-430-303-525-512-110-141-344-256-259-557-299-532-383-524-240-421-460-480-356-439-170-271-282-233-245-495-396-441-386-199-79-217-458-511-50-492-101-317-333-330-454-45-138-559-515-313-206-122-185-295-565-558-540-505-202-378-112-549-218-178-277-92-149-27-58-250-221-419-96-203-304-15-5-456-274-10-475-530-305-69-164-300-392-264-293-370-147-247-535-391-464-242-296-363-357-412-140-1-465-200-175-205-143-379-158-432-314-374-527-150-513-537-34-318-534-341-124-498-448-560-367-21-84-474-385-360-335-389-402-188-336-42-539-443-266-486-179-195-466-121-494-471-545-133-52 => 0.0019\n",
      "0-401-22-466-463-207-16-126-176-358-32-61-55-233-243-8-516-359-216-110-226-30-304-149-59-221-429-418-303-156-410-267-552-13-399-272-58-339-111-19-313-391-161-51-117-204-541-491-341-548-511-18-39-531-217-218-195-314-381-492-376-162-345-187-411-403-108-177-363-323-188-245-257-146-266-342-189-209-356-496-522-46-124-119-246-248-281-387-52-540-275-75-347-317-332-398-96-43-219-509-17-202-319-407-291-42-259-331-479-472-86-533-164-489-369-284-140-62-433-477-316-550-518-173-92-155-225-340-102-445-406-348-191-392-439-428-262-152-27-28-416-469-408-48-128-50-524-328-333-235-116-229-85-474-141-490-441-127-251-443-375-471-11-20-326-442-528-283-279-238-504-230-7-150-274-190-71-254-88-499-422-542-2-393-72-232-510-337-301-137-252-121-512-104-405-454-343-532-506-270-415-242-320-198-172-397-68-74-125-537-282-57-536-87-529-538-24-402-60-45-143-305-309-455-300-420-367-25-37-438-247-29-461-70-73-99-436-507-539-462-285-147-318-467-547-53-69-264-371-4-413-208-421-213-483-36-40-286-31-158-497-223-417-374-179-487-446-44-430-112-222-334-203-101-34-81-63-449-185-201-292-310-205-494-378-549-321-297-515-502-486-175-97-214-100-382-344-123-498-151-54-65-478-451-139-132-280-389-210-278-178-453-365-105-500-192-182-459-129-352-400-3-240-106-288-460-136-338-9-220-6-444-273-231-130-370-255-163-545-475-534-167-184-508-12-277-47-94-296-384-249-228-237-336-103-350-470-452-424-468-349-181-84-482-353-388-346-165-154-256-206-90-481-495-134-519-95-364-287-148-186-302-355-493-404-458-306-170-78-239-227-166-265-276-311-253-250-543-293-521-394-131-122-395-82-38-513-64-76-366-83-66-115-431-437-315-480-261-23-530-168-362-447-419-505-307-200-360-409-435-269-295-157-14-473-169-224-520-423-79-448-107-10-465-377-514-234-414-15-5-426-35-427-114-91-98-385-241-464-41-183-120-56-386-258-174-142-212-144-322-138-425-324-390-434-380-193-77-211-450-501-49-26-308-368-526-159-527-263-89-199-118-180-80-289-432-551-544-456-236-194-485-171-268-327-244-215-412-93-197-325-294-271-330-113-351-196-476-372-109-535-373-153-135-1-357-298-290-517-145-160-329-67-383-299-503-525-133-33-312-523-335-488-440-546-361-21-396-484-379-354-260-457 => 0.0017\n",
      "0-404-22-473-470-212-16-130-181-362-33-63-57-238-247-8-523-363-221-113-231-31-308-154-61-226-433-422-307-161-414-271-563-13-402-276-60-343-114-19-317-394-166-53-120-209-552-498-345-559-518-18-40-541-222-223-200-318-384-499-379-167-349-192-415-406-111-182-367-327-193-249-261-151-270-346-194-214-360-503-530-47-128-123-250-252-285-390-54-551-279-77-351-321-336-401-99-44-224-516-17-207-323-411-295-43-263-335-486-479-89-544-169-496-373-288-145-64-437-484-320-561-525-178-95-160-230-344-105-450-410-352-196-395-444-432-266-157-28-29-420-476-412-49-132-51-533-332-337-240-119-234-88-481-146-497-446-131-255-448-378-478-11-20-330-447-538-287-283-242-511-235-7-155-278-195-73-258-91-506-426-553-2-396-74-237-517-341-305-142-256-125-519-107-408-459-347-543-513-274-419-246-324-203-177-400-70-76-129-548-286-23-117-94-430-309-313-460-304-424-371-26-38-443-251-30-90-539-549-25-405-62-46-148-116-139-501-381-560-325-514-550-469-289-152-322-474-558-55-71-268-375-4-417-441-102-75-72-467-233-241-340-106-494-184-377-421-228-504-103-385-348-127-505-156-56-67-59-547-458-369-213-425-218-490-37-41-290-32-163-452-423-482-545-172-507-108-281-48-97-300-387-253-314-210-488-93-98-392-215-282-183-83-65-454-190-206-296-100-219-189-515-12-82-485-456-144-137-284-434-115-227-338-208-104-35-273-299-162-14-480-540-173-244-109-292-466-526-431-36-259-168-3-403-356-134-80-175-310-464-407-397-136-126-398-85-39-520-66-78-370-86-9-225-6-449-277-236-135-374-326-364-413-439-267-350-170-159-260-211-329-357-391-535-118-68-197-187-465-428-475-353-186-87-493-180-442-435-319-487-265-24-366-500-359-306-191-153-81-174-229-528-427-301-522-509-110-141-342-254-257-554-297-529-380-521-239-418-457-477-354-436-171-269-280-232-243-492-393-438-383-198-79-216-455-508-50-489-101-315-331-328-451-45-138-556-512-311-205-122-185-293-562-555-537-502-201-112-546-217-179-275-92-149-27-58-248-220-416-96-202-302-15-5-453-272-10-472-527-303-69-164-298-389-262-291-368-147-245-532-388-461-294-361-355-409-140-1-462-199-176-204-143-376-158-429-312-372-524-150-165-510-534-34-316-531-339-124-495-445-557-365-21-84-471-382-358-333-386-399-188-334-42-536-440-264-483-463-121-491-468-542-133-52 => 0.0010\n",
      "0-403-22-472-469-210-16-130-179-361-33-63-57-236-246-8-522-362-219-113-229-31-307-153-61-224-432-421-306-160-413-270-562-13-401-275-60-342-114-19-316-393-164-53-120-207-551-497-344-558-517-18-40-540-220-221-198-317-384-498-379-165-348-190-414-405-111-180-366-326-191-248-260-150-269-345-192-212-359-502-529-47-128-123-249-251-284-389-54-550-278-77-350-320-335-400-99-44-222-515-17-205-322-410-294-43-262-334-485-478-89-543-167-495-372-287-144-64-436-483-319-560-524-176-95-159-228-343-105-449-409-351-194-394-443-431-265-156-28-29-419-475-411-49-132-51-532-331-336-238-119-232-88-480-145-496-445-131-254-447-378-477-11-20-329-446-537-286-282-241-510-233-7-154-277-193-73-257-91-505-425-552-2-395-74-235-516-340-304-141-255-125-518-107-407-458-346-542-512-273-418-245-323-201-175-399-70-76-129-547-285-23-117-94-429-308-312-459-303-423-370-26-38-442-250-30-90-538-548-25-404-62-46-147-116-138-500-381-559-324-513-549-468-288-151-321-473-557-55-71-267-374-4-416-440-102-75-72-466-231-240-339-106-493-182-377-420-226-503-103-385-347-127-504-155-56-67-59-546-457-368-211-424-216-489-37-41-289-32-162-451-422-481-544-170-506-108-280-48-97-299-386-252-313-208-487-93-98-391-213-281-181-83-65-453-188-204-295-100-217-187-514-12-82-484-455-143-136-283-433-115-225-337-206-104-35-272-298-161-14-479-539-171-243-109-291-465-525-430-36-258-166-3-402-355-133-80-173-309-463-406-396-135-126-397-85-39-519-66-78-369-86-9-223-6-448-276-234-134-373-325-363-412-438-266-349-168-158-259-209-328-356-390-534-118-68-195-185-464-427-474-352-184-87-492-178-441-434-318-486-264-24-365-499-358-305-189-152-81-172-227-527-426-300-521-508-110-140-341-253-256-553-296-528-380-520-237-417-456-476-353-435-169-268-279-230-242-491-392-437-383-196-79-214-454-507-50-488-101-314-330-327-450-45-137-555-511-310-203-122-183-292-561-554-536-501-199-375-112-545-215-177-274-92-148-27-58-247-218-415-96-200-301-15-5-452-271-10-471-526-302-69-163-297-388-261-290-367-146-244-531-387-460-239-293-360-354-408-139-1-461-197-174-202-142-376-157-428-311-371-523-149-509-533-34-315-530-338-124-494-444-556-364-21-84-470-382-357-332-398-186-333-42-535-439-263-482-462-121-490-467-541-52 => 0.0008\n",
      "0-404-22-472-469-212-16-130-181-362-33-63-57-238-247-8-522-363-221-113-231-31-308-154-61-226-433-422-307-161-414-271-562-13-402-276-60-343-114-19-317-394-166-53-120-209-551-497-345-558-517-18-40-540-222-223-200-318-384-498-379-167-349-192-415-406-111-182-367-327-193-249-261-151-270-346-194-214-360-502-529-47-128-123-250-252-285-390-54-550-279-77-351-321-336-401-99-44-224-515-17-207-323-411-295-43-263-335-485-478-89-543-169-495-373-288-145-64-437-483-320-560-524-178-95-160-230-344-105-450-410-352-196-395-444-432-266-157-28-29-420-475-412-49-132-51-532-332-337-240-119-234-88-480-146-496-446-131-255-448-378-477-11-20-330-447-537-287-283-242-510-235-7-155-278-195-73-258-91-505-426-552-2-396-74-237-516-341-305-142-256-125-518-107-408-459-347-542-512-274-419-246-324-203-177-400-70-76-129-547-286-23-117-94-430-309-313-460-304-424-371-26-38-443-251-30-90-538-548-25-405-62-46-148-116-139-500-381-559-325-513-549-468-289-152-322-473-557-55-71-268-375-4-417-441-102-75-72-467-233-241-340-106-493-184-377-421-228-503-103-385-348-127-504-156-56-67-59-546-458-369-213-425-218-489-37-41-290-32-163-452-423-481-544-172-506-108-281-48-97-300-387-253-314-210-487-93-98-392-215-282-183-83-65-454-190-206-296-100-219-189-514-12-82-484-456-144-137-284-434-115-227-338-208-104-35-273-299-162-14-479-539-173-244-109-292-466-525-431-36-259-168-3-403-356-134-80-175-310-464-407-397-136-126-398-85-39-519-66-78-370-86-9-225-6-449-277-236-135-374-326-364-413-439-267-350-170-159-260-211-329-357-391-534-118-68-197-187-465-428-474-353-186-87-492-180-442-435-319-486-265-24-366-499-359-306-191-153-81-174-229-527-427-301-521-508-110-141-342-254-257-553-297-528-380-520-239-418-457-476-354-436-171-269-280-232-243-491-393-438-383-198-79-216-455-507-50-488-101-315-331-328-451-45-138-555-511-311-205-122-185-293-561-554-536-501-201-112-545-217-179-275-92-149-27-58-248-220-416-96-202-302-15-5-453-272-10-471-526-303-69-164-298-389-262-291-368-147-245-531-388-461-294-361-355-409-140-1-462-199-176-204-143-376-158-429-312-372-523-150-165-509-533-34-316-530-339-124-494-445-556-365-21-84-470-382-358-333-386-399-188-334-42-535-440-264-482-463-121-490-541-133-52 => 0.0006\n",
      "0-407-22-475-472-214-16-130-182-365-33-63-57-240-250-8-525-366-223-113-233-31-311-154-61-228-436-425-310-161-417-274-565-13-405-279-60-346-114-19-320-397-166-53-120-211-554-500-348-561-520-18-40-543-224-225-202-321-387-501-382-167-352-193-418-409-111-183-370-330-194-252-264-151-273-349-195-216-363-505-532-47-128-123-253-255-288-393-54-553-282-77-354-324-339-404-99-44-226-518-17-209-326-414-298-43-266-338-488-481-89-546-169-498-376-291-145-64-440-486-323-563-527-178-95-160-232-347-105-453-413-355-198-398-447-435-269-157-28-29-423-478-415-49-132-51-535-335-340-242-119-236-88-483-146-499-449-131-258-451-381-480-11-20-333-450-540-290-286-245-513-237-7-155-281-197-73-261-91-508-429-555-2-399-74-239-519-344-308-142-259-125-521-107-411-462-350-545-515-277-422-249-327-205-177-403-70-76-129-550-289-23-117-94-433-312-316-463-307-427-374-26-38-446-254-30-90-541-551-25-408-62-46-148-116-139-503-384-562-328-516-552-471-292-152-325-476-560-55-71-271-378-4-420-444-102-75-72-470-235-244-343-106-496-185-380-424-230-506-103-388-351-127-507-156-56-67-59-549-461-372-215-428-220-492-37-41-293-32-163-455-426-484-547-172-509-108-284-48-97-303-390-256-317-212-490-93-98-395-217-285-184-83-65-457-191-208-299-100-221-190-517-12-82-487-459-144-137-287-437-115-229-341-210-104-35-276-302-162-14-482-542-173-247-109-295-469-528-434-36-262-168-3-406-359-134-80-175-313-467-410-400-136-126-401-85-39-522-66-78-373-86-9-227-6-452-280-238-135-377-329-367-416-442-270-353-170-159-263-213-332-360-394-537-118-68-199-188-468-431-477-356-187-87-495-181-445-438-322-489-268-24-369-502-362-309-192-153-81-174-231-530-430-304-524-511-110-141-345-257-260-556-300-531-383-523-241-421-460-479-357-439-171-272-283-234-246-494-396-441-386-200-79-218-458-510-50-491-101-318-334-331-454-45-138-558-514-314-207-122-186-296-564-557-539-504-203-112-548-219-179-278-92-149-27-58-251-222-419-96-204-305-15-5-456-275-10-474-529-306-69-164-301-392-265-294-371-147-248-534-391-464-243-297-364-358-412-140-1-465-201-176-206-143-379-158-432-315-375-526-150-165-512-536-34-319-533-342-124-497-448-559-368-21-84-473-385-361-336-389-402-189-337-42-538-443-267-485-180-196-466-121-493-544-133-52 => 0.0005\n",
      "0-406-22-474-471-214-16-130-182-364-33-63-57-240-250-8-524-365-223-113-233-31-311-154-61-228-435-424-310-161-416-274-564-13-404-279-60-345-114-19-320-396-166-53-120-211-553-499-347-560-519-18-40-542-224-225-202-321-386-500-381-167-351-193-417-408-111-183-368-330-194-252-264-151-273-348-195-216-362-504-531-47-128-123-253-255-288-392-54-552-282-77-353-324-338-403-99-44-226-517-17-209-326-413-298-43-266-337-487-480-89-545-169-497-374-291-145-64-439-485-323-562-526-178-95-160-232-346-105-451-412-354-198-397-446-434-269-157-28-29-422-477-414-49-132-51-534-335-339-242-119-236-88-482-146-498-447-131-258-449-380-479-11-20-333-448-539-290-286-245-512-237-7-155-281-197-73-261-91-507-428-554-2-398-74-239-518-343-308-142-259-125-520-107-410-460-349-544-514-277-421-249-327-205-177-402-70-76-129-549-289-23-117-94-432-312-316-461-307-426-372-26-38-445-254-30-90-540-550-25-407-62-46-148-116-139-502-383-561-328-515-551-470-292-152-325-475-559-55-71-271-376-4-419-443-102-75-72-468-235-244-342-106-495-185-379-423-230-505-103-387-350-127-506-156-56-67-59-548-459-370-215-427-220-491-37-41-293-32-163-453-425-483-546-172-508-108-284-48-97-303-389-256-317-212-489-93-98-394-217-285-184-83-65-455-191-208-299-100-221-190-516-12-82-486-457-144-137-287-436-115-229-340-210-104-35-276-302-162-14-481-541-173-247-109-295-467-527-433-36-262-168-3-405-358-134-80-175-313-465-409-399-136-126-400-85-39-521-66-78-371-86-9-227-6-450-280-238-135-375-329-366-415-441-270-352-170-159-263-213-332-359-393-536-118-68-199-188-466-430-476-355-187-87-494-181-444-437-322-488-268-24-367-501-361-309-192-153-81-174-231-529-429-304-523-510-110-141-344-257-260-555-300-530-382-522-241-420-458-478-356-438-171-272-283-234-246-493-395-440-385-200-79-218-456-509-50-490-101-318-334-331-452-45-138-557-513-314-207-122-186-296-563-556-538-503-203-377-112-547-219-179-278-92-149-27-58-251-222-418-96-204-305-15-5-454-275-10-473-528-306-69-164-301-391-265-294-369-147-248-533-390-462-243-297-363-357-411-140-1-463-201-176-206-143-378-158-431-315-373-525-150-165-511-535-34-319-532-341-124-496-21-84-472-384-360-388-401-189-336-42-537-442-558-267-484-180-196-464-121-492-469-543-133-52 => 0.0003\n",
      "0-401-22-470-467-210-16-130-179-360-33-63-57-236-246-8-520-361-219-113-229-31-307-153-61-224-430-419-306-160-411-270-560-13-399-275-60-341-114-19-316-391-164-53-120-207-549-495-343-556-515-18-40-538-220-221-198-317-382-496-377-165-347-190-412-403-111-180-364-326-191-248-260-150-269-344-192-212-358-500-527-47-128-123-249-251-284-387-54-548-278-77-349-320-334-398-99-44-222-513-17-205-322-408-294-43-262-333-483-476-89-541-167-493-370-287-144-64-434-481-319-558-522-176-95-159-228-342-105-447-407-350-194-392-441-429-265-156-28-29-417-473-409-49-132-51-530-331-335-238-119-232-88-478-145-494-443-131-254-445-376-475-11-20-329-444-535-286-282-241-508-233-7-154-277-193-73-257-91-503-423-550-2-393-74-235-514-339-304-141-255-125-516-107-405-456-345-540-510-273-416-245-323-201-175-397-70-76-129-545-285-23-117-94-427-308-312-457-303-421-368-26-38-440-250-30-90-536-546-25-402-62-46-147-116-138-498-379-557-324-511-547-466-288-151-321-471-555-55-71-267-372-4-414-438-102-75-72-464-231-240-338-106-491-182-375-418-226-501-103-383-346-127-502-155-56-67-59-544-455-366-211-422-216-487-37-41-289-32-162-449-420-479-542-170-504-108-280-48-97-299-384-252-313-208-485-93-98-389-213-281-181-83-65-451-188-204-295-100-217-187-512-12-82-482-453-143-136-283-431-115-225-336-206-104-35-272-298-161-14-477-537-171-243-109-291-463-523-428-36-258-166-3-400-354-133-80-173-309-461-404-394-135-126-395-85-39-517-66-78-367-86-9-223-6-446-276-234-134-371-325-362-410-436-266-348-168-158-259-209-328-355-388-532-118-68-195-185-462-425-472-351-184-87-490-178-439-432-318-484-264-24-363-497-357-305-189-152-81-172-227-525-424-300-519-506-110-140-340-253-256-551-296-526-378-518-237-415-454-474-352-433-169-268-279-230-242-489-390-435-381-196-79-214-452-505-50-486-101-314-330-327-448-45-137-553-509-310-203-122-183-292-559-552-534-499-199-373-112-543-215-177-274-92-148-27-58-247-218-413-96-200-301-15-5-450-271-10-469-524-302-69-163-297-386-261-290-365-146-244-529-385-458-239-293-359-353-406-139-1-459-197-174-202-142-374-157-426-311-369-521-149-507-531-34-315-528-337-124-492-442-554-21-84-468-380-356-396-186-332-42-533-437-263-480-460-121-488-465-539-52 => 0.0001\n"
     ]
    }
   ],
   "source": [
    "shap_vals = shapley_values(\n",
    "    model=model,\n",
    "    data=data,\n",
    "    idx=idx,\n",
    "    unique_ctree_codes=unique_ctree_codes,\n",
    "    node_ctree_codes=node_ctree_codes,\n",
    "    target_class=None,  # or an int specifying which class\n",
    "    num_samples=50\n",
    ")\n",
    "\n",
    "# Sort Shapley results\n",
    "sorted_shap = sorted(shap_vals.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nTop-10 Concepts by Shapley Value:\")\n",
    "for c, val in sorted_shap[:10]:\n",
    "    print(f\"{c} => {val:.4f}\")\n",
    "\n",
    "# Done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
